[
    {
        "Title": "Beyond Simple Edits: Composed Video Retrieval with Dense Modifications",
        "Field & Subfield": "Computer Science > Computer Vision",
        "Key Contributions": "• Introduces Dense-WebVid-CoVR dataset with 1.6M samples & dense modification text. • Proposes a robust CoVR model with unified grounding encoder for input video, description, & modification text fusion. • Achieves SOTA on Dense-WebVid-CoVR (71.3% Recall@1), outperforming existing methods by 3.4%. • Shows improved performance on CoIR datasets (CIRR, FashionIQ) & Ego-CVR.",
        "Methodology": "Dense-WebVid-CoVR created using Gemini-Pro for video descriptions & GPT-40 for dense modification texts, with manual verification. The model uses ViT-L for vision, BLIP-pretrained text encoder, & a BLIP-2 based grounding text encoder for unified fusion of query video, description, & modification text via cross-attention, trained with contrastive loss.",
        "Strengths": "• Introduces a large-scale, high-quality dataset with dense modification texts (7x longer). • Novel unified fusion strategy for better multimodal alignment. • Achieves SOTA performance on multiple CoVR & CoIR benchmarks. • Enhanced fine-grained retrieval capturing subtle visual/temporal changes. • Robust quality control for dataset generation.",
        "Limitations": "• Minor inaccuracies (2-3%) in training set, claimed minimal impact. • Future work: multilingual CoVR for low-resource languages. • Future work: efficient techniques for processing very long videos.",
        "Datasets / Benchmarks": "Introduced: Dense-WebVid-CoVR (1.6M samples). Used/Compared against: WebVid-CoVR, EgoCVR, CIRR, FashionIQ.",
        "Results Summary": "Achieves SOTA on Dense-WebVid-CoVR with 71.3% Recall@1 (3.4% gain over prior SOTA). Consistently outperforms baselines on Ego-CVR, CIRR (56.30% R@1), and FashionIQ, demonstrating superior fine-grained retrieval accuracy across all settings.",
        "Why It Matters": "Advances composed video retrieval by providing a richer, contextually aware dataset & an effective model. Enables precise video retrieval based on subtle modifications, crucial for applications like video editing & media production. Dense modification texts are a key innovation.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Fig 1: Example CoVR triplets comparing modification texts. Fig 3: Proposed CoVR architecture. Table 1: Comparative analysis of CoVR benchmarks. Table 2: Main results on Dense-WebVid-CoVR. Fig 13: Detailed modification-text comparison."
    },
    {
        "Title": "COMPUTERRL: SCALING END-TO-END ONLINE REINFORCEMENT LEARNING FOR COMPUTER USE AGENTS",
        "Field & Subfield": "Artificial Intelligence > Reinforcement Learning",
        "Key Contributions": "- Introduces API-GUI paradigm for machine-oriented desktop interaction, unifying API calls and GUI actions.\n- Establishes a large-scale, distributed RL infrastructure supporting thousands of parallel virtual desktop environments for scalable training.\n- Proposes Entropulse, a novel training strategy alternating RL with supervised fine-tuning to mitigate entropy collapse and ensure sustained learning.\n- Achieves new state-of-the-art accuracy of 48.1% on the OSWorld benchmark for general desktop automation agents.",
        "Methodology": "COMPUTERRL combines a novel API-GUI paradigm unifying programmatic control and GUI interaction, a distributed RL infrastructure using Docker and gRPC for scalable parallel environments, and Entropulse, a training strategy that alternates RL with SFT to maintain exploration and prevent entropy collapse. It leverages LLMs for API construction and employs a step-level GRPO algorithm with rule-based verifiable rewards.",
        "Strengths": [
            "Novel API-GUI paradigm offers superior operational efficiency and generalization.",
            "Highly scalable distributed RL infrastructure supports thousands of parallel environments.",
            "Entropulse strategy ensures robust and sustained performance gains in extended RL training.",
            "Achieves state-of-the-art performance on a challenging real-world desktop automation benchmark (OSWorld).",
            "Significantly reduces steps required for task completion compared to baselines (1/3)."
        ],
        "Limitations": [
            "Errors observed in visual perception and multi-application coordination.",
            "Challenges with operational illusions and other miscellaneous errors.",
            "Genuine universality and adaptation to unfamiliar applications remain open questions.",
            "Long-horizon autonomy and complex, multi-step objectives are still being explored."
        ],
        "Datasets / Benchmarks": "Evaluated on the OSWorld benchmark and OSWorld-Verified benchmark. No new datasets were introduced by the paper.",
        "Results Summary": "COMPUTERRL-trained AutoGLM-OS-9B achieved 48.1% success on OSWorld, surpassing SOTA models like OpenAI CUA (42.9%), UI-TARS-1.5 (42.5%), and Claude Sonnet 4 (30.7%). The API-GUI paradigm showed a 134% improvement over GUI-only approaches. Entropulse increased average training rewards and improved learning efficiency by mitigating entropy collapse.",
        "Why It Matters": "This work addresses critical challenges in developing autonomous agents for complex digital workspaces, offering a scalable and robust framework for end-to-end online RL. By bridging the gap between machine and human-centric GUIs, it lays a foundational step towards truly intelligent desktop automation, improving efficiency and paving the way for more capable generalist agents.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1: Shows state-of-the-art success rates on OSWorld (1a) and COMPUTERRL training reward curves (1b), demonstrating Entropulse's benefits. Table 1: Provides a comprehensive comparison of AUTOGLM-OS performance against various proprietary and open models on OSWorld benchmarks."
    },
    {
        "Title": "Relational Visual Similarity",
        "Field & Subfield": "Computer Vision & Image Understanding. Focuses on a novel visual similarity metric leveraging Vision-Language Models for abstract relational reasoning, moving beyond surface attributes to capture human-like conceptual similarities.",
        "Key Contributions": [
            "Introduced 'relational visual similarity' as a new dimension.",
            "Curated a 114k {image-anonymous caption} dataset for relational training.",
            "Developed 'relsim,' a new metric for relational visual similarity.",
            "Demonstrated relsim's utility in image retrieval & generation.",
            "Analyzed relationship between attribute and relational similarity."
        ],
        "Methodology": "Proposed `relsim` by: 1) Filtering LAION-2B to select images with relational cues. 2) Generating anonymous captions (describing relational logic, not surface content) from image groups using a fine-tuned VLM. 3) Training `relsim` (Qwen2.5-VL-7B) with InfoNCE loss to align image/caption embeddings.",
        "Strengths": "Captures human-like abstract relational reasoning, a key missing dimension in visual AI. `relsim` significantly outperforms existing attribute-based metrics in recognizing deeper conceptual similarities, validated by user studies & GPT-40 evaluation. Enables novel image retrieval/generation applications.",
        "Limitations": "Anonymous caption dataset curated manually (532 groups), limiting scalability & prone to bias. VLMs can hallucinate captions. Difficulty in specifying user intent for multi-relational images. Expanding dataset/pipeline is a future direction.",
        "Datasets / Benchmarks": "New: Relational Dataset (114k {image, anonymous caption} pairs derived from LAION-2B). Baselines: LPIPS, DINO, CLIP, dreamsim (image-to-image); CLIP-T, Qwen-T (caption-based). Evaluation: GPT-40 automated judge & human user study.",
        "Results Summary": "`relsim` achieved a GPT score of 6.77 (vs. LPIPS 4.56, CLIP-I 5.91), indicating superior relational similarity detection. User studies showed consistent human preference for `relsim` (42.5-60.7%). Proprietary models excel in analogical generation (GPT40 relsim 0.82) over open-source models.",
        "Why It Matters": "Addresses a critical gap in AI's visual understanding by moving beyond surface attributes to capture abstract, relational logic, mirroring human cognition. This unlocks new possibilities for creative AI applications, enhancing image retrieval, and analogical content generation.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Illustrates relational vs. attribute similarity), Figure 2 (Overall Pipeline), Figure 6 (relsim performance vs. baselines), Figure 8 (User study results), Table 2 (Analogical image generation benchmarks).",
        "arxiv_id": "http://arxiv.org/abs/2512.07833v1"
    },
    {
        "Title": "ASTRA: General INTERACTIVE WORLD MODEL WITH AUTOREGRESSIVE DENOISING",
        "Field & Subfield": "Computer Vision, Generative Models, World Models",
        "Key Contributions": "Introduces Astra, an interactive general world model. Key contributions: Autoregressive denoising with temporal causal attention; Action-aware adapter for precise control; Noise-augmented history memory for consistency; Mixture of Action Experts for diverse modalities.",
        "Methodology": "Autoregressive denoising framework based on a pre-trained video diffusion backbone. Utilizes an Action-Aware Flow Transformer (AFT) with ACT-Adapter for action injection, a noise-as-mask strategy for history memory to reduce visual inertia, and a Mixture of Action Experts (MoAE) for multimodal actions.",
        "Strengths": "Achieves high visual fidelity, strong temporal consistency, and precise action responsiveness. Versatile across diverse tasks (robotics, AD, exploration) and generalizes well to out-of-domain scenes. Parameter-efficient design with low compute overhead.",
        "Limitations": "Inference efficiency is limited, requiring multiple denoising steps per frame. This makes real-time deployment challenging for latency-sensitive applications like online control or interactive robotics.",
        "Datasets / Benchmarks": "nuScenes, Sekai, SpatialVID, RT-1, Multi-Cam Video. Evaluated on Astra-Bench (custom benchmark) and CityWalker for out-of-domain generalization.",
        "Results Summary": "Outperforms SOTA models (Wan-2.1, MatrixGame, YUME) across all metrics: instruction following, subject/background consistency, motion smoothness, and visual quality. Achieves lower rotation/translation errors and superior action-following accuracy on diverse benchmarks.",
        "Why It Matters": "Astra advances interactive world modeling, enabling more general, scalable simulators for exploration, robotics, autonomous driving, and embodied intelligence, bridging the gap to true interactive world simulation.",
        "Should Read Fully?": "Yes, for researchers and practitioners interested in interactive world models, diffusion-based video generation, robotics, autonomous driving, or embodied AI.",
        "Key Figures or Tables": "Fig. 1 (diverse applications), Fig. 3 (Astra framework), Fig. 5 & 6 (qualitative results), Table 2 (SOTA comparison), Table 3 (ablation studies).",
        "arxiv_id": "http://arxiv.org/abs/2512.08931v1"
    },
    {
        "Title": "Efficiently Reconstructing Dynamic Scenes One D4RT at a Time",
        "Field & Subfield": "Computer Vision; 4D Reconstruction & Tracking",
        "Key Contributions": "Novel query-based feedforward model for efficient 4D scene reconstruction. Unified transformer architecture for depth, correspondence, point clouds, camera params. SOTA accuracy & speed in dynamic 4D reconstruction & tracking. Enables efficient dense, holistic scene reconstruction.",
        "Methodology": "D4RT uses an encoder (ViT-g) to create a global scene representation (F) from video. A lightweight cross-attention decoder queries F with spatio-temporal parameters (u,v, t_src, t_tgt, t_cam) and local RGB patch to predict 3D point positions.",
        "Strengths": "Highly efficient (18-300x faster, 200+ FPS), scalable, unified interface for multiple 4D tasks, SOTA accuracy across diverse benchmarks, handles dynamic scenes, supports subpixel precision & high-res decoding with novel querying.",
        "Limitations": "No explicit limitations for D4RT are stated, as the paper focuses on overcoming prior methods' drawbacks. Implicitly, a large encoder requires pre-training. Dense inference still has computational cost, partially addressed by occupancy grid optimization.",
        "Datasets / Benchmarks": "BlendedMVS, Co3Dv2, Dynamic Replica, Kubric, MVS-Synth, PointOdyssey, ScanNet, ScanNet++, Tartanair, VirtualKitti, Waymo Open (training). Sintel, ScanNet, KITTI, Bonn, TapVid-3D (evaluation).",
        "Results Summary": "D4RT sets SOTA across 4D tasks (depth, point cloud, 3D tracking, camera pose) on various benchmarks. Achieves significantly higher throughput (e.g., 200+ FPS, 100x faster than MegaSaM) while delivering superior accuracy. Generalizes to long videos & high-res decoding.",
        "Why It Matters": "Offers a unified, efficient, and highly scalable framework for 4D reconstruction, overcoming fragmentation and computational bottlenecks of prior methods, paving the way for next-generation 4D perception in complex dynamic environments.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Fig. 1 (Model overview), Fig. 3 (Pose accuracy vs. speed), Fig. 4 (Qualitative reconstruction), Table 3 (3D tracking throughput), Table 4 (4D tracking metrics).",
        "arxiv_id": "http://arxiv.org/abs/2512.08924v1"
    },
    {
        "Title": "Selfi: Self Improving Reconstruction Engine via 3D Geometric Feature Alignment",
        "Field & Subfield": "Computer Vision & 3D Reconstruction, Novel View Synthesis",
        "Key Contributions": "• Self-improving geometric feature learning from unposed RGB via reprojection-based consistency loss. • Achieves state-of-the-art NVS and pose estimation using a frozen VFM backbone. • Enhances NVS quality and pose accuracy via dense bundle adjustment with depth shift.",
        "Methodology": "Selfi uses a pre-trained VGGT (3D VFM) as a backbone. A DPT adapter learns geometrically-aligned features using a self-supervised reprojection-based consistency loss with pseudo-ground-truth from VGGT outputs. These features predict 3D Gaussian parameters via a U-Net decoder. Poses are refined using dense bundle adjustment, with Gaussian centers adjusted via an affine depth shift.",
        "Strengths": "State-of-the-art NVS and pose estimation from unposed, uncalibrated images. Achieves high-fidelity rendering, including thin structures and fine details. Robust to varying input views/overlap. Self-supervised, efficiently leveraging VFMs.",
        "Limitations": "Relies on VGGT's normalized scale, leading to depth inaccuracies in distant regions (e.g., sky). Sensitive to exposure discrepancies between input and target images. Currently restricted to static scenes, failing on dynamic environments.",
        "Datasets / Benchmarks": "Trained on DL3DV and RealEstate10K. Evaluated on DL3DV, RealEstate10K, MipNeRF, Tanks&Temples, and RayZer split. Metrics include PSNR, SSIM, LPIPS for NVS; AUC@3, AUC@5, AUC@15 for pose estimation.",
        "Results Summary": "Selfi consistently outperforms existing pose-free feed-forward Gaussian methods (e.g., AnySplat, WorldMirror) and achieves performance comparable to/exceeding 3DGS with ground-truth poses. Achieves SOTA on NVS & pose estimation benchmarks across various input settings (varying sequence lengths, overlap).",
        "Why It Matters": "Selfi advances novel view synthesis and 3D reconstruction by bypassing the fragile and computationally intensive SfM pipeline. It enables robust, high-fidelity 3D scene understanding from unposed images, making 3D VFMs more practical for real-world applications without explicit 3D ground truth.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Selfi pipeline overview), Figure 3 & 5 (Qualitative NVS comparisons), Table 1 & 2 (NVS performance with varying inputs/overlap), Table 5 (BA for NVS/Pose).",
        "arxiv_id": "http://arxiv.org/abs/2512.08930v1"
    },
    {
        "Title": "Closing the Train-Test Gap in World Models for Gradient-Based Planning",
        "Field & Subfield": "Reinforcement Learning & Robotics; Model-Based Planning",
        "Key Contributions": "- Proposes Online World Modeling (OWM) to expand WM's reliable state space via simulator-corrected trajectories.\n- Introduces Adversarial World Modeling (AWM) to smooth loss landscape & promote robustness by finetuning on perturbed inputs.\n- Achieves CEM-level performance with 10x speedup for gradient-based planning.",
        "Methodology": "Two finetuning methods (OWM, AWM) for latent world models (DINO-WM) used in gradient-based planning (GBP). OWM generates and finetunes on simulator-corrected GBP trajectories. AWM perturbs inputs to maximize prediction error, smoothing the planning loss landscape.",
        "Strengths": "Significantly improves gradient-based planning reliability and performance, often matching or exceeding CEM. Achieves a 10x computational speedup over CEM. Smooths the planning loss landscape, reducing local minima. Narrows the train-test gap in world model error.",
        "Limitations": "Online World Modeling relies on access to an environment simulator, which might be costly or infeasible in real-world settings. Performance benefits vary by task and specific gradient optimizer used (e.g., Adam generally outperforms GD).",
        "Datasets / Benchmarks": "PushT, PointMaze, Wall (main experiments); Rope, Granular (additional robotic manipulation tasks). Utilizes existing DINO-WM datasets and architectures.",
        "Results Summary": "AWM with Adam GBP outperforms or matches CEM on PushT, PointMaze, Wall tasks, achieving up to +30% success rate increase over DINO-WM (open-loop) and up to 10x faster execution. Both OWM and AWM narrow the train-test gap.",
        "Why It Matters": "Enables practical and efficient gradient-based planning with world models, a critical step for real-world robotics. Addresses a fundamental train-test gap, making learned dynamics models more reliable for long-horizon control.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Table 1 (Planning Results), Figure 2 (Optimization landscape of DINO-WM vs. AWM), Figure 3 (Planning efficiency)",
        "arxiv_id": "2512.09929v1"
    },
    {
        "Title": "GAINS: Gaussian-based Inverse Rendering from Sparse Multi-View Captures",
        "Field & Subfield": "Computer Vision & Graphics, Inverse Rendering",
        "Key Contributions": "- Two-stage inverse rendering for sparse multi-view captures.\n- Synergizes learning-based priors (depth/normal, diffusion) for robust geometry.\n- Combines segmentation, IID, and diffusion priors for stable material recovery.\n- Achieves SOTA relighting, NVS, and material accuracy on sparse inputs.",
        "Methodology": "Two-stage pipeline using 2DGS. Stage I: Geometry refinement with monocular depth/normal and diffusion priors. Stage II: Material properties & lighting estimation using segmentation, intrinsic image decomposition (IID), and diffusion priors, all combined with PBR.",
        "Strengths": "Significantly improves material parameter accuracy, relighting quality, and novel-view synthesis, especially under sparse-view conditions. More robust against overfitting and ambiguity than prior methods. Better intrinsic separation and reduced reflection baking.",
        "Limitations": "Segmentation guidance can trade off robustness for high-frequency detail under dense supervision. IID prior can exhibit view-inconsistency, though mitigated by a weighted loss.",
        "Datasets / Benchmarks": "Synthetic4Relight [44], TensorIR [10], Ref-Real [29]. Evaluated using PSNR, SSIM, LPIPS for NVS/Albedo/Relighting, and MSE for Roughness.",
        "Results Summary": "GAINS consistently outperforms SOTA Gaussian-based IR methods (Ref-GS, GI-GS) on all datasets and metrics, especially with sparse (4-8) views. Achieves higher PSNR, SSIM, lower LPIPS/MSE for NVS, albedo, roughness, and relighting.",
        "Why It Matters": "Addresses the challenging problem of inverse rendering from sparse inputs, which is critical for real-world applications where dense multi-view captures are impractical or impossible, enabling more robust 3D scene understanding.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Table 1, Table 2 (Quantitative evaluation across datasets), Table 3 (Ablation study of priors), Figure 1 (Qualitative intrinsics/relighting), Figure 6 (View-dependent performance)."
    },
    {
        "Title": "ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning",
        "Field & Subfield": "Computer Vision & Generative AI: Video Editing",
        "Key Contributions": "- Introduces Reason-Informed Video Editing (RVE) task.\n- Proposes RVE-Bench, a comprehensive benchmark for RVE.\n- Develops ReViSE, a Self-Reflective Reasoning (SRF) framework.\n- Leverages internal VLM for intrinsic, differential feedback.\n- Achieves SOTA on RVE-Bench.",
        "Methodology": "ReViSE employs a Self-Reflective Reasoning (SRF) framework, unifying generation & internal evaluation. An internal VLM acts as a critic, providing differential feedback. Training uses Unified Semantic Optimization (USO) & Reward Weighted Optimization (RWO) with flow-matching loss.",
        "Strengths": "Bridges the reasoning-editing gap in video models, provides self-supervised intrinsic feedback, significantly improves reasoning accuracy & visual fidelity, and introduces a robust, reasoning-aware evaluation framework using GPT-4o.",
        "Limitations": "Performance is primarily constrained by the capabilities of the base models. Cannot fully exploit potential without access to more powerful foundational models due to resource limitations.",
        "Datasets / Benchmarks": "RVE-Bench (new, 1,000 unique triplets), comprising two subsets: Reasoning-Informed Video Editing (809 samples) & In-Context Video Generation (200 samples). Leverages Ditto-1M and collected movie data.",
        "Results Summary": "ReViSE significantly enhances editing accuracy and visual fidelity on RVE-Bench, achieving a 32% improvement in Overall score on the reasoning-informed video editing subset over state-of-the-art methods across all reasoning categories.",
        "Why It Matters": "Addresses a crucial gap in video editing by enabling models to understand & incorporate complex reasoning (physical plausibility, causal dynamics), moving beyond literal transformations to generate more realistic, logically coherent, and contextually rich videos.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (ReViSE framework overview), Figure 2 (RVE-Bench overview & statistics), Table 1 (Quantitative Results on RVE-Bench).",
        "arxiv_id": "http://arxiv.org/abs/2512.09924v1"
    },
    {
        "Title": "SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model",
        "Field & Subfield": "Computer Vision & 3D Scene Generation",
        "Key Contributions": "- Decoupled 3D scene generation framework for better open-set prior learning.\n- Robust de-occlusion model leveraging image datasets & a new 10K curated dataset.\n- Unified pose estimation diffusion model with global/local attention & new 200K synthetic scene dataset for generalization.",
        "Methodology": "Decoupled 3D scene generation into de-occlusion, 3D object generation, and pose estimation. De-occlusion model finetunes Flux Kontext on a 10K image dataset. Pose estimation uses a diffusion model with global/local self/cross-attention, trained on a 200K synthetic scene dataset, predicting 6D pose & size.",
        "Strengths": "Achieves SOTA on indoor & open-set scenes for geometry quality & pose accuracy. Robust under severe occlusion & diverse objects. Strong generalization. Enables text-controllable de-occlusion, enhancing quality & controllability.",
        "Limitations": "Real-world object arrangements (e.g., force interactions, interpenetration) are not fully captured by datasets. Existing methods have limited control signals (images/simple captions). Deeper understanding for embodied AI tasks remains an unsolved challenge.",
        "Datasets / Benchmarks": "New 10K object image de-occlusion dataset, new 200K synthetic open-set scene dataset. Trained on Objaverse. Evaluated against MIDI, 3D-Front, and other SOTA methods.",
        "Results Summary": "Outperforms SOTA methods on indoor & open-set scenes in both geometry quality (CD-S, F-Score-S, IoU-B) and pose accuracy, and de-occlusion metrics (PSNR, SSIM, CLIP). Demonstrates superior performance, especially under severe occlusion.",
        "Why It Matters": "Advances realistic open-set 3D scene generation, addressing crucial limitations in occlusion handling and accurate pose estimation. Essential for progress in AIGC, embodied AI, simulation environment construction, and 3D perception.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (SceneMaker outputs), Figure 3 (Framework), Table 3 (Quantitative comparison on indoor & open-set), Figure 6 (De-occlusion comparison).",
        "arxiv_id": "http://arxiv.org/abs/2512.10957v1"
    },
    {
        "Title": "StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space",
        "Field & Subfield": "Computer Vision & Novel View Synthesis / Stereo Generation",
        "Key Contributions": "- Depth-free monocular-to-stereo synthesis via end-to-end diffusion in a canonical space.\n- Viewpoint-conditioned framework using Plücker ray embeddings for metric baseline control.\n- Novel evaluation protocol with iSQoE (perceptual comfort) and MEt3R (geometric consistency).",
        "Methodology": "Diffusion-based framework (LDM) using a dual U-Net architecture (initialized from Stable Diffusion 2.0). Models geometry via viewpoint conditioning with Plücker ray embeddings in a canonical rectified space, avoiding explicit depth or warping.",
        "Strengths": "Depth-free handling of complex multi-layer and non-Lambertian scenes. Strong cross-baseline generalization with metric control. Superior perceptual comfort (iSQoE) and geometric consistency (MEt3R) compared to baselines.",
        "Limitations": "All methods, including StereoSpace, still exhibit degradation on extreme multi-layer/transparent scenes. Auxiliary disparity loss slightly degrades iSQoE scores.",
        "Datasets / Benchmarks": "Training: TartanAir, Dynamic Replica, IRS, Falling Things, LayeredFlow, NeRF-Stereo, SceneSplat-7K. Evaluation: Middlebury 2014, DrivingStereo, Booster, LayeredFlow.",
        "Results Summary": "Outperforms prior warp/inpaint, latent-warping, and warped-conditioning methods on iSQoE and MEt3R metrics. Achieves best perceptual comfort and geometric consistency, particularly on complex multi-layer geometries.",
        "Why It Matters": "Enables high-quality stereo image generation from monocular inputs without costly 3D reconstruction, reducing production costs and facilitating 2D to 3D content conversion for immersive media.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Framework Overview), Table 2 (Main Results on Middlebury & DrivingStereo)",
        "arxiv_id": "http://arxiv.org/abs/2512.10959v1"
    },
    {
        "Title": "WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World",
        "Field & Subfield": "Embodied AI & Computer Vision; Generative World Models",
        "Key Contributions": "- Introduces WorldLens, a benchmark for driving world models.\n- Defines 5 evaluation aspects: Generation, Reconstruction, Action-Following, Downstream Task, Human Preference.\n- Curates WorldLens-26K, a large-scale human-annotated video dataset.\n- Develops WorldLens-Agent, an auto-evaluator distilled from human preferences.",
        "Methodology": "WorldLens evaluates driving world models across 5 aspects using 20+ metrics. It combines quantitative metrics (e.g., FVD, LPIPS, NDS) with human preference scores via WorldLens-26K. WorldLens-Agent (Qwen3-VL-8B fine-tuned) enables scalable, human-aligned auto-evaluation.",
        "Strengths": "Comprehensive, multi-dimensional evaluation covering visual realism, geometric consistency, physical plausibility, and functional reliability. Integrates human judgment for alignment. Scalable and interpretable auto-evaluation via WorldLens-Agent. Bridges objective metrics with subjective perception.",
        "Limitations": "Primarily focused on driving scenarios. Human preference dataset may reflect annotator bias. Evaluation agent inherits LLM limitations. Physical realism evaluation is open-ended, requiring new metrics for interactive/multimodal 4D.",
        "Datasets / Benchmarks": "WorldLens (benchmark), WorldLens-26K (human-annotated video dataset), nuScenes (for some evaluations), Kinetics (for I3D features).",
        "Results Summary": "No single world model excels universally. Models with strong textures often violate physics; geometry-stable ones lack behavioral fidelity. DiST-4D performs well in geometry/novel-view. Perceptual quality doesn't imply usability. Temporal consistency and dataset alignment are critical for task-specific effectiveness.",
        "Why It Matters": "Establishes a unified protocol to measure driving world model fidelity, beyond visual realism, to include physical reliability & functional safety. Essential for developing robust embodied AI, safer autonomous driving, and more trustworthy synthetic data generation.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (WorldLens Framework), Figure 2 (Evaluation Aspects), Table 1 (Benchmarking Results).",
        "arxiv_id": "http://arxiv.org/abs/2512.10958v1"
    },
    {
        "Title": "DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders",
        "Field & Subfield": "Computer Vision; Generative Models; Human-Computer Interaction",
        "Key Contributions": "- Lightweight, model-agnostic framework for interactive video diffusion previews.\n- Preserves full generation quality, supports rapid iteration, and is plug-and-play.\n- Generates rich multi-modal previews (RGB, albedo, depth, normals) via multi-branch, multi-loss predictor.\n- Enables novel interactive generation control via stochasticity renoising and latent steering.",
        "Methodology": "A model-agnostic, lightweight multi-branch decoder framework. Linear probing identifies early intrinsic emergence. A multi-loss predictor mitigates superposition. Variations are generated via stochastic renoising (diff. noise) & latent steering (gradient-based optimization).",
        "Strengths": "Provides fast (sub-second), multi-modal previews, enabling early termination & rapid iteration. Offers interactive control over generation. Addresses superposition artifacts. Preserves full base model quality & is plug-and-play. User study confirms high perceptual value.",
        "Limitations": "Scope limited to scene intrinsics; text prompts not explicitly considered. Steering can have failure cases due to out-of-distribution issues or limited model capacity. Higher resolution/coherence could be improved. Simplified steering methods.",
        "Datasets / Benchmarks": "Synthetic video dataset (1,000 videos, 40 categories) generated using DiffusionRenderer. Compared against x0-pred, Video Depth Anything, Diffusion Renderer, Linear Probing. PSNR, MSE, L1, LPIPS metrics used.",
        "Results Summary": "Achieves previews in <1s for 4s videos, outperforming baselines in PSNR for most channels (e.g., RGB 18.03 Ours vs 16.98 x0-pred at 10% steps) and being significantly more efficient. Multi-branch decoder reduces artifacts. User study favors DiffusionBrowser for content predictability, visual fidelity, & scene clarity (74-77% preference).",
        "Why It Matters": "Transforms opaque video diffusion into an interactive, controllable, and resource-efficient process. Allows users to guide generations, terminate unpromising outcomes early, and better understand diffusion's internal dynamics, crucial for practical deployment and advanced research.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (DiffusionBrowser overview), Figure 2 (Linear probing results), Figure 5 (MB decoder comparison), Table 1 (Baselines comparison), Table 3 (User study results).",
        "arxiv_id": "http://arxiv.org/abs/2512.13690v1"
    },
    {
        "Title": "MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives",
        "Field & Subfield": "Computer Vision, Video Generation",
        "Key Contributions": "- Proposes MemFlow for consistent and efficient long video generation.\n- Introduces Narrative Adaptive Memory (NAM) for dynamic retrieval of semantically-aligned context.\n- Develops Sparse Memory Activation (SMA) for relevance-gated memory filtering to balance efficiency and quality.\n- Achieves SOTA quality with real-time inference (18.7 FPS) on H100.",
        "Methodology": "MemFlow integrates Narrative Adaptive Memory (NAM) for dynamic context retrieval (semantic retrieval, redundant removal) and Sparse Memory Activation (SMA) for efficiency. It's built upon an autoregressive-diffusion framework, using a streaming long-tuning strategy with Self-Forcing and DMD loss.",
        "Strengths": "Maintains long-term consistency and narrative coherence, even with complex character/scene changes. Achieves state-of-the-art visual quality and semantic alignment. Balances memory efficiency and quality effectively. Compatible with existing streaming video generation models with KV cache.",
        "Limitations": "Incurs a slight speed reduction (7.9% vs. memory-free baseline, 8.6% vs. LongLive) due to memory updating/activation. Larger memory capacities in NAM can lead to performance instability or underperform baselines due to attention field imbalance.",
        "Datasets / Benchmarks": "Customized 100 narrative scripts (6x10s prompts) for multi-prompt generation. Evaluated using VBench-Long metrics for quality, consistency, and aesthetics. CLIP score for text alignment.",
        "Results Summary": "MemFlow achieved highest overall quality (85.02) and aesthetic score (61.07) for multi-prompt 60s videos. Demonstrated superior subject/background consistency and prompt adherence. Achieved 18.7 FPS inference on a single NVIDIA H100 while maintaining high quality.",
        "Why It Matters": "Addresses the critical challenge of maintaining content consistency in interactive long video generation, which is crucial for advancing creative and cinematic applications. Enables coherent narratives that adapt to dynamic prompts and scene transitions.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Qualitative comparison), Figure 2 (Overall Framework), Table 1 (Multi-prompt 60s comparison), Table 2 (Single-prompt 5s comparison), Table 3 (Memory mechanism ablation).",
        "arxiv_id": "2512.14699v1"
    },
    {
        "Title": "Spherical Leech Quantization for Visual Tokenization and Generation",
        "Field & Subfield": "Computer Vision, Image Quantization & Generation",
        "Key Contributions": "- Unified NPQ methods via lattice coding.\n- Introduced Spherical Leech Quantization (A24-SQ) based on Leech lattice.\n- Simplified autoencoder training without entropy regularization.\n- Achieved state-of-the-art visual tokenization/generation with large codebooks (~200K).",
        "Methodology": "A24-SQ uses fixed 24-dimensional Leech lattice vectors for quantization, mapped from a hypersphere. It's integrated into auto-encoders and autoregressive models using standard l1, GAN, LPIPS losses, eliminating entropy regularization. Efficiency maintained by tiling/JIT-compiling and VF loss for alignment.",
        "Strengths": "Simplified training (no regularization), high efficiency (fixed lattice, no gradient updates, memory/runtime efficient), improved rate-distortion tradeoff, better reconstruction & generation quality, and scalability to very large codebooks, achieving oracle-like performance.",
        "Limitations": "Factorized d-itwise prediction yields worse gFID and lower recall. Codebook usage imbalance with large codebooks requires specific training techniques (Z-loss, Dion optimizer). Future work includes larger-scale, text-conditioned visual generation.",
        "Datasets / Benchmarks": "ImageNet-1k, COCO2017 val, Kodak Lossless True Color Image Suite. Metrics: PSNR, SSIM, LPIPS, rFID, gFID, IS, Precision, Recall.",
        "Results Summary": "A24-SQ outperforms BSQ in reconstruction (rFID 0.83 vs 1.14, better PSNR/SSIM/LPIPS) using fewer bits. Achieved 1.82 FID for AR generation on ImageNet-1k, comparable to oracle (1.78 FID), with a 196,560-way codebook. Pushed precision-recall frontier closer to oracle.",
        "Why It Matters": "Addresses codebook scalability and principled design in visual tokenization. Enables high-quality image compression and generation with very large vocabularies, bridging the gap between vision and language models' vocabulary sizes for general-purpose visual AI.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Method overview & results), Figure 2 (dmin comparison of lattices), Table 4 (A24-SQ vs BSQ details), Table 5 (Reconstruction results), Table 7 (Generation results).",
        "arxiv_id": "http://arxiv.org/abs/2512.14697v1"
    },
    {
        "Title": "TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs",
        "Field & Subfield": "Multimodal AI, Video Understanding, Temporal Grounding",
        "Key Contributions": "• Exposed critical quality issues in existing VTG benchmarks. • Introduced TimeLens-Bench (re-annotated, high-quality eval suite). • Created TimeLens-100K (large-scale, high-quality training data). • Derived optimal algorithmic practices: interleaved textual encoding & thinking-free RLVR. • TimeLens models: SOTA VTG, surpasses GPT-5/Gemini-2.5-Flash.",
        "Methodology": "Systematic investigation on data quality (manual & automated re-annotation for benchmarks/training) & algorithmic design. Key insights: interleaved textual encoding for time; thinking-free RLVR with early stopping & difficulty-based sampling.",
        "Strengths": "Rigorous data overhaul created high-quality, reliable benchmarks & training data. Comprehensive algorithmic study yields practical insights. Achieves SOTA performance. All code, data, models are open-sourced.",
        "Limitations": "Paper focuses on establishing an 'essential baseline' rather than novel methods. Current work is not focused on reasoning-intensive VTG scenarios, which is left for future exploration.",
        "Datasets / Benchmarks": "TimeLens-Bench (re-annotated Charades-STA, ActivityNet Captions, QVHighlights). TimeLens-100K (large-scale, high-quality training dataset).",
        "Results Summary": "TimeLens models achieve SOTA VTG, outperforming open-source and surpassing proprietary models (GPT-5, Gemini-2.5-Flash) on TimeLens-Bench. Performance significantly improved by high-quality data & optimized algorithmic practices.",
        "Why It Matters": "Addresses a critical limitation of MLLMs in temporal awareness for video understanding. Provides robust tools and insights, offering a reliable foundation for future research and evaluation in this vital field.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Fig 2 (Data quality impact, performance gains), Table 1 (Main Results), Fig 5 (Timestamp encoding schemes), Table 3 (Training paradigms).",
        "arxiv_id": "http://arxiv.org/abs/2512.14698v1"
    },
    {
        "Title": "DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models",
        "Field & Subfield": "Artificial Intelligence, Vision-Language Models, Diffusion Models",
        "Key Contributions": "- Proposes DiffusionVL, translating any AR models into diffusion VLMs.\n- Achieves SOTA among diffusion VLMs with <5% of prior data.\n- Narrows performance gap with advanced AR-VLMs.\n- Introduces block-decoding for arbitrary-length generation & KV cache reuse.\n- Attains 2x inference speedup.",
        "Methodology": "Simple diffusion finetuning converts AR-VLMs directly. For AR-LMs, a two-stage process (connector pretraining, then diffusion finetuning) is used. Employs a block diffusion strategy for parallel decoding, arbitrary length generation, and efficient KV-cache reuse.",
        "Strengths": "Achieves SOTA among diffusion VLMs with significantly less training data (<5%). Provides 2x inference speedup over prior dVLMs. Supports arbitrary-length generation and efficient KV-cache reuse, previously absent. Enables feasible conversion from *any* AR model.",
        "Limitations": "Performance, while competitive with AR-VLMs, may still lag top-tier models in some specific aspects. There is a trade-off between smaller block size (slightly better performance) and poorer parallelism.",
        "Datasets / Benchmarks": "MMMU (Pro/Std/Val), MME (Cog/Perp), SeedBench (Image/Video), ChartQA, AI2D, MMBench, RealworldQA, Muirbench. Trained on LLaVA-Pretrain (580K) and LLaVA-Next (738K).",
        "Results Summary": "DiffusionVL achieves SOTA among diffusion VLMs (e.g., 34.4% gain on MMMU-Pro, 37.5% on MME) using <5% data, is competitive with advanced AR-VLMs, and achieves a 2x inference speedup.",
        "Why It Matters": "This work bridges the gap between autoregressive and diffusion paradigms for Vision Language Models, enabling the efficient development of high-performance dVLMs with superior parallel decoding, arbitrary-length generation, and KV cache reuse capabilities.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Performance Comparison), Figure 4 (Speed & Quality for Image Captioning), Tables 1 & 2 (Benchmark Performance Comparison of DiffusionVL vs. AR & other Diffusion Models).",
        "arxiv_id": "http://arxiv.org/abs/2512.15713v1"
    },
    {
        "Title": "In Pursuit of Pixel Supervision for Visual Pre-training",
        "Field & Subfield": "Computer Vision, Self-Supervised Learning, Masked Autoencoders (MAE)",
        "Key Contributions": "- Introduces \"Pixio,\" an enhanced MAE with a deeper decoder, larger mask blocks, and multiple [CLS] tokens.\n- Trains on 2B web-crawled images with a novel, soft self-curation strategy.\n- Achieves competitive or superior performance over SOTA DINOv3 on various dense prediction tasks.",
        "Methodology": "Pixio is an enhanced MAE for pixel-space self-supervised learning. It uses a deeper decoder, larger 4x4 patch mask blocks for challenging pretext, and 8 class tokens. Trained on 2B web images with reconstruction loss and soft self-curation based on reconstruction difficulty.",
        "Strengths": "Demonstrates pixel-space SSL's competitiveness, simplicity, stability, & efficiency. Outperforms or matches DINOv3 on many dense prediction and robot learning tasks. Uses minimally-curated web-scale data, reducing benchmark bias.",
        "Limitations": "Masking is an artificial distortion introducing biases. Fixed masking ratios/granularities can be suboptimal for diverse image complexities. Inferior on KITTI (driving) due to less domain-specific training data than DINOv3.",
        "Datasets / Benchmarks": "Pre-training: 2B web-crawled images (self-curated). Evaluation: NYUv2, KITTI, DIODE, Sintel, DA-2K (Depth); ScanNet++, ETH3D, TartanAirV2-WB (3D Recon); ADE20K, Pascal VOC, LoveDA (Semantic Seg); CortexBench (Robot Learning).",
        "Results Summary": "Pixio significantly outperforms original MAE. It surpasses DINOv3 on most depth estimation benchmarks (e.g., 0.268 RMSE vs 0.320 on NYUv2), 3D reconstruction, & robot learning, and is competitive in semantic segmentation.",
        "Why It Matters": "Validates pixel-space self-supervised learning as a powerful, scalable, and less biased alternative or complement to latent-space methods. Offers a promising path for generic visual representations, especially for future video SSL.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Visualizations of Pixio's understanding), Figure 2 (Pixio updates over MAE), Table 1 (Monocular Depth Estimation), Table 3 (Feed-Forward 3D Reconstruction), Table 5 (Robot Learning).",
        "arxiv_id": "http://arxiv.org/abs/2512.15715v1"
    },
    {
        "Title": "Spatia: Video Generation with Updatable Spatial Memory",
        "Field & Subfield": "Computer Vision & Graphics, Video Generation, 3D-Aware Synthesis, Long-Horizon Consistency",
        "Key Contributions": [
            "Explicit 3D scene point cloud as updatable spatial memory.",
            "Dynamic-static disentanglement for consistent videos with dynamic entities.",
            "Spatially consistent generation across multiple views.",
            "Explicit 3D-aware camera control & interactive editing."
        ],
        "Methodology": "Iterative generation: estimates initial 3D point cloud from image, generates video clips conditioned on memory, then updates memory via visual SLAM. Uses a multi-modal conditional diffusion transformer (Wan2.2) with ControlNet. Static entities are disentangled during point cloud estimation.",
        "Strengths": "Achieves robust long-term spatial/temporal consistency. Handles dynamic entities while preserving static scenes. Enables explicit camera control & 3D-aware interactive editing. Outperforms SOTA in consistency and visual quality metrics. Provides a geometrically grounded framework.",
        "Limitations": "Point cloud density (memory vs. fine-grained guidance tradeoff). Potential for error accumulation in extremely long sequences, though mitigated by iterative updates. Dependency on accurate 3D scene point cloud estimation and camera pose.",
        "Datasets / Benchmarks": [
            "Training: RealEstate (40K videos), SpatialVID (10K HD videos).",
            "Evaluation: WorldScore benchmark, RealEstate test set."
        ],
        "Results Summary": "Achieves SOTA on WorldScore (Avg Score 69.73) and RealEstate (PSNR 18.58, SSIM 0.646, LPIPS 0.254), significantly improving static scene consistency while maintaining dynamic entity quality. Superior memory consistency in closed-loop generation (Match Acc 0.698).",
        "Why It Matters": "Addresses a critical challenge in video generation: long-term consistency. Enables novel applications like interactive 3D editing and precise camera control, moving towards robust \"world models\" for AI with persistent memory and geometrically grounded, scalable frameworks.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Spatia Overview), Figure 2 (Training Pipeline), Table 1 (WorldScore Results), Table 3 (Memory Evaluation).",
        "arxiv_id": "http://arxiv.org/abs/2512.15716v1"
    },
    {
        "Title": "Generative Refocusing: Flexible Defocus Control from a Single Image",
        "Field & Subfield": "Computer Vision & Image Processing",
        "Key Contributions": [
            "Flexible refocusing pipeline accepts any focus state, offering control over focus plane, bokeh intensity, and aperture shape via two-stage decomposition.",
            "Semi-supervised framework connects synthetic & unpaired real bokeh images, learning genuine optical characteristics from EXIF metadata.",
            "Achieves top performance in defocus deblurring, bokeh synthesis, and refocusing; supports text-guided adjustments & custom aperture shapes."
        ],
        "Methodology": "Two-stage diffusion framework: DeblurNet recovers all-in-focus images using diffusion guided by initial deblurring predictions; BokehNet synthesizes controllable bokeh via semi-supervised training with synthetic paired data and real unpaired bokeh images (leveraging EXIF metadata).",
        "Strengths": "Flexible input (any focus state), comprehensive user control (focus plane, bokeh intensity, aperture shape), robust to depth-scale inaccuracies, captures authentic lens characteristics, outperforms SOTA across multiple benchmarks.",
        "Limitations": "Relies on monocular depth estimation (prone to mislocalized focal planes), complex aperture shapes require simulator data, occasional hallucination of incorrect details in severe blur.",
        "Datasets / Benchmarks": "DPDD, REALDOF (deblurring); LF-BOKEH (bokeh synthesis); LF-REFOCUS (refocusing); POINTLIGHT-1K (custom aperture shapes); REALBOKEH_3MP, EBB!, Flickr-collected unpaired bokeh.",
        "Results Summary": "Outperforms all baselines in defocus deblurring, bokeh synthesis, and refocusing across fidelity and perceptual metrics, demonstrating realistic blur gradients, accurate focus-plane placement, and natural scene consistency.",
        "Why It Matters": "Enables advanced post-capture depth-of-field control from a single image, democratizing professional photographic effects, and pushing boundaries in image restoration and generative AI with a novel semi-supervised approach.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Visualizing controls), Table 2 (Defocus deblurring benchmark), Table 3 (Bokeh synthesis benchmark), Table 4 (Refocusing benchmark).",
        "arxiv_id": "http://arxiv.org/abs/2512.16923v1"
    },
    {
        "Title": "Next-Embedding Prediction Makes Strong Vision Learners",
        "Field & Subfield": "Computer Vision & Self-supervised Learning",
        "Key Contributions": "• Introduces Next-Embedding Predictive Autoregression (NEPA) for vision, predicting future patch embeddings autoregressively.\n• Achieves strong SOTA performance on ImageNet-1K (83.8% ViT-B, 85.3% ViT-L) and ADE20K (48.3% ViT-B, 54.0% ViT-L).\n• Demonstrates a minimalist approach (no pixel reconstruction, discrete tokens, contrastive loss, separate decoders).\n• Shows autoregression in continuous embedding space is a simple, scalable, modality-agnostic SSL alternative.",
        "Methodology": "NEPA trains a Vision Transformer to autoregressively predict future patch embeddings in continuous space. It uses causal masking, a similarity-based loss with stop-gradient, and modern architecture stabilization (RoPE, LayerScale, SwiGLU, QK-Norm). No pixel decoder or discrete tokens are used.",
        "Strengths": "Architectural simplicity & scalability, competitive SOTA performance on classification & segmentation without complex heads or decoders. Learns semantically meaningful, long-range dependencies & object-level structure. Modality-agnostic potential.",
        "Limitations": "Poor performance under standard linear probing. Struggles with images requiring complex physical understanding (reflections, shading, small/overlapping objects). Potential for dataset biases like other large models.",
        "Datasets / Benchmarks": "Pretraining on ImageNet-1K (unlabeled). Evaluated on ImageNet-1K for classification and ADE20K for semantic segmentation.",
        "Results Summary": "NEPA-B achieved 83.8% top-1 accuracy on ImageNet-1K and 48.3% mIoU on ADE20K. NEPA-L achieved 85.3% top-1 accuracy on ImageNet-1K and 54.0% mIoU on ADE20K, outperforming/matching prior SOTA SSL methods.",
        "Why It Matters": "This work shows that the generative pretraining paradigm, successful in NLP, can be directly applied to vision by predicting embeddings. It offers a simpler, more scalable, and potentially modality-agnostic alternative to current visual self-supervised learning methods.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Table 1 (Ablation studies on NEPA components); Table 2 (ImageNet-1K classification comparison); Table 3 (ADE20K segmentation comparison); Figure 5 (Attention and embedding analysis visualizations).",
        "arxiv_id": "http://arxiv.org/abs/2512.16922v1"
    },
    {
        "Title": "The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text",
        "Field & Subfield": "Computer Vision & Generative AI: Video Generation & World Models",
        "Key Contributions": "• Multimodal framework (text, trajectories, ref. images) for controllable world events. • Novel data curation pipeline for trajectory-video-text triplets. • Spatial-Aware Weighted Cross-Attention for text-trajectory alignment. • Demonstrates object/scene consistency, physical plausibility, causal reasoning, & counterfactual generation.",
        "Methodology": "WorldCanvas builds on Wan2.2 I2V 14B. It uses a novel data curation pipeline (keypoint tracking, motion captioning, ref. image extraction) to generate trajectory-video-text triplets. Incorporates Trajectory Injection & Spatial-Aware Weighted Cross-Attention for fine-grained control and alignment. Trained using flow matching (L1 loss).",
        "Strengths": "Offers precise, fine-grained control (when, where, who, what) for video generation. Achieves strong temporal coherence, maintaining object identity & scene consistency across occlusions. Demonstrates physical plausibility, causal reasoning, & counterfactual generation capabilities.",
        "Limitations": "Fails in some challenging scenarios involving complex spatial transformations or logical reasoning, e.g., precise water level tracking during camera pans or maintaining object consistency during drastic camera rotations.",
        "Datasets / Benchmarks": "Curated dataset of 280k trajectory-video-text triplets for training. Evaluated on 100 image-trajectory pairs. Compared against Wan2.2 I2V, ATI, & Frame In-N-Out baselines.",
        "Results Summary": "Outperforms baselines (Wan2.2 I2V, ATI, Frame In-N-Out) in ObjMC, Appearance Rate, Subject/Background Consistency, & CLIP-T (Global/Local) metrics. Human evaluation confirms superiority in trajectory following, prompt adherence, text-trajectory alignment, reference fidelity, & overall video quality.",
        "Why It Matters": "Transforms world models from passive predictors to interactive, user-shaped simulators. Enables creation of complex, coherent, promptable world events with precise multimodal control, advancing controllable video generation and foundational AI research.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Fig 1 (examples), Fig 2 (architecture), Table 1 (quantitative results), Table S2 (human evaluation)",
        "arxiv_id": "http://arxiv.org/abs/2512.16924v1"
    },
    {
        "Title": "Interact2Ar: Full-Body Human-Human Interaction Generation via Autoregressive Diffusion Models",
        "Field & Subfield": "Computer Vision, Human Motion Generation",
        "Key Contributions": "- End-to-end text-conditioned autoregressive diffusion model for full-body human-human interactions.\n- Novel cooperative denoiser architecture with body-part-specialized heads for detailed hand motions.\n- Autoregressive pipeline with Mixed Memory for adaptive motion synthesis.\n- Extended evaluation pipeline with robust, specialized metrics.",
        "Methodology": "Text-conditioned autoregressive diffusion model with cooperative denoisers & body-part-specialized heads (trajectory, body, hands). Employs a novel Mixed Memory strategy for efficient short/long-term context during sequential motion generation. Trained on SMPL-X parameters with various kinematic losses.",
        "Strengths": "Achieves SOTA performance on Inter-X for full-body interaction generation, including detailed hands. Enables adaptive capabilities: temporal composition, real-time disturbance adaptation, & sequential multi-person interactions. Robust evaluation pipeline.",
        "Limitations": "Dataset constraints limit human diversity (e.g., body shapes, hand contact precision), affecting the precision of hand contacts. Future work needs to address body shape variations.",
        "Datasets / Benchmarks": "Inter-X dataset (11K full-body interactions, 40 actions, detailed textual descriptions). Evaluated against InterMask, InterGen, T2M benchmarks.",
        "Results Summary": "Outperforms previous SOTA methods (InterMask) on Inter-X across all metrics (R-Prec.↑, FID↓, PJ↑, AUJ↓) for full-body, body, & hands. Autoregressive version consistently outperforms non-autoregressive. User study confirms superior text alignment & hand realism.",
        "Why It Matters": "Addresses the challenging task of generating realistic, coherent, and adaptable human-human interactions with detailed motions, crucial for virtual agents, robotics, and creative applications. Enables dynamic, reactive motion synthesis beyond pre-planned sequences.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (overview), Figure 2 (architecture), Figure 3 (Mixed Memory), Table 2 (SOTA comparison), Figure 4 (User Study).",
        "arxiv_id": "http://arxiv.org/abs/2512.19692v1"
    },
    {
        "Title": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding",
        "Field & Subfield": "Computer Vision & Generative Models / Representation Learning",
        "Key Contributions": "- Introduces 'Prism Hypothesis' for spectral characteristics of semantic & pixel encoders.\n- Proposes Unified Autoencoding (UAE) with a frequency-band modulator.\n- Achieves SOTA reconstruction and semantic alignment in a unified latent space.\n- Provides diffusion-friendly latents for generation.",
        "Methodology": "Unified Autoencoding (UAE) learns a shared latent space by factorizing content into fundamental semantic (low-frequency) and residual pixel (high-frequency) bands using an innovative frequency-band modulator, guided by the Prism Hypothesis.",
        "Strengths": "SOTA reconstruction quality (PSNR, SSIM, rFID) and generative capability. Effectively unifies semantic abstraction and pixel fidelity. Robust to frequency band granularity. Preserves strong semantic discriminability.",
        "Limitations": "The paper does not explicitly state limitations of the proposed UAE model; it primarily focuses on its strengths and superior performance compared to existing methods.",
        "Datasets / Benchmarks": "ImageNet-1K, MS-COCO 2017",
        "Results Summary": "UAE achieves SOTA reconstruction (e.g., ImageNet PSNR 33.08, SSIM 0.94, rFID 0.16 with DINOv2-L) and competitive generative performance (gFID 1.68, IS 301.6). It matches RAE's 83.0% top-1 accuracy in linear probing semantic understanding.",
        "Why It Matters": "It unifies previously fragmented semantic and pixel representations into a single, compact latent space. This streamlines the development of foundation models for both understanding and generation, leading to more efficient and coherent multimodal AI.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Prism Hypothesis), Figure 2 (Frequency Energy Distribution), Table 1 (Reconstruction Quality), Table 3 (Linear Probing Accuracy)",
        "arxiv_id": "http://arxiv.org/abs/2512.19693v1"
    },
    {
        "Title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos",
        "Field & Subfield": "Artificial Intelligence & Multimodal Large Language Models (MLLMs)",
        "Key Contributions": "- Novel multi-agent architecture for long-video QA, coordinating master LLM, grounding agent, and vision agent.\n- Reward-driven reinforcement learning scheme for concise, step-wise reasoning in MLLMs.\n- Introduction of new episode-level long video datasets: LongTVQA and LongTVQA+.",
        "Methodology": "A master LLM coordinates a grounding agent (temporal localization) and a vision agent (visual info extraction). The master iteratively reasons using accumulated evidence. It's fine-tuned with GRPO using structural and answer rewards to promote efficient, multi-step, tool-augmented reasoning.",
        "Strengths": "Achieves superior accuracy on long-video QA, provides interpretable decision-making trajectories, effectively handles sparse information in long videos, and is model-agnostic. Ablations show grounding and vision agents are critical.",
        "Limitations": "Relies on provided subtitles (no raw audio/ASR). Grounding and vision modules are fixed during RL training (no joint optimization). Reward function is intentionally simple, with room for improvement.",
        "Datasets / Benchmarks": "LongTVQA and LongTVQA+ (episode-level aggregations from TVQA/TVQA+), designed for hour-scale video QA. Evaluated against closed-source (GPT-4o, Gemini 2.5 Pro, Grok) and open-source LLM baselines.",
        "Results Summary": "LongVideoAgent significantly outperforms non-agent baselines on LongTVQA/+. Agentic RL yields substantial gains, especially for open-source models (e.g., Qwen2.5-7B with RL matches GPT-5-mini). Grounding+vision improves accuracy by +10.5%.",
        "Why It Matters": "Addresses key challenges in long-form video understanding by enabling efficient, fine-grained temporal reasoning. Its multi-agent framework provides interpretability and enhances LLM capabilities for complex multimodal tasks, setting new SOTA benchmarks.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Multi-agent Reasoning Architecture), Table 2 (Performance on LongTVQA/LongTVQA+), Table 4 (Ablation Studies).",
        "arxiv_id": "http://arxiv.org/abs/2512.20618v1"
    },
    {
        "Title": "SemanticGen: Video Generation in Semantic Space",
        "Field & Subfield": "Computer Vision & Generative AI, Video Generation",
        "Key Contributions": "- Proposed SemanticGen: novel two-stage framework for video generation in compact semantic space.\n- Identified semantic encoder requirements & developed MLP-based semantic representation compression.\n- Demonstrated faster convergence, high-quality, and robust scalability to long video generation.",
        "Methodology": "Two-stage diffusion model framework. Stage 1: Diffusion model generates compact semantic video features via pre-trained semantic encoder (Qwen-2.5-VL) & MLP compression. Stage 2: Another diffusion model generates VAE latents conditioned on these features. Uses Swin attention for long videos.",
        "Strengths": "Faster convergence than VAE latent-space methods. Generates high-quality, long-form videos with superior temporal consistency & reduced drift. Computationally efficient, especially for long videos, by operating in a compact semantic space.",
        "Limitations": "Struggles to maintain fine-grained texture consistency in very long videos. Performance is constrained by semantic encoder's ability to capture high-frequency temporal information, especially at low sampling FPS.",
        "Datasets / Benchmarks": "Internal text-video pair dataset. Standard VBench & VBench-Long benchmarks for evaluation.",
        "Results Summary": "SemanticGen achieves comparable/superior performance to SOTA on short videos. Substantially outperforms baselines in long video consistency & temporal stability (lower drift) on VBench-Long. Semantic space compression accelerates convergence.",
        "Why It Matters": "It addresses key challenges in video generation: slow convergence and poor scalability to long videos. By planning globally in a compact semantic space, SemanticGen enables more efficient, higher-quality, and temporally consistent generation of both short and long videos.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Fig 1: Examples synthesized by SemanticGen.\nFig 2: Overview of SemanticGen vs. previous methods.\nFig 8 & Table 3: Semantic space compression ablation studies.\nFig 9: Ablation on representation space (semantic vs. VAE latent).",
        "arxiv_id": "http://arxiv.org/abs/2512.20619v1"
    },
    {
        "Title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs",
        "Field & Subfield": "AI, Multimodal Large Language Models (MLLMs), Spatial Reasoning",
        "Key Contributions": "• First capability-centric hierarchical benchmark (SpatialTree) for spatial intelligence in MLLMs. • Revealed structural transfer dynamics of spatial skills via SFT: L1 prerequisite for L4 agentic competence. • Identified reasoning-perception trade-off in RL; proposed auto-think strategy for consistent improvement.",
        "Methodology": "Proposed SpatialTree: 4-level cognitive hierarchy (Perception, Mental Mapping, Simulation, Agentic). Constructed SpatialTree-Bench from existing & new data (SpatialPlus) using expert models/LLM rephrasing. Evaluated 27 sub-abilities via SFT and Hierarchy-Aware Reinforcement Learning (RLVR) with auto-think strategy.",
        "Strengths": "First capability-centric benchmark. Systematic evaluation across diverse MLLMs and 27 sub-abilities. Revealed crucial cross-level transfer dynamics & reasoning-perception trade-off. Proposes an effective 'auto-think' RL strategy for consistent performance gains.",
        "Limitations": "Naive RL on low-level skills can overfit, leading to limited generalization. Uniform RL strategies struggle to balance diverse requirements from atomic perception to complex planning, hindering broad performance gains.",
        "Datasets / Benchmarks": "SpatialTree-Bench (primary, custom-built), SpatialPlus (new data for underrepresented abilities). Reorganized existing datasets: SUNRGBD, Hypersim, Matterport3D, ArkitScenes, LLaVA-Video, LLaVA-NeXT-Interleave, LLaVA-OneVision, EmbodiedBench, etc.",
        "Results Summary": "SpatialTree reveals hierarchical structure: L1 orthogonal, L2-L4 correlated. SFT showed negative transfer within L1 but strong cross-level transfer. Naive RL is unreliable; auto-think strategy consistently improves all levels by suppressing unnecessary deliberation for perception. Gemini2.5-Pro performed best.",
        "Why It Matters": "Provides a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs. Offers fundamental insights into how spatial intelligence emerges, composes, and transfers, paving the way for more capable and robust spatial AI agents.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (SpatialTree hierarchy), Figure 5 (Inter-Capability Dependencies heatmap), Table 1 (Overall Performance), Table 3 (RLVR Comparisons)",
        "arxiv_id": "http://arxiv.org/abs/2512.20617v1"
    },
    {
        "Title": "Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models",
        "Field & Subfield": "Computer Vision & Vision-Language Models; Ordinal Regression, Explainable AI",
        "Key Contributions": "• Introduced YearGuessr: largest open, global, multi-modal building age dataset (55k images, 1001-2024 CE); • Proposed YearCLIP: CLIP-based model with ordinal regression, GPS integration & reasoning prompts for explainable age prediction; • Exposed significant popularity bias in VLMs (up to +34% accuracy on popular buildings); • Provided comprehensive benchmark of 30+ models & novel popularity-aware metrics.",
        "Methodology": "Developed YearGuessr dataset from Wikipedia (55k images, GPS, text, page views). Framed building age as ordinal regression. Proposed YearCLIP, a CLIP-based model integrating GPS via zero-convolution and reasoning prompts for explainability. Evaluated with MAE, Interval Accuracy, and new popularity-aware metrics.",
        "Strengths": "First open, global, large-scale benchmark for building age. Addresses VLM memorization bias beyond true architectural understanding. YearCLIP offers explainable predictions with architectural rationales. Comprehensive evaluation across 30+ diverse models.",
        "Limitations": "Dataset is geographically & temporally skewed towards modern examples. Labels are based on original construction years, even for renovated/rebuilt buildings, introducing noise. Generalization to underrepresented regions/early styles is limited.",
        "Datasets / Benchmarks": "YearGuessr (new, 55,546 images, 157 countries, 1001-2024 CE, CC BY-SA 4.0). Compared against MyCD, CMAB, MTBF-33. Benchmarked CNNs, Transformers, CLIP-based, Closed VLMs (Gemini, Claude, Grok), and Open VLMs.",
        "Results Summary": "VLMs show significant popularity bias (e.g., Gemini2.0-flash +34.18% IA5 gain on popular buildings) due to memorization. YearCLIP reduces MAE to 39.52, outperforming baselines and improving fine-grained prediction and explainability. Closed-source VLMs generally dominate, but all models struggle with early-period buildings and regional disparities.",
        "Why It Matters": "Provides a crucial benchmark for automated building age estimation, vital for sustainability, heritage, and disaster assessment. Exposes a fundamental flaw (memorization bias) in state-of-the-art VLMs, pushing for more robust and generalizable AI.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (YearGuessr/YearCLIP overview); Table 2 (Main results: MAE, Interval Accuracy, Popularity Bias); Figure 6 (YearCLIP Explainable Predictions).",
        "arxiv_id": "http://arxiv.org/abs/2512.21337v1"
    },
    {
        "Title": "HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming",
        "Field & Subfield": "Computer Vision; High-Resolution Video Generation; Diffusion Models",
        "Key Contributions": "HiStream framework for efficient autoregressive high-resolution video generation via redundancy elimination. Includes Dual-Resolution Caching (spatial), Anchor-Guided Sliding Window (temporal), & Asymmetric Denoising (timestep). Achieves up to 107.5x faster 1080p video generation with SOTA quality.",
        "Methodology": "Autoregressive diffusion model on WAN-2.1 DiT. Employs Dual-Resolution Caching (low-res denoising, high-res refinement), Anchor-Guided Sliding Window (fixed-size chunking with keyframe/cache), and Asymmetric Denoising (fewer steps for subsequent chunks).",
        "Strengths": "Significantly reduces inference cost (up to 107.5x faster), maintains state-of-the-art visual quality & temporal consistency, practical for 1080p, scalable. User study & VBench show superior perceptual quality across metrics.",
        "Limitations": "VAE decoder is a new primary bottleneck (16.45s/81 frames on A100). High memory costs during distillation limited training to 1.3B student model, not full 1080p data supervision. Remaining flaws like lack of physical realism.",
        "Datasets / Benchmarks": "Evaluated on 1080p video generation benchmarks. Quantitative comparison using VBench metrics. User preference study. Based on Wan2.1-T2V-1.3B model.",
        "Results Summary": "HiStream: 76.2x faster (0.48s/frame) than Wan2.1 baseline (36.56s/frame), SOTA quality. HiStream+: 107.5x faster (0.34s/frame) than baseline, with compelling speed-quality trade-off. Achieves highest user preference.",
        "Why It Matters": "Makes high-resolution, high-fidelity video generation practical and scalable for digital media, film, and VR, overcoming current diffusion model bottlenecks. Provides a foundation for accessible cinematic content creation.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1: Per-Frame Denoising Latency & Higher Preference (speedup, user study). Table 1: Quantitative comparisons with baselines (speed, quality metrics). Figure 2: Pipeline details (core mechanisms).",
        "arxiv_id": "2512.21338v1"
    },
    {
        "Title": "OPTIMIZING DECODING PATHS IN MASKED DIFFUSION MODELS BY QUANTIFYING UNCERTAINTY",
        "Field & Subfield": "Natural Language Processing & Generative Models (Masked Diffusion Models, Decoding Strategies)",
        "Key Contributions": "- First to formalize MDM decoding order sensitivity as Path Uncertainty.\n- Introduced Denoising Entropy (State/Path Entropy) to quantify generative uncertainty.\n- Proposed E-BON (post-hoc selection) and E-SMC (real-time guidance) algorithms.\n- Theoretically justified Denoising Entropy as proxy for generation quality & loss.\n- Significantly improved MDM generation quality on various benchmarks.",
        "Methodology": "Introduced Denoising Entropy (State Entropy, Path Entropy) to quantify cumulative predictive uncertainty in Masked Diffusion Models (MDMs). Proposed two algorithms: E-BON for post-hoc path selection based on minimizing Path Entropy, and E-SMC for real-time guidance via weighted resampling using State Entropy.",
        "Strengths": "Provides a principled, computable internal signal (Denoising Entropy) for MDMs. Significantly improves generation quality, consistency, and accuracy across challenging tasks. Scalable to large models and complex reasoning/planning tasks. Turns MDM uncertainty into a key advantage.",
        "Limitations": "Finding a globally optimal decoding path remains intractable, with algorithms providing approximations. E-BON uses uniform computational budget. Greedy search variants may sacrifice generation diversity. Theoretical bounds show local errors accumulate over denoising steps.",
        "Datasets / Benchmarks": "OpenWebText (for PPL); Reasoning & Planning: GSM8K, MATH500, GPQA, Countdown, Sudoku; Code Generation: HumanEval, HumanEval+, MBPP, MBPP+.",
        "Results Summary": "Denoising Entropy correlated strongly with text quality (lower HDE = lower PPL). E-SMC consistently outperformed vanilla and E-BON, achieving significant accuracy gains (up to +5.9% on Countdown, +1.9% on GSM8K over PC-Sampler). Performance improved with increased particles and resampling frequency, demonstrating scalability.",
        "Why It Matters": "Addresses a critical, previously unquantified problem in MDMs: the sensitivity of output quality to decoding order. Introduces a foundational tool (Denoising Entropy) for understanding and controlling MDM generation uncertainty, paving the way for more calibrated decoding strategies and improved overall generative performance.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1: Quantifying Path Uncertainty in MDMs with Denoising Entropy.\nFigure 2: Overview of entropy-guided decoding algorithms (E-BON, E-SMC).\nTable 2: Accuracy (%) on LLADA models across reasoning & planning benchmarks.",
        "arxiv_id": "http://arxiv.org/abs/2512.21336v1"
    }
]