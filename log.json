[
    {
        "Title": "Beyond Simple Edits: Composed Video Retrieval with Dense Modifications",
        "Field & Subfield": "Computer Science > Computer Vision",
        "Key Contributions": "• Introduces Dense-WebVid-CoVR dataset with 1.6M samples & dense modification text. • Proposes a robust CoVR model with unified grounding encoder for input video, description, & modification text fusion. • Achieves SOTA on Dense-WebVid-CoVR (71.3% Recall@1), outperforming existing methods by 3.4%. • Shows improved performance on CoIR datasets (CIRR, FashionIQ) & Ego-CVR.",
        "Methodology": "Dense-WebVid-CoVR created using Gemini-Pro for video descriptions & GPT-40 for dense modification texts, with manual verification. The model uses ViT-L for vision, BLIP-pretrained text encoder, & a BLIP-2 based grounding text encoder for unified fusion of query video, description, & modification text via cross-attention, trained with contrastive loss.",
        "Strengths": "• Introduces a large-scale, high-quality dataset with dense modification texts (7x longer). • Novel unified fusion strategy for better multimodal alignment. • Achieves SOTA performance on multiple CoVR & CoIR benchmarks. • Enhanced fine-grained retrieval capturing subtle visual/temporal changes. • Robust quality control for dataset generation.",
        "Limitations": "• Minor inaccuracies (2-3%) in training set, claimed minimal impact. • Future work: multilingual CoVR for low-resource languages. • Future work: efficient techniques for processing very long videos.",
        "Datasets / Benchmarks": "Introduced: Dense-WebVid-CoVR (1.6M samples). Used/Compared against: WebVid-CoVR, EgoCVR, CIRR, FashionIQ.",
        "Results Summary": "Achieves SOTA on Dense-WebVid-CoVR with 71.3% Recall@1 (3.4% gain over prior SOTA). Consistently outperforms baselines on Ego-CVR, CIRR (56.30% R@1), and FashionIQ, demonstrating superior fine-grained retrieval accuracy across all settings.",
        "Why It Matters": "Advances composed video retrieval by providing a richer, contextually aware dataset & an effective model. Enables precise video retrieval based on subtle modifications, crucial for applications like video editing & media production. Dense modification texts are a key innovation.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Fig 1: Example CoVR triplets comparing modification texts. Fig 3: Proposed CoVR architecture. Table 1: Comparative analysis of CoVR benchmarks. Table 2: Main results on Dense-WebVid-CoVR. Fig 13: Detailed modification-text comparison."
    },
    {
        "Title": "COMPUTERRL: SCALING END-TO-END ONLINE REINFORCEMENT LEARNING FOR COMPUTER USE AGENTS",
        "Field & Subfield": "Artificial Intelligence > Reinforcement Learning",
        "Key Contributions": "- Introduces API-GUI paradigm for machine-oriented desktop interaction, unifying API calls and GUI actions.\n- Establishes a large-scale, distributed RL infrastructure supporting thousands of parallel virtual desktop environments for scalable training.\n- Proposes Entropulse, a novel training strategy alternating RL with supervised fine-tuning to mitigate entropy collapse and ensure sustained learning.\n- Achieves new state-of-the-art accuracy of 48.1% on the OSWorld benchmark for general desktop automation agents.",
        "Methodology": "COMPUTERRL combines a novel API-GUI paradigm unifying programmatic control and GUI interaction, a distributed RL infrastructure using Docker and gRPC for scalable parallel environments, and Entropulse, a training strategy that alternates RL with SFT to maintain exploration and prevent entropy collapse. It leverages LLMs for API construction and employs a step-level GRPO algorithm with rule-based verifiable rewards.",
        "Strengths": [
            "Novel API-GUI paradigm offers superior operational efficiency and generalization.",
            "Highly scalable distributed RL infrastructure supports thousands of parallel environments.",
            "Entropulse strategy ensures robust and sustained performance gains in extended RL training.",
            "Achieves state-of-the-art performance on a challenging real-world desktop automation benchmark (OSWorld).",
            "Significantly reduces steps required for task completion compared to baselines (1/3)."
        ],
        "Limitations": [
            "Errors observed in visual perception and multi-application coordination.",
            "Challenges with operational illusions and other miscellaneous errors.",
            "Genuine universality and adaptation to unfamiliar applications remain open questions.",
            "Long-horizon autonomy and complex, multi-step objectives are still being explored."
        ],
        "Datasets / Benchmarks": "Evaluated on the OSWorld benchmark and OSWorld-Verified benchmark. No new datasets were introduced by the paper.",
        "Results Summary": "COMPUTERRL-trained AutoGLM-OS-9B achieved 48.1% success on OSWorld, surpassing SOTA models like OpenAI CUA (42.9%), UI-TARS-1.5 (42.5%), and Claude Sonnet 4 (30.7%). The API-GUI paradigm showed a 134% improvement over GUI-only approaches. Entropulse increased average training rewards and improved learning efficiency by mitigating entropy collapse.",
        "Why It Matters": "This work addresses critical challenges in developing autonomous agents for complex digital workspaces, offering a scalable and robust framework for end-to-end online RL. By bridging the gap between machine and human-centric GUIs, it lays a foundational step towards truly intelligent desktop automation, improving efficiency and paving the way for more capable generalist agents.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1: Shows state-of-the-art success rates on OSWorld (1a) and COMPUTERRL training reward curves (1b), demonstrating Entropulse's benefits. Table 1: Provides a comprehensive comparison of AUTOGLM-OS performance against various proprietary and open models on OSWorld benchmarks."
    },
    {
        "Title": "Relational Visual Similarity",
        "Field & Subfield": "Computer Vision & Image Understanding. Focuses on a novel visual similarity metric leveraging Vision-Language Models for abstract relational reasoning, moving beyond surface attributes to capture human-like conceptual similarities.",
        "Key Contributions": [
            "Introduced 'relational visual similarity' as a new dimension.",
            "Curated a 114k {image-anonymous caption} dataset for relational training.",
            "Developed 'relsim,' a new metric for relational visual similarity.",
            "Demonstrated relsim's utility in image retrieval & generation.",
            "Analyzed relationship between attribute and relational similarity."
        ],
        "Methodology": "Proposed `relsim` by: 1) Filtering LAION-2B to select images with relational cues. 2) Generating anonymous captions (describing relational logic, not surface content) from image groups using a fine-tuned VLM. 3) Training `relsim` (Qwen2.5-VL-7B) with InfoNCE loss to align image/caption embeddings.",
        "Strengths": "Captures human-like abstract relational reasoning, a key missing dimension in visual AI. `relsim` significantly outperforms existing attribute-based metrics in recognizing deeper conceptual similarities, validated by user studies & GPT-40 evaluation. Enables novel image retrieval/generation applications.",
        "Limitations": "Anonymous caption dataset curated manually (532 groups), limiting scalability & prone to bias. VLMs can hallucinate captions. Difficulty in specifying user intent for multi-relational images. Expanding dataset/pipeline is a future direction.",
        "Datasets / Benchmarks": "New: Relational Dataset (114k {image, anonymous caption} pairs derived from LAION-2B). Baselines: LPIPS, DINO, CLIP, dreamsim (image-to-image); CLIP-T, Qwen-T (caption-based). Evaluation: GPT-40 automated judge & human user study.",
        "Results Summary": "`relsim` achieved a GPT score of 6.77 (vs. LPIPS 4.56, CLIP-I 5.91), indicating superior relational similarity detection. User studies showed consistent human preference for `relsim` (42.5-60.7%). Proprietary models excel in analogical generation (GPT40 relsim 0.82) over open-source models.",
        "Why It Matters": "Addresses a critical gap in AI's visual understanding by moving beyond surface attributes to capture abstract, relational logic, mirroring human cognition. This unlocks new possibilities for creative AI applications, enhancing image retrieval, and analogical content generation.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Illustrates relational vs. attribute similarity), Figure 2 (Overall Pipeline), Figure 6 (relsim performance vs. baselines), Figure 8 (User study results), Table 2 (Analogical image generation benchmarks).",
        "arxiv_id": "http://arxiv.org/abs/2512.07833v1"
    },
    {
        "Title": "ASTRA: General INTERACTIVE WORLD MODEL WITH AUTOREGRESSIVE DENOISING",
        "Field & Subfield": "Computer Vision, Generative Models, World Models",
        "Key Contributions": "Introduces Astra, an interactive general world model. Key contributions: Autoregressive denoising with temporal causal attention; Action-aware adapter for precise control; Noise-augmented history memory for consistency; Mixture of Action Experts for diverse modalities.",
        "Methodology": "Autoregressive denoising framework based on a pre-trained video diffusion backbone. Utilizes an Action-Aware Flow Transformer (AFT) with ACT-Adapter for action injection, a noise-as-mask strategy for history memory to reduce visual inertia, and a Mixture of Action Experts (MoAE) for multimodal actions.",
        "Strengths": "Achieves high visual fidelity, strong temporal consistency, and precise action responsiveness. Versatile across diverse tasks (robotics, AD, exploration) and generalizes well to out-of-domain scenes. Parameter-efficient design with low compute overhead.",
        "Limitations": "Inference efficiency is limited, requiring multiple denoising steps per frame. This makes real-time deployment challenging for latency-sensitive applications like online control or interactive robotics.",
        "Datasets / Benchmarks": "nuScenes, Sekai, SpatialVID, RT-1, Multi-Cam Video. Evaluated on Astra-Bench (custom benchmark) and CityWalker for out-of-domain generalization.",
        "Results Summary": "Outperforms SOTA models (Wan-2.1, MatrixGame, YUME) across all metrics: instruction following, subject/background consistency, motion smoothness, and visual quality. Achieves lower rotation/translation errors and superior action-following accuracy on diverse benchmarks.",
        "Why It Matters": "Astra advances interactive world modeling, enabling more general, scalable simulators for exploration, robotics, autonomous driving, and embodied intelligence, bridging the gap to true interactive world simulation.",
        "Should Read Fully?": "Yes, for researchers and practitioners interested in interactive world models, diffusion-based video generation, robotics, autonomous driving, or embodied AI.",
        "Key Figures or Tables": "Fig. 1 (diverse applications), Fig. 3 (Astra framework), Fig. 5 & 6 (qualitative results), Table 2 (SOTA comparison), Table 3 (ablation studies).",
        "arxiv_id": "http://arxiv.org/abs/2512.08931v1"
    },
    {
        "Title": "Efficiently Reconstructing Dynamic Scenes One D4RT at a Time",
        "Field & Subfield": "Computer Vision; 4D Reconstruction & Tracking",
        "Key Contributions": "Novel query-based feedforward model for efficient 4D scene reconstruction. Unified transformer architecture for depth, correspondence, point clouds, camera params. SOTA accuracy & speed in dynamic 4D reconstruction & tracking. Enables efficient dense, holistic scene reconstruction.",
        "Methodology": "D4RT uses an encoder (ViT-g) to create a global scene representation (F) from video. A lightweight cross-attention decoder queries F with spatio-temporal parameters (u,v, t_src, t_tgt, t_cam) and local RGB patch to predict 3D point positions.",
        "Strengths": "Highly efficient (18-300x faster, 200+ FPS), scalable, unified interface for multiple 4D tasks, SOTA accuracy across diverse benchmarks, handles dynamic scenes, supports subpixel precision & high-res decoding with novel querying.",
        "Limitations": "No explicit limitations for D4RT are stated, as the paper focuses on overcoming prior methods' drawbacks. Implicitly, a large encoder requires pre-training. Dense inference still has computational cost, partially addressed by occupancy grid optimization.",
        "Datasets / Benchmarks": "BlendedMVS, Co3Dv2, Dynamic Replica, Kubric, MVS-Synth, PointOdyssey, ScanNet, ScanNet++, Tartanair, VirtualKitti, Waymo Open (training). Sintel, ScanNet, KITTI, Bonn, TapVid-3D (evaluation).",
        "Results Summary": "D4RT sets SOTA across 4D tasks (depth, point cloud, 3D tracking, camera pose) on various benchmarks. Achieves significantly higher throughput (e.g., 200+ FPS, 100x faster than MegaSaM) while delivering superior accuracy. Generalizes to long videos & high-res decoding.",
        "Why It Matters": "Offers a unified, efficient, and highly scalable framework for 4D reconstruction, overcoming fragmentation and computational bottlenecks of prior methods, paving the way for next-generation 4D perception in complex dynamic environments.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Fig. 1 (Model overview), Fig. 3 (Pose accuracy vs. speed), Fig. 4 (Qualitative reconstruction), Table 3 (3D tracking throughput), Table 4 (4D tracking metrics).",
        "arxiv_id": "http://arxiv.org/abs/2512.08924v1"
    },
    {
        "Title": "Selfi: Self Improving Reconstruction Engine via 3D Geometric Feature Alignment",
        "Field & Subfield": "Computer Vision & 3D Reconstruction, Novel View Synthesis",
        "Key Contributions": "• Self-improving geometric feature learning from unposed RGB via reprojection-based consistency loss. • Achieves state-of-the-art NVS and pose estimation using a frozen VFM backbone. • Enhances NVS quality and pose accuracy via dense bundle adjustment with depth shift.",
        "Methodology": "Selfi uses a pre-trained VGGT (3D VFM) as a backbone. A DPT adapter learns geometrically-aligned features using a self-supervised reprojection-based consistency loss with pseudo-ground-truth from VGGT outputs. These features predict 3D Gaussian parameters via a U-Net decoder. Poses are refined using dense bundle adjustment, with Gaussian centers adjusted via an affine depth shift.",
        "Strengths": "State-of-the-art NVS and pose estimation from unposed, uncalibrated images. Achieves high-fidelity rendering, including thin structures and fine details. Robust to varying input views/overlap. Self-supervised, efficiently leveraging VFMs.",
        "Limitations": "Relies on VGGT's normalized scale, leading to depth inaccuracies in distant regions (e.g., sky). Sensitive to exposure discrepancies between input and target images. Currently restricted to static scenes, failing on dynamic environments.",
        "Datasets / Benchmarks": "Trained on DL3DV and RealEstate10K. Evaluated on DL3DV, RealEstate10K, MipNeRF, Tanks&Temples, and RayZer split. Metrics include PSNR, SSIM, LPIPS for NVS; AUC@3, AUC@5, AUC@15 for pose estimation.",
        "Results Summary": "Selfi consistently outperforms existing pose-free feed-forward Gaussian methods (e.g., AnySplat, WorldMirror) and achieves performance comparable to/exceeding 3DGS with ground-truth poses. Achieves SOTA on NVS & pose estimation benchmarks across various input settings (varying sequence lengths, overlap).",
        "Why It Matters": "Selfi advances novel view synthesis and 3D reconstruction by bypassing the fragile and computationally intensive SfM pipeline. It enables robust, high-fidelity 3D scene understanding from unposed images, making 3D VFMs more practical for real-world applications without explicit 3D ground truth.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Selfi pipeline overview), Figure 3 & 5 (Qualitative NVS comparisons), Table 1 & 2 (NVS performance with varying inputs/overlap), Table 5 (BA for NVS/Pose).",
        "arxiv_id": "http://arxiv.org/abs/2512.08930v1"
    },
    {
        "Title": "Closing the Train-Test Gap in World Models for Gradient-Based Planning",
        "Field & Subfield": "Reinforcement Learning & Robotics; Model-Based Planning",
        "Key Contributions": "- Proposes Online World Modeling (OWM) to expand WM's reliable state space via simulator-corrected trajectories.\n- Introduces Adversarial World Modeling (AWM) to smooth loss landscape & promote robustness by finetuning on perturbed inputs.\n- Achieves CEM-level performance with 10x speedup for gradient-based planning.",
        "Methodology": "Two finetuning methods (OWM, AWM) for latent world models (DINO-WM) used in gradient-based planning (GBP). OWM generates and finetunes on simulator-corrected GBP trajectories. AWM perturbs inputs to maximize prediction error, smoothing the planning loss landscape.",
        "Strengths": "Significantly improves gradient-based planning reliability and performance, often matching or exceeding CEM. Achieves a 10x computational speedup over CEM. Smooths the planning loss landscape, reducing local minima. Narrows the train-test gap in world model error.",
        "Limitations": "Online World Modeling relies on access to an environment simulator, which might be costly or infeasible in real-world settings. Performance benefits vary by task and specific gradient optimizer used (e.g., Adam generally outperforms GD).",
        "Datasets / Benchmarks": "PushT, PointMaze, Wall (main experiments); Rope, Granular (additional robotic manipulation tasks). Utilizes existing DINO-WM datasets and architectures.",
        "Results Summary": "AWM with Adam GBP outperforms or matches CEM on PushT, PointMaze, Wall tasks, achieving up to +30% success rate increase over DINO-WM (open-loop) and up to 10x faster execution. Both OWM and AWM narrow the train-test gap.",
        "Why It Matters": "Enables practical and efficient gradient-based planning with world models, a critical step for real-world robotics. Addresses a fundamental train-test gap, making learned dynamics models more reliable for long-horizon control.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Table 1 (Planning Results), Figure 2 (Optimization landscape of DINO-WM vs. AWM), Figure 3 (Planning efficiency)",
        "arxiv_id": "2512.09929v1"
    },
    {
        "Title": "GAINS: Gaussian-based Inverse Rendering from Sparse Multi-View Captures",
        "Field & Subfield": "Computer Vision & Graphics, Inverse Rendering",
        "Key Contributions": "- Two-stage inverse rendering for sparse multi-view captures.\n- Synergizes learning-based priors (depth/normal, diffusion) for robust geometry.\n- Combines segmentation, IID, and diffusion priors for stable material recovery.\n- Achieves SOTA relighting, NVS, and material accuracy on sparse inputs.",
        "Methodology": "Two-stage pipeline using 2DGS. Stage I: Geometry refinement with monocular depth/normal and diffusion priors. Stage II: Material properties & lighting estimation using segmentation, intrinsic image decomposition (IID), and diffusion priors, all combined with PBR.",
        "Strengths": "Significantly improves material parameter accuracy, relighting quality, and novel-view synthesis, especially under sparse-view conditions. More robust against overfitting and ambiguity than prior methods. Better intrinsic separation and reduced reflection baking.",
        "Limitations": "Segmentation guidance can trade off robustness for high-frequency detail under dense supervision. IID prior can exhibit view-inconsistency, though mitigated by a weighted loss.",
        "Datasets / Benchmarks": "Synthetic4Relight [44], TensorIR [10], Ref-Real [29]. Evaluated using PSNR, SSIM, LPIPS for NVS/Albedo/Relighting, and MSE for Roughness.",
        "Results Summary": "GAINS consistently outperforms SOTA Gaussian-based IR methods (Ref-GS, GI-GS) on all datasets and metrics, especially with sparse (4-8) views. Achieves higher PSNR, SSIM, lower LPIPS/MSE for NVS, albedo, roughness, and relighting.",
        "Why It Matters": "Addresses the challenging problem of inverse rendering from sparse inputs, which is critical for real-world applications where dense multi-view captures are impractical or impossible, enabling more robust 3D scene understanding.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Table 1, Table 2 (Quantitative evaluation across datasets), Table 3 (Ablation study of priors), Figure 1 (Qualitative intrinsics/relighting), Figure 6 (View-dependent performance)."
    },
    {
        "Title": "ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning",
        "Field & Subfield": "Computer Vision & Generative AI: Video Editing",
        "Key Contributions": "- Introduces Reason-Informed Video Editing (RVE) task.\n- Proposes RVE-Bench, a comprehensive benchmark for RVE.\n- Develops ReViSE, a Self-Reflective Reasoning (SRF) framework.\n- Leverages internal VLM for intrinsic, differential feedback.\n- Achieves SOTA on RVE-Bench.",
        "Methodology": "ReViSE employs a Self-Reflective Reasoning (SRF) framework, unifying generation & internal evaluation. An internal VLM acts as a critic, providing differential feedback. Training uses Unified Semantic Optimization (USO) & Reward Weighted Optimization (RWO) with flow-matching loss.",
        "Strengths": "Bridges the reasoning-editing gap in video models, provides self-supervised intrinsic feedback, significantly improves reasoning accuracy & visual fidelity, and introduces a robust, reasoning-aware evaluation framework using GPT-4o.",
        "Limitations": "Performance is primarily constrained by the capabilities of the base models. Cannot fully exploit potential without access to more powerful foundational models due to resource limitations.",
        "Datasets / Benchmarks": "RVE-Bench (new, 1,000 unique triplets), comprising two subsets: Reasoning-Informed Video Editing (809 samples) & In-Context Video Generation (200 samples). Leverages Ditto-1M and collected movie data.",
        "Results Summary": "ReViSE significantly enhances editing accuracy and visual fidelity on RVE-Bench, achieving a 32% improvement in Overall score on the reasoning-informed video editing subset over state-of-the-art methods across all reasoning categories.",
        "Why It Matters": "Addresses a crucial gap in video editing by enabling models to understand & incorporate complex reasoning (physical plausibility, causal dynamics), moving beyond literal transformations to generate more realistic, logically coherent, and contextually rich videos.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (ReViSE framework overview), Figure 2 (RVE-Bench overview & statistics), Table 1 (Quantitative Results on RVE-Bench).",
        "arxiv_id": "http://arxiv.org/abs/2512.09924v1"
    },
    {
        "Title": "SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model",
        "Field & Subfield": "Computer Vision & 3D Scene Generation",
        "Key Contributions": "- Decoupled 3D scene generation framework for better open-set prior learning.\n- Robust de-occlusion model leveraging image datasets & a new 10K curated dataset.\n- Unified pose estimation diffusion model with global/local attention & new 200K synthetic scene dataset for generalization.",
        "Methodology": "Decoupled 3D scene generation into de-occlusion, 3D object generation, and pose estimation. De-occlusion model finetunes Flux Kontext on a 10K image dataset. Pose estimation uses a diffusion model with global/local self/cross-attention, trained on a 200K synthetic scene dataset, predicting 6D pose & size.",
        "Strengths": "Achieves SOTA on indoor & open-set scenes for geometry quality & pose accuracy. Robust under severe occlusion & diverse objects. Strong generalization. Enables text-controllable de-occlusion, enhancing quality & controllability.",
        "Limitations": "Real-world object arrangements (e.g., force interactions, interpenetration) are not fully captured by datasets. Existing methods have limited control signals (images/simple captions). Deeper understanding for embodied AI tasks remains an unsolved challenge.",
        "Datasets / Benchmarks": "New 10K object image de-occlusion dataset, new 200K synthetic open-set scene dataset. Trained on Objaverse. Evaluated against MIDI, 3D-Front, and other SOTA methods.",
        "Results Summary": "Outperforms SOTA methods on indoor & open-set scenes in both geometry quality (CD-S, F-Score-S, IoU-B) and pose accuracy, and de-occlusion metrics (PSNR, SSIM, CLIP). Demonstrates superior performance, especially under severe occlusion.",
        "Why It Matters": "Advances realistic open-set 3D scene generation, addressing crucial limitations in occlusion handling and accurate pose estimation. Essential for progress in AIGC, embodied AI, simulation environment construction, and 3D perception.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (SceneMaker outputs), Figure 3 (Framework), Table 3 (Quantitative comparison on indoor & open-set), Figure 6 (De-occlusion comparison).",
        "arxiv_id": "http://arxiv.org/abs/2512.10957v1"
    },
    {
        "Title": "StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space",
        "Field & Subfield": "Computer Vision & Novel View Synthesis / Stereo Generation",
        "Key Contributions": "- Depth-free monocular-to-stereo synthesis via end-to-end diffusion in a canonical space.\n- Viewpoint-conditioned framework using Plücker ray embeddings for metric baseline control.\n- Novel evaluation protocol with iSQoE (perceptual comfort) and MEt3R (geometric consistency).",
        "Methodology": "Diffusion-based framework (LDM) using a dual U-Net architecture (initialized from Stable Diffusion 2.0). Models geometry via viewpoint conditioning with Plücker ray embeddings in a canonical rectified space, avoiding explicit depth or warping.",
        "Strengths": "Depth-free handling of complex multi-layer and non-Lambertian scenes. Strong cross-baseline generalization with metric control. Superior perceptual comfort (iSQoE) and geometric consistency (MEt3R) compared to baselines.",
        "Limitations": "All methods, including StereoSpace, still exhibit degradation on extreme multi-layer/transparent scenes. Auxiliary disparity loss slightly degrades iSQoE scores.",
        "Datasets / Benchmarks": "Training: TartanAir, Dynamic Replica, IRS, Falling Things, LayeredFlow, NeRF-Stereo, SceneSplat-7K. Evaluation: Middlebury 2014, DrivingStereo, Booster, LayeredFlow.",
        "Results Summary": "Outperforms prior warp/inpaint, latent-warping, and warped-conditioning methods on iSQoE and MEt3R metrics. Achieves best perceptual comfort and geometric consistency, particularly on complex multi-layer geometries.",
        "Why It Matters": "Enables high-quality stereo image generation from monocular inputs without costly 3D reconstruction, reducing production costs and facilitating 2D to 3D content conversion for immersive media.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Framework Overview), Table 2 (Main Results on Middlebury & DrivingStereo)",
        "arxiv_id": "http://arxiv.org/abs/2512.10959v1"
    },
    {
        "Title": "WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World",
        "Field & Subfield": "Embodied AI & Computer Vision; Generative World Models",
        "Key Contributions": "- Introduces WorldLens, a benchmark for driving world models.\n- Defines 5 evaluation aspects: Generation, Reconstruction, Action-Following, Downstream Task, Human Preference.\n- Curates WorldLens-26K, a large-scale human-annotated video dataset.\n- Develops WorldLens-Agent, an auto-evaluator distilled from human preferences.",
        "Methodology": "WorldLens evaluates driving world models across 5 aspects using 20+ metrics. It combines quantitative metrics (e.g., FVD, LPIPS, NDS) with human preference scores via WorldLens-26K. WorldLens-Agent (Qwen3-VL-8B fine-tuned) enables scalable, human-aligned auto-evaluation.",
        "Strengths": "Comprehensive, multi-dimensional evaluation covering visual realism, geometric consistency, physical plausibility, and functional reliability. Integrates human judgment for alignment. Scalable and interpretable auto-evaluation via WorldLens-Agent. Bridges objective metrics with subjective perception.",
        "Limitations": "Primarily focused on driving scenarios. Human preference dataset may reflect annotator bias. Evaluation agent inherits LLM limitations. Physical realism evaluation is open-ended, requiring new metrics for interactive/multimodal 4D.",
        "Datasets / Benchmarks": "WorldLens (benchmark), WorldLens-26K (human-annotated video dataset), nuScenes (for some evaluations), Kinetics (for I3D features).",
        "Results Summary": "No single world model excels universally. Models with strong textures often violate physics; geometry-stable ones lack behavioral fidelity. DiST-4D performs well in geometry/novel-view. Perceptual quality doesn't imply usability. Temporal consistency and dataset alignment are critical for task-specific effectiveness.",
        "Why It Matters": "Establishes a unified protocol to measure driving world model fidelity, beyond visual realism, to include physical reliability & functional safety. Essential for developing robust embodied AI, safer autonomous driving, and more trustworthy synthetic data generation.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (WorldLens Framework), Figure 2 (Evaluation Aspects), Table 1 (Benchmarking Results).",
        "arxiv_id": "http://arxiv.org/abs/2512.10958v1"
    },
    {
        "Title": "DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders",
        "Field & Subfield": "Computer Vision; Generative Models; Human-Computer Interaction",
        "Key Contributions": "- Lightweight, model-agnostic framework for interactive video diffusion previews.\n- Preserves full generation quality, supports rapid iteration, and is plug-and-play.\n- Generates rich multi-modal previews (RGB, albedo, depth, normals) via multi-branch, multi-loss predictor.\n- Enables novel interactive generation control via stochasticity renoising and latent steering.",
        "Methodology": "A model-agnostic, lightweight multi-branch decoder framework. Linear probing identifies early intrinsic emergence. A multi-loss predictor mitigates superposition. Variations are generated via stochastic renoising (diff. noise) & latent steering (gradient-based optimization).",
        "Strengths": "Provides fast (sub-second), multi-modal previews, enabling early termination & rapid iteration. Offers interactive control over generation. Addresses superposition artifacts. Preserves full base model quality & is plug-and-play. User study confirms high perceptual value.",
        "Limitations": "Scope limited to scene intrinsics; text prompts not explicitly considered. Steering can have failure cases due to out-of-distribution issues or limited model capacity. Higher resolution/coherence could be improved. Simplified steering methods.",
        "Datasets / Benchmarks": "Synthetic video dataset (1,000 videos, 40 categories) generated using DiffusionRenderer. Compared against x0-pred, Video Depth Anything, Diffusion Renderer, Linear Probing. PSNR, MSE, L1, LPIPS metrics used.",
        "Results Summary": "Achieves previews in <1s for 4s videos, outperforming baselines in PSNR for most channels (e.g., RGB 18.03 Ours vs 16.98 x0-pred at 10% steps) and being significantly more efficient. Multi-branch decoder reduces artifacts. User study favors DiffusionBrowser for content predictability, visual fidelity, & scene clarity (74-77% preference).",
        "Why It Matters": "Transforms opaque video diffusion into an interactive, controllable, and resource-efficient process. Allows users to guide generations, terminate unpromising outcomes early, and better understand diffusion's internal dynamics, crucial for practical deployment and advanced research.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (DiffusionBrowser overview), Figure 2 (Linear probing results), Figure 5 (MB decoder comparison), Table 1 (Baselines comparison), Table 3 (User study results).",
        "arxiv_id": "http://arxiv.org/abs/2512.13690v1"
    },
    {
        "Title": "MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives",
        "Field & Subfield": "Computer Vision, Video Generation",
        "Key Contributions": "- Proposes MemFlow for consistent and efficient long video generation.\n- Introduces Narrative Adaptive Memory (NAM) for dynamic retrieval of semantically-aligned context.\n- Develops Sparse Memory Activation (SMA) for relevance-gated memory filtering to balance efficiency and quality.\n- Achieves SOTA quality with real-time inference (18.7 FPS) on H100.",
        "Methodology": "MemFlow integrates Narrative Adaptive Memory (NAM) for dynamic context retrieval (semantic retrieval, redundant removal) and Sparse Memory Activation (SMA) for efficiency. It's built upon an autoregressive-diffusion framework, using a streaming long-tuning strategy with Self-Forcing and DMD loss.",
        "Strengths": "Maintains long-term consistency and narrative coherence, even with complex character/scene changes. Achieves state-of-the-art visual quality and semantic alignment. Balances memory efficiency and quality effectively. Compatible with existing streaming video generation models with KV cache.",
        "Limitations": "Incurs a slight speed reduction (7.9% vs. memory-free baseline, 8.6% vs. LongLive) due to memory updating/activation. Larger memory capacities in NAM can lead to performance instability or underperform baselines due to attention field imbalance.",
        "Datasets / Benchmarks": "Customized 100 narrative scripts (6x10s prompts) for multi-prompt generation. Evaluated using VBench-Long metrics for quality, consistency, and aesthetics. CLIP score for text alignment.",
        "Results Summary": "MemFlow achieved highest overall quality (85.02) and aesthetic score (61.07) for multi-prompt 60s videos. Demonstrated superior subject/background consistency and prompt adherence. Achieved 18.7 FPS inference on a single NVIDIA H100 while maintaining high quality.",
        "Why It Matters": "Addresses the critical challenge of maintaining content consistency in interactive long video generation, which is crucial for advancing creative and cinematic applications. Enables coherent narratives that adapt to dynamic prompts and scene transitions.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Qualitative comparison), Figure 2 (Overall Framework), Table 1 (Multi-prompt 60s comparison), Table 2 (Single-prompt 5s comparison), Table 3 (Memory mechanism ablation).",
        "arxiv_id": "2512.14699v1"
    },
    {
        "Title": "Spherical Leech Quantization for Visual Tokenization and Generation",
        "Field & Subfield": "Computer Vision, Image Quantization & Generation",
        "Key Contributions": "- Unified NPQ methods via lattice coding.\n- Introduced Spherical Leech Quantization (A24-SQ) based on Leech lattice.\n- Simplified autoencoder training without entropy regularization.\n- Achieved state-of-the-art visual tokenization/generation with large codebooks (~200K).",
        "Methodology": "A24-SQ uses fixed 24-dimensional Leech lattice vectors for quantization, mapped from a hypersphere. It's integrated into auto-encoders and autoregressive models using standard l1, GAN, LPIPS losses, eliminating entropy regularization. Efficiency maintained by tiling/JIT-compiling and VF loss for alignment.",
        "Strengths": "Simplified training (no regularization), high efficiency (fixed lattice, no gradient updates, memory/runtime efficient), improved rate-distortion tradeoff, better reconstruction & generation quality, and scalability to very large codebooks, achieving oracle-like performance.",
        "Limitations": "Factorized d-itwise prediction yields worse gFID and lower recall. Codebook usage imbalance with large codebooks requires specific training techniques (Z-loss, Dion optimizer). Future work includes larger-scale, text-conditioned visual generation.",
        "Datasets / Benchmarks": "ImageNet-1k, COCO2017 val, Kodak Lossless True Color Image Suite. Metrics: PSNR, SSIM, LPIPS, rFID, gFID, IS, Precision, Recall.",
        "Results Summary": "A24-SQ outperforms BSQ in reconstruction (rFID 0.83 vs 1.14, better PSNR/SSIM/LPIPS) using fewer bits. Achieved 1.82 FID for AR generation on ImageNet-1k, comparable to oracle (1.78 FID), with a 196,560-way codebook. Pushed precision-recall frontier closer to oracle.",
        "Why It Matters": "Addresses codebook scalability and principled design in visual tokenization. Enables high-quality image compression and generation with very large vocabularies, bridging the gap between vision and language models' vocabulary sizes for general-purpose visual AI.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Method overview & results), Figure 2 (dmin comparison of lattices), Table 4 (A24-SQ vs BSQ details), Table 5 (Reconstruction results), Table 7 (Generation results).",
        "arxiv_id": "http://arxiv.org/abs/2512.14697v1"
    },
    {
        "Title": "TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs",
        "Field & Subfield": "Multimodal AI, Video Understanding, Temporal Grounding",
        "Key Contributions": "• Exposed critical quality issues in existing VTG benchmarks. • Introduced TimeLens-Bench (re-annotated, high-quality eval suite). • Created TimeLens-100K (large-scale, high-quality training data). • Derived optimal algorithmic practices: interleaved textual encoding & thinking-free RLVR. • TimeLens models: SOTA VTG, surpasses GPT-5/Gemini-2.5-Flash.",
        "Methodology": "Systematic investigation on data quality (manual & automated re-annotation for benchmarks/training) & algorithmic design. Key insights: interleaved textual encoding for time; thinking-free RLVR with early stopping & difficulty-based sampling.",
        "Strengths": "Rigorous data overhaul created high-quality, reliable benchmarks & training data. Comprehensive algorithmic study yields practical insights. Achieves SOTA performance. All code, data, models are open-sourced.",
        "Limitations": "Paper focuses on establishing an 'essential baseline' rather than novel methods. Current work is not focused on reasoning-intensive VTG scenarios, which is left for future exploration.",
        "Datasets / Benchmarks": "TimeLens-Bench (re-annotated Charades-STA, ActivityNet Captions, QVHighlights). TimeLens-100K (large-scale, high-quality training dataset).",
        "Results Summary": "TimeLens models achieve SOTA VTG, outperforming open-source and surpassing proprietary models (GPT-5, Gemini-2.5-Flash) on TimeLens-Bench. Performance significantly improved by high-quality data & optimized algorithmic practices.",
        "Why It Matters": "Addresses a critical limitation of MLLMs in temporal awareness for video understanding. Provides robust tools and insights, offering a reliable foundation for future research and evaluation in this vital field.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Fig 2 (Data quality impact, performance gains), Table 1 (Main Results), Fig 5 (Timestamp encoding schemes), Table 3 (Training paradigms).",
        "arxiv_id": "http://arxiv.org/abs/2512.14698v1"
    },
    {
        "Title": "DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models",
        "Field & Subfield": "Artificial Intelligence, Vision-Language Models, Diffusion Models",
        "Key Contributions": "- Proposes DiffusionVL, translating any AR models into diffusion VLMs.\n- Achieves SOTA among diffusion VLMs with <5% of prior data.\n- Narrows performance gap with advanced AR-VLMs.\n- Introduces block-decoding for arbitrary-length generation & KV cache reuse.\n- Attains 2x inference speedup.",
        "Methodology": "Simple diffusion finetuning converts AR-VLMs directly. For AR-LMs, a two-stage process (connector pretraining, then diffusion finetuning) is used. Employs a block diffusion strategy for parallel decoding, arbitrary length generation, and efficient KV-cache reuse.",
        "Strengths": "Achieves SOTA among diffusion VLMs with significantly less training data (<5%). Provides 2x inference speedup over prior dVLMs. Supports arbitrary-length generation and efficient KV-cache reuse, previously absent. Enables feasible conversion from *any* AR model.",
        "Limitations": "Performance, while competitive with AR-VLMs, may still lag top-tier models in some specific aspects. There is a trade-off between smaller block size (slightly better performance) and poorer parallelism.",
        "Datasets / Benchmarks": "MMMU (Pro/Std/Val), MME (Cog/Perp), SeedBench (Image/Video), ChartQA, AI2D, MMBench, RealworldQA, Muirbench. Trained on LLaVA-Pretrain (580K) and LLaVA-Next (738K).",
        "Results Summary": "DiffusionVL achieves SOTA among diffusion VLMs (e.g., 34.4% gain on MMMU-Pro, 37.5% on MME) using <5% data, is competitive with advanced AR-VLMs, and achieves a 2x inference speedup.",
        "Why It Matters": "This work bridges the gap between autoregressive and diffusion paradigms for Vision Language Models, enabling the efficient development of high-performance dVLMs with superior parallel decoding, arbitrary-length generation, and KV cache reuse capabilities.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Performance Comparison), Figure 4 (Speed & Quality for Image Captioning), Tables 1 & 2 (Benchmark Performance Comparison of DiffusionVL vs. AR & other Diffusion Models).",
        "arxiv_id": "http://arxiv.org/abs/2512.15713v1"
    },
    {
        "Title": "In Pursuit of Pixel Supervision for Visual Pre-training",
        "Field & Subfield": "Computer Vision, Self-Supervised Learning, Masked Autoencoders (MAE)",
        "Key Contributions": "- Introduces \"Pixio,\" an enhanced MAE with a deeper decoder, larger mask blocks, and multiple [CLS] tokens.\n- Trains on 2B web-crawled images with a novel, soft self-curation strategy.\n- Achieves competitive or superior performance over SOTA DINOv3 on various dense prediction tasks.",
        "Methodology": "Pixio is an enhanced MAE for pixel-space self-supervised learning. It uses a deeper decoder, larger 4x4 patch mask blocks for challenging pretext, and 8 class tokens. Trained on 2B web images with reconstruction loss and soft self-curation based on reconstruction difficulty.",
        "Strengths": "Demonstrates pixel-space SSL's competitiveness, simplicity, stability, & efficiency. Outperforms or matches DINOv3 on many dense prediction and robot learning tasks. Uses minimally-curated web-scale data, reducing benchmark bias.",
        "Limitations": "Masking is an artificial distortion introducing biases. Fixed masking ratios/granularities can be suboptimal for diverse image complexities. Inferior on KITTI (driving) due to less domain-specific training data than DINOv3.",
        "Datasets / Benchmarks": "Pre-training: 2B web-crawled images (self-curated). Evaluation: NYUv2, KITTI, DIODE, Sintel, DA-2K (Depth); ScanNet++, ETH3D, TartanAirV2-WB (3D Recon); ADE20K, Pascal VOC, LoveDA (Semantic Seg); CortexBench (Robot Learning).",
        "Results Summary": "Pixio significantly outperforms original MAE. It surpasses DINOv3 on most depth estimation benchmarks (e.g., 0.268 RMSE vs 0.320 on NYUv2), 3D reconstruction, & robot learning, and is competitive in semantic segmentation.",
        "Why It Matters": "Validates pixel-space self-supervised learning as a powerful, scalable, and less biased alternative or complement to latent-space methods. Offers a promising path for generic visual representations, especially for future video SSL.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Visualizations of Pixio's understanding), Figure 2 (Pixio updates over MAE), Table 1 (Monocular Depth Estimation), Table 3 (Feed-Forward 3D Reconstruction), Table 5 (Robot Learning).",
        "arxiv_id": "http://arxiv.org/abs/2512.15715v1"
    },
    {
        "Title": "Spatia: Video Generation with Updatable Spatial Memory",
        "Field & Subfield": "Computer Vision & Graphics, Video Generation, 3D-Aware Synthesis, Long-Horizon Consistency",
        "Key Contributions": [
            "Explicit 3D scene point cloud as updatable spatial memory.",
            "Dynamic-static disentanglement for consistent videos with dynamic entities.",
            "Spatially consistent generation across multiple views.",
            "Explicit 3D-aware camera control & interactive editing."
        ],
        "Methodology": "Iterative generation: estimates initial 3D point cloud from image, generates video clips conditioned on memory, then updates memory via visual SLAM. Uses a multi-modal conditional diffusion transformer (Wan2.2) with ControlNet. Static entities are disentangled during point cloud estimation.",
        "Strengths": "Achieves robust long-term spatial/temporal consistency. Handles dynamic entities while preserving static scenes. Enables explicit camera control & 3D-aware interactive editing. Outperforms SOTA in consistency and visual quality metrics. Provides a geometrically grounded framework.",
        "Limitations": "Point cloud density (memory vs. fine-grained guidance tradeoff). Potential for error accumulation in extremely long sequences, though mitigated by iterative updates. Dependency on accurate 3D scene point cloud estimation and camera pose.",
        "Datasets / Benchmarks": [
            "Training: RealEstate (40K videos), SpatialVID (10K HD videos).",
            "Evaluation: WorldScore benchmark, RealEstate test set."
        ],
        "Results Summary": "Achieves SOTA on WorldScore (Avg Score 69.73) and RealEstate (PSNR 18.58, SSIM 0.646, LPIPS 0.254), significantly improving static scene consistency while maintaining dynamic entity quality. Superior memory consistency in closed-loop generation (Match Acc 0.698).",
        "Why It Matters": "Addresses a critical challenge in video generation: long-term consistency. Enables novel applications like interactive 3D editing and precise camera control, moving towards robust \"world models\" for AI with persistent memory and geometrically grounded, scalable frameworks.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Spatia Overview), Figure 2 (Training Pipeline), Table 1 (WorldScore Results), Table 3 (Memory Evaluation).",
        "arxiv_id": "http://arxiv.org/abs/2512.15716v1"
    },
    {
        "Title": "Generative Refocusing: Flexible Defocus Control from a Single Image",
        "Field & Subfield": "Computer Vision & Image Processing",
        "Key Contributions": [
            "Flexible refocusing pipeline accepts any focus state, offering control over focus plane, bokeh intensity, and aperture shape via two-stage decomposition.",
            "Semi-supervised framework connects synthetic & unpaired real bokeh images, learning genuine optical characteristics from EXIF metadata.",
            "Achieves top performance in defocus deblurring, bokeh synthesis, and refocusing; supports text-guided adjustments & custom aperture shapes."
        ],
        "Methodology": "Two-stage diffusion framework: DeblurNet recovers all-in-focus images using diffusion guided by initial deblurring predictions; BokehNet synthesizes controllable bokeh via semi-supervised training with synthetic paired data and real unpaired bokeh images (leveraging EXIF metadata).",
        "Strengths": "Flexible input (any focus state), comprehensive user control (focus plane, bokeh intensity, aperture shape), robust to depth-scale inaccuracies, captures authentic lens characteristics, outperforms SOTA across multiple benchmarks.",
        "Limitations": "Relies on monocular depth estimation (prone to mislocalized focal planes), complex aperture shapes require simulator data, occasional hallucination of incorrect details in severe blur.",
        "Datasets / Benchmarks": "DPDD, REALDOF (deblurring); LF-BOKEH (bokeh synthesis); LF-REFOCUS (refocusing); POINTLIGHT-1K (custom aperture shapes); REALBOKEH_3MP, EBB!, Flickr-collected unpaired bokeh.",
        "Results Summary": "Outperforms all baselines in defocus deblurring, bokeh synthesis, and refocusing across fidelity and perceptual metrics, demonstrating realistic blur gradients, accurate focus-plane placement, and natural scene consistency.",
        "Why It Matters": "Enables advanced post-capture depth-of-field control from a single image, democratizing professional photographic effects, and pushing boundaries in image restoration and generative AI with a novel semi-supervised approach.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Visualizing controls), Table 2 (Defocus deblurring benchmark), Table 3 (Bokeh synthesis benchmark), Table 4 (Refocusing benchmark).",
        "arxiv_id": "http://arxiv.org/abs/2512.16923v1"
    },
    {
        "Title": "Next-Embedding Prediction Makes Strong Vision Learners",
        "Field & Subfield": "Computer Vision & Self-supervised Learning",
        "Key Contributions": "• Introduces Next-Embedding Predictive Autoregression (NEPA) for vision, predicting future patch embeddings autoregressively.\n• Achieves strong SOTA performance on ImageNet-1K (83.8% ViT-B, 85.3% ViT-L) and ADE20K (48.3% ViT-B, 54.0% ViT-L).\n• Demonstrates a minimalist approach (no pixel reconstruction, discrete tokens, contrastive loss, separate decoders).\n• Shows autoregression in continuous embedding space is a simple, scalable, modality-agnostic SSL alternative.",
        "Methodology": "NEPA trains a Vision Transformer to autoregressively predict future patch embeddings in continuous space. It uses causal masking, a similarity-based loss with stop-gradient, and modern architecture stabilization (RoPE, LayerScale, SwiGLU, QK-Norm). No pixel decoder or discrete tokens are used.",
        "Strengths": "Architectural simplicity & scalability, competitive SOTA performance on classification & segmentation without complex heads or decoders. Learns semantically meaningful, long-range dependencies & object-level structure. Modality-agnostic potential.",
        "Limitations": "Poor performance under standard linear probing. Struggles with images requiring complex physical understanding (reflections, shading, small/overlapping objects). Potential for dataset biases like other large models.",
        "Datasets / Benchmarks": "Pretraining on ImageNet-1K (unlabeled). Evaluated on ImageNet-1K for classification and ADE20K for semantic segmentation.",
        "Results Summary": "NEPA-B achieved 83.8% top-1 accuracy on ImageNet-1K and 48.3% mIoU on ADE20K. NEPA-L achieved 85.3% top-1 accuracy on ImageNet-1K and 54.0% mIoU on ADE20K, outperforming/matching prior SOTA SSL methods.",
        "Why It Matters": "This work shows that the generative pretraining paradigm, successful in NLP, can be directly applied to vision by predicting embeddings. It offers a simpler, more scalable, and potentially modality-agnostic alternative to current visual self-supervised learning methods.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Table 1 (Ablation studies on NEPA components); Table 2 (ImageNet-1K classification comparison); Table 3 (ADE20K segmentation comparison); Figure 5 (Attention and embedding analysis visualizations).",
        "arxiv_id": "http://arxiv.org/abs/2512.16922v1"
    },
    {
        "Title": "The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text",
        "Field & Subfield": "Computer Vision & Generative AI: Video Generation & World Models",
        "Key Contributions": "• Multimodal framework (text, trajectories, ref. images) for controllable world events. • Novel data curation pipeline for trajectory-video-text triplets. • Spatial-Aware Weighted Cross-Attention for text-trajectory alignment. • Demonstrates object/scene consistency, physical plausibility, causal reasoning, & counterfactual generation.",
        "Methodology": "WorldCanvas builds on Wan2.2 I2V 14B. It uses a novel data curation pipeline (keypoint tracking, motion captioning, ref. image extraction) to generate trajectory-video-text triplets. Incorporates Trajectory Injection & Spatial-Aware Weighted Cross-Attention for fine-grained control and alignment. Trained using flow matching (L1 loss).",
        "Strengths": "Offers precise, fine-grained control (when, where, who, what) for video generation. Achieves strong temporal coherence, maintaining object identity & scene consistency across occlusions. Demonstrates physical plausibility, causal reasoning, & counterfactual generation capabilities.",
        "Limitations": "Fails in some challenging scenarios involving complex spatial transformations or logical reasoning, e.g., precise water level tracking during camera pans or maintaining object consistency during drastic camera rotations.",
        "Datasets / Benchmarks": "Curated dataset of 280k trajectory-video-text triplets for training. Evaluated on 100 image-trajectory pairs. Compared against Wan2.2 I2V, ATI, & Frame In-N-Out baselines.",
        "Results Summary": "Outperforms baselines (Wan2.2 I2V, ATI, Frame In-N-Out) in ObjMC, Appearance Rate, Subject/Background Consistency, & CLIP-T (Global/Local) metrics. Human evaluation confirms superiority in trajectory following, prompt adherence, text-trajectory alignment, reference fidelity, & overall video quality.",
        "Why It Matters": "Transforms world models from passive predictors to interactive, user-shaped simulators. Enables creation of complex, coherent, promptable world events with precise multimodal control, advancing controllable video generation and foundational AI research.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Fig 1 (examples), Fig 2 (architecture), Table 1 (quantitative results), Table S2 (human evaluation)",
        "arxiv_id": "http://arxiv.org/abs/2512.16924v1"
    },
    {
        "Title": "Interact2Ar: Full-Body Human-Human Interaction Generation via Autoregressive Diffusion Models",
        "Field & Subfield": "Computer Vision, Human Motion Generation",
        "Key Contributions": "- End-to-end text-conditioned autoregressive diffusion model for full-body human-human interactions.\n- Novel cooperative denoiser architecture with body-part-specialized heads for detailed hand motions.\n- Autoregressive pipeline with Mixed Memory for adaptive motion synthesis.\n- Extended evaluation pipeline with robust, specialized metrics.",
        "Methodology": "Text-conditioned autoregressive diffusion model with cooperative denoisers & body-part-specialized heads (trajectory, body, hands). Employs a novel Mixed Memory strategy for efficient short/long-term context during sequential motion generation. Trained on SMPL-X parameters with various kinematic losses.",
        "Strengths": "Achieves SOTA performance on Inter-X for full-body interaction generation, including detailed hands. Enables adaptive capabilities: temporal composition, real-time disturbance adaptation, & sequential multi-person interactions. Robust evaluation pipeline.",
        "Limitations": "Dataset constraints limit human diversity (e.g., body shapes, hand contact precision), affecting the precision of hand contacts. Future work needs to address body shape variations.",
        "Datasets / Benchmarks": "Inter-X dataset (11K full-body interactions, 40 actions, detailed textual descriptions). Evaluated against InterMask, InterGen, T2M benchmarks.",
        "Results Summary": "Outperforms previous SOTA methods (InterMask) on Inter-X across all metrics (R-Prec.↑, FID↓, PJ↑, AUJ↓) for full-body, body, & hands. Autoregressive version consistently outperforms non-autoregressive. User study confirms superior text alignment & hand realism.",
        "Why It Matters": "Addresses the challenging task of generating realistic, coherent, and adaptable human-human interactions with detailed motions, crucial for virtual agents, robotics, and creative applications. Enables dynamic, reactive motion synthesis beyond pre-planned sequences.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (overview), Figure 2 (architecture), Figure 3 (Mixed Memory), Table 2 (SOTA comparison), Figure 4 (User Study).",
        "arxiv_id": "http://arxiv.org/abs/2512.19692v1"
    },
    {
        "Title": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding",
        "Field & Subfield": "Computer Vision & Generative Models / Representation Learning",
        "Key Contributions": "- Introduces 'Prism Hypothesis' for spectral characteristics of semantic & pixel encoders.\n- Proposes Unified Autoencoding (UAE) with a frequency-band modulator.\n- Achieves SOTA reconstruction and semantic alignment in a unified latent space.\n- Provides diffusion-friendly latents for generation.",
        "Methodology": "Unified Autoencoding (UAE) learns a shared latent space by factorizing content into fundamental semantic (low-frequency) and residual pixel (high-frequency) bands using an innovative frequency-band modulator, guided by the Prism Hypothesis.",
        "Strengths": "SOTA reconstruction quality (PSNR, SSIM, rFID) and generative capability. Effectively unifies semantic abstraction and pixel fidelity. Robust to frequency band granularity. Preserves strong semantic discriminability.",
        "Limitations": "The paper does not explicitly state limitations of the proposed UAE model; it primarily focuses on its strengths and superior performance compared to existing methods.",
        "Datasets / Benchmarks": "ImageNet-1K, MS-COCO 2017",
        "Results Summary": "UAE achieves SOTA reconstruction (e.g., ImageNet PSNR 33.08, SSIM 0.94, rFID 0.16 with DINOv2-L) and competitive generative performance (gFID 1.68, IS 301.6). It matches RAE's 83.0% top-1 accuracy in linear probing semantic understanding.",
        "Why It Matters": "It unifies previously fragmented semantic and pixel representations into a single, compact latent space. This streamlines the development of foundation models for both understanding and generation, leading to more efficient and coherent multimodal AI.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Prism Hypothesis), Figure 2 (Frequency Energy Distribution), Table 1 (Reconstruction Quality), Table 3 (Linear Probing Accuracy)",
        "arxiv_id": "http://arxiv.org/abs/2512.19693v1"
    },
    {
        "Title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos",
        "Field & Subfield": "Artificial Intelligence & Multimodal Large Language Models (MLLMs)",
        "Key Contributions": "- Novel multi-agent architecture for long-video QA, coordinating master LLM, grounding agent, and vision agent.\n- Reward-driven reinforcement learning scheme for concise, step-wise reasoning in MLLMs.\n- Introduction of new episode-level long video datasets: LongTVQA and LongTVQA+.",
        "Methodology": "A master LLM coordinates a grounding agent (temporal localization) and a vision agent (visual info extraction). The master iteratively reasons using accumulated evidence. It's fine-tuned with GRPO using structural and answer rewards to promote efficient, multi-step, tool-augmented reasoning.",
        "Strengths": "Achieves superior accuracy on long-video QA, provides interpretable decision-making trajectories, effectively handles sparse information in long videos, and is model-agnostic. Ablations show grounding and vision agents are critical.",
        "Limitations": "Relies on provided subtitles (no raw audio/ASR). Grounding and vision modules are fixed during RL training (no joint optimization). Reward function is intentionally simple, with room for improvement.",
        "Datasets / Benchmarks": "LongTVQA and LongTVQA+ (episode-level aggregations from TVQA/TVQA+), designed for hour-scale video QA. Evaluated against closed-source (GPT-4o, Gemini 2.5 Pro, Grok) and open-source LLM baselines.",
        "Results Summary": "LongVideoAgent significantly outperforms non-agent baselines on LongTVQA/+. Agentic RL yields substantial gains, especially for open-source models (e.g., Qwen2.5-7B with RL matches GPT-5-mini). Grounding+vision improves accuracy by +10.5%.",
        "Why It Matters": "Addresses key challenges in long-form video understanding by enabling efficient, fine-grained temporal reasoning. Its multi-agent framework provides interpretability and enhances LLM capabilities for complex multimodal tasks, setting new SOTA benchmarks.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Multi-agent Reasoning Architecture), Table 2 (Performance on LongTVQA/LongTVQA+), Table 4 (Ablation Studies).",
        "arxiv_id": "http://arxiv.org/abs/2512.20618v1"
    },
    {
        "Title": "SemanticGen: Video Generation in Semantic Space",
        "Field & Subfield": "Computer Vision & Generative AI, Video Generation",
        "Key Contributions": "- Proposed SemanticGen: novel two-stage framework for video generation in compact semantic space.\n- Identified semantic encoder requirements & developed MLP-based semantic representation compression.\n- Demonstrated faster convergence, high-quality, and robust scalability to long video generation.",
        "Methodology": "Two-stage diffusion model framework. Stage 1: Diffusion model generates compact semantic video features via pre-trained semantic encoder (Qwen-2.5-VL) & MLP compression. Stage 2: Another diffusion model generates VAE latents conditioned on these features. Uses Swin attention for long videos.",
        "Strengths": "Faster convergence than VAE latent-space methods. Generates high-quality, long-form videos with superior temporal consistency & reduced drift. Computationally efficient, especially for long videos, by operating in a compact semantic space.",
        "Limitations": "Struggles to maintain fine-grained texture consistency in very long videos. Performance is constrained by semantic encoder's ability to capture high-frequency temporal information, especially at low sampling FPS.",
        "Datasets / Benchmarks": "Internal text-video pair dataset. Standard VBench & VBench-Long benchmarks for evaluation.",
        "Results Summary": "SemanticGen achieves comparable/superior performance to SOTA on short videos. Substantially outperforms baselines in long video consistency & temporal stability (lower drift) on VBench-Long. Semantic space compression accelerates convergence.",
        "Why It Matters": "It addresses key challenges in video generation: slow convergence and poor scalability to long videos. By planning globally in a compact semantic space, SemanticGen enables more efficient, higher-quality, and temporally consistent generation of both short and long videos.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Fig 1: Examples synthesized by SemanticGen.\nFig 2: Overview of SemanticGen vs. previous methods.\nFig 8 & Table 3: Semantic space compression ablation studies.\nFig 9: Ablation on representation space (semantic vs. VAE latent).",
        "arxiv_id": "http://arxiv.org/abs/2512.20619v1"
    },
    {
        "Title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs",
        "Field & Subfield": "AI, Multimodal Large Language Models (MLLMs), Spatial Reasoning",
        "Key Contributions": "• First capability-centric hierarchical benchmark (SpatialTree) for spatial intelligence in MLLMs. • Revealed structural transfer dynamics of spatial skills via SFT: L1 prerequisite for L4 agentic competence. • Identified reasoning-perception trade-off in RL; proposed auto-think strategy for consistent improvement.",
        "Methodology": "Proposed SpatialTree: 4-level cognitive hierarchy (Perception, Mental Mapping, Simulation, Agentic). Constructed SpatialTree-Bench from existing & new data (SpatialPlus) using expert models/LLM rephrasing. Evaluated 27 sub-abilities via SFT and Hierarchy-Aware Reinforcement Learning (RLVR) with auto-think strategy.",
        "Strengths": "First capability-centric benchmark. Systematic evaluation across diverse MLLMs and 27 sub-abilities. Revealed crucial cross-level transfer dynamics & reasoning-perception trade-off. Proposes an effective 'auto-think' RL strategy for consistent performance gains.",
        "Limitations": "Naive RL on low-level skills can overfit, leading to limited generalization. Uniform RL strategies struggle to balance diverse requirements from atomic perception to complex planning, hindering broad performance gains.",
        "Datasets / Benchmarks": "SpatialTree-Bench (primary, custom-built), SpatialPlus (new data for underrepresented abilities). Reorganized existing datasets: SUNRGBD, Hypersim, Matterport3D, ArkitScenes, LLaVA-Video, LLaVA-NeXT-Interleave, LLaVA-OneVision, EmbodiedBench, etc.",
        "Results Summary": "SpatialTree reveals hierarchical structure: L1 orthogonal, L2-L4 correlated. SFT showed negative transfer within L1 but strong cross-level transfer. Naive RL is unreliable; auto-think strategy consistently improves all levels by suppressing unnecessary deliberation for perception. Gemini2.5-Pro performed best.",
        "Why It Matters": "Provides a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs. Offers fundamental insights into how spatial intelligence emerges, composes, and transfers, paving the way for more capable and robust spatial AI agents.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (SpatialTree hierarchy), Figure 5 (Inter-Capability Dependencies heatmap), Table 1 (Overall Performance), Table 3 (RLVR Comparisons)",
        "arxiv_id": "http://arxiv.org/abs/2512.20617v1"
    },
    {
        "Title": "Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models",
        "Field & Subfield": "Computer Vision & Vision-Language Models; Ordinal Regression, Explainable AI",
        "Key Contributions": "• Introduced YearGuessr: largest open, global, multi-modal building age dataset (55k images, 1001-2024 CE); • Proposed YearCLIP: CLIP-based model with ordinal regression, GPS integration & reasoning prompts for explainable age prediction; • Exposed significant popularity bias in VLMs (up to +34% accuracy on popular buildings); • Provided comprehensive benchmark of 30+ models & novel popularity-aware metrics.",
        "Methodology": "Developed YearGuessr dataset from Wikipedia (55k images, GPS, text, page views). Framed building age as ordinal regression. Proposed YearCLIP, a CLIP-based model integrating GPS via zero-convolution and reasoning prompts for explainability. Evaluated with MAE, Interval Accuracy, and new popularity-aware metrics.",
        "Strengths": "First open, global, large-scale benchmark for building age. Addresses VLM memorization bias beyond true architectural understanding. YearCLIP offers explainable predictions with architectural rationales. Comprehensive evaluation across 30+ diverse models.",
        "Limitations": "Dataset is geographically & temporally skewed towards modern examples. Labels are based on original construction years, even for renovated/rebuilt buildings, introducing noise. Generalization to underrepresented regions/early styles is limited.",
        "Datasets / Benchmarks": "YearGuessr (new, 55,546 images, 157 countries, 1001-2024 CE, CC BY-SA 4.0). Compared against MyCD, CMAB, MTBF-33. Benchmarked CNNs, Transformers, CLIP-based, Closed VLMs (Gemini, Claude, Grok), and Open VLMs.",
        "Results Summary": "VLMs show significant popularity bias (e.g., Gemini2.0-flash +34.18% IA5 gain on popular buildings) due to memorization. YearCLIP reduces MAE to 39.52, outperforming baselines and improving fine-grained prediction and explainability. Closed-source VLMs generally dominate, but all models struggle with early-period buildings and regional disparities.",
        "Why It Matters": "Provides a crucial benchmark for automated building age estimation, vital for sustainability, heritage, and disaster assessment. Exposes a fundamental flaw (memorization bias) in state-of-the-art VLMs, pushing for more robust and generalizable AI.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (YearGuessr/YearCLIP overview); Table 2 (Main results: MAE, Interval Accuracy, Popularity Bias); Figure 6 (YearCLIP Explainable Predictions).",
        "arxiv_id": "http://arxiv.org/abs/2512.21337v1"
    },
    {
        "Title": "HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming",
        "Field & Subfield": "Computer Vision; High-Resolution Video Generation; Diffusion Models",
        "Key Contributions": "HiStream framework for efficient autoregressive high-resolution video generation via redundancy elimination. Includes Dual-Resolution Caching (spatial), Anchor-Guided Sliding Window (temporal), & Asymmetric Denoising (timestep). Achieves up to 107.5x faster 1080p video generation with SOTA quality.",
        "Methodology": "Autoregressive diffusion model on WAN-2.1 DiT. Employs Dual-Resolution Caching (low-res denoising, high-res refinement), Anchor-Guided Sliding Window (fixed-size chunking with keyframe/cache), and Asymmetric Denoising (fewer steps for subsequent chunks).",
        "Strengths": "Significantly reduces inference cost (up to 107.5x faster), maintains state-of-the-art visual quality & temporal consistency, practical for 1080p, scalable. User study & VBench show superior perceptual quality across metrics.",
        "Limitations": "VAE decoder is a new primary bottleneck (16.45s/81 frames on A100). High memory costs during distillation limited training to 1.3B student model, not full 1080p data supervision. Remaining flaws like lack of physical realism.",
        "Datasets / Benchmarks": "Evaluated on 1080p video generation benchmarks. Quantitative comparison using VBench metrics. User preference study. Based on Wan2.1-T2V-1.3B model.",
        "Results Summary": "HiStream: 76.2x faster (0.48s/frame) than Wan2.1 baseline (36.56s/frame), SOTA quality. HiStream+: 107.5x faster (0.34s/frame) than baseline, with compelling speed-quality trade-off. Achieves highest user preference.",
        "Why It Matters": "Makes high-resolution, high-fidelity video generation practical and scalable for digital media, film, and VR, overcoming current diffusion model bottlenecks. Provides a foundation for accessible cinematic content creation.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1: Per-Frame Denoising Latency & Higher Preference (speedup, user study). Table 1: Quantitative comparisons with baselines (speed, quality metrics). Figure 2: Pipeline details (core mechanisms).",
        "arxiv_id": "2512.21338v1"
    },
    {
        "Title": "OPTIMIZING DECODING PATHS IN MASKED DIFFUSION MODELS BY QUANTIFYING UNCERTAINTY",
        "Field & Subfield": "Natural Language Processing & Generative Models (Masked Diffusion Models, Decoding Strategies)",
        "Key Contributions": "- First to formalize MDM decoding order sensitivity as Path Uncertainty.\n- Introduced Denoising Entropy (State/Path Entropy) to quantify generative uncertainty.\n- Proposed E-BON (post-hoc selection) and E-SMC (real-time guidance) algorithms.\n- Theoretically justified Denoising Entropy as proxy for generation quality & loss.\n- Significantly improved MDM generation quality on various benchmarks.",
        "Methodology": "Introduced Denoising Entropy (State Entropy, Path Entropy) to quantify cumulative predictive uncertainty in Masked Diffusion Models (MDMs). Proposed two algorithms: E-BON for post-hoc path selection based on minimizing Path Entropy, and E-SMC for real-time guidance via weighted resampling using State Entropy.",
        "Strengths": "Provides a principled, computable internal signal (Denoising Entropy) for MDMs. Significantly improves generation quality, consistency, and accuracy across challenging tasks. Scalable to large models and complex reasoning/planning tasks. Turns MDM uncertainty into a key advantage.",
        "Limitations": "Finding a globally optimal decoding path remains intractable, with algorithms providing approximations. E-BON uses uniform computational budget. Greedy search variants may sacrifice generation diversity. Theoretical bounds show local errors accumulate over denoising steps.",
        "Datasets / Benchmarks": "OpenWebText (for PPL); Reasoning & Planning: GSM8K, MATH500, GPQA, Countdown, Sudoku; Code Generation: HumanEval, HumanEval+, MBPP, MBPP+.",
        "Results Summary": "Denoising Entropy correlated strongly with text quality (lower HDE = lower PPL). E-SMC consistently outperformed vanilla and E-BON, achieving significant accuracy gains (up to +5.9% on Countdown, +1.9% on GSM8K over PC-Sampler). Performance improved with increased particles and resampling frequency, demonstrating scalability.",
        "Why It Matters": "Addresses a critical, previously unquantified problem in MDMs: the sensitivity of output quality to decoding order. Introduces a foundational tool (Denoising Entropy) for understanding and controlling MDM generation uncertainty, paving the way for more calibrated decoding strategies and improved overall generative performance.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1: Quantifying Path Uncertainty in MDMs with Denoising Entropy.\nFigure 2: Overview of entropy-guided decoding algorithms (E-BON, E-SMC).\nTable 2: Accuracy (%) on LLADA models across reasoning & planning benchmarks.",
        "arxiv_id": "http://arxiv.org/abs/2512.21336v1"
    },
    {
        "Title": "Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation",
        "Field & Subfield": "Computer Vision & Robotics, Deep Learning",
        "Key Contributions": "- Introduces TransPhy3D (1.32M frames), the first synthetic video dataset for transparent/reflective objects. - DKT: foundation model for video depth/normal, repurposing VDM via LoRA & co-training. - Achieves zero-shot SOTA on real/synthetic transparency benchmarks. - Significantly improves robotic grasping success rates.",
        "Methodology": "Leverages video diffusion models (VDM) for their implicit optical rule understanding. Creates TransPhy3D: 11k physically-based rendered video sequences of transparent/reflective objects. DKT: VDM fine-tuned with LoRA adapters for video-to-video translation (RGB to depth/normals). Co-trains on TransPhy3D & existing image datasets.",
        "Strengths": "Achieves SOTA zero-shot depth/normal estimation for transparent/reflective objects. Provides temporally consistent predictions. Improves robotic grasping success significantly. High computational efficiency (1.3B version at 0.17 s/frame). Robust generalization to in-the-wild videos.",
        "Limitations": "Primarily relies on synthetic data for training, despite its scale. While robust, inherent ambiguities of transparency & reflection still present complex generalization challenges for some real-world edge cases. Explicit exploration of failure modes is not detailed.",
        "Datasets / Benchmarks": "TransPhy3D (new, 1.32M frames). ClearPose, DREDS (CatKnown/CatNovel), HISS, ClearGrasp. Evaluated zero-shot on all. Robotic grasping experiments on reflective, translucent, diffusive surfaces.",
        "Results Summary": "DKT achieves SOTA on ClearPose, DREDS, & TransPhy3D-Test for depth. DKT-Normal sets SOTA for normal estimation on ClearPose. Significantly outperforms prior methods, boosting robotic grasping success (e.g., 0.73 mean vs 0.48). 1.3B DKT runs at 0.17 s/frame (832x480).",
        "Why It Matters": "Addresses a critical challenge in robotics and 3D perception: robustly perceiving transparent and reflective objects. Enables more reliable robotic manipulation in complex environments, leveraging generative AI for perception without real-world labels.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Fig. 1 (Qualitative Results), Fig. 2 (DKT Qualitative Comparison), Table I & II (Quantitative Comparisons for Depth), Table VI (Grasping Success Rate)",
        "arxiv_id": "http://arxiv.org/abs/2512.23705v1"
    },
    {
        "Title": "Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion",
        "Field & Subfield": "Computer Vision & Deep Learning; Video Super-Resolution",
        "Key Contributions": "- First diffusion-based framework for online, low-latency VSR via 4-step distillation.\n- Novel Auto-regressive Temporal Guidance (ARTG) & Temporal-aware Decoder to leverage past frames for consistency.\n- Outperforms existing methods in perceptual quality & consistency with practical inference speeds.",
        "Methodology": "A causally conditioned diffusion framework using a 4-step distilled U-Net for fast inference. Integrates Auto-regressive Temporal Guidance (ARTG) for motion-aligned latent denoising & a Temporal-aware Decoder (with TPM) for detail enhancement & coherence, strictly on past frames.",
        "Strengths": "Achieves superior perceptual quality (lowest LPIPS) & high temporal consistency. Drastically reduces inference latency (0.328s vs. 4600s for prior diffusion methods) and memory footprint (20.8GB), making diffusion VSR viable for real-time online deployment.",
        "Limitations": "Remains heavier than CNN/Transformer models. Optical flow usage can introduce artifacts in fast motion. Auto-regressive design may weaken initial frames due to lack of prior temporal context. Robustness to real-world degradations needs improvement.",
        "Datasets / Benchmarks": "Evaluated on REDS4, Vimeo-90K-T, VideoLQ, and Vid4. Compared against CNN-based (BasicVSR++, TMP), Transformer-based (RVRT, MIA-VSR, RealViformer), and Diffusion-based (StableVSR, MGLD-VSR) methods.",
        "Results Summary": "Achieved LPIPS 0.099 (REDS4) and 0.056 (Vimeo-90K-T), superior to all baselines. Reduced latency from >4600s to 0.328s. Maintained competitive runtime (0.328s/frame) vs. online CNN/Transformer methods while vastly outperforming diffusion counterparts in speed and memory.",
        "Why It Matters": "Bridges the gap between high-quality but slow diffusion models and fast but lower-quality CNN/Transformer VSR. Enables practical, low-latency online deployment of diffusion-based VSR for critical applications like AR/VR, video conferencing, and surveillance.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Quality & Runtime Comparison), Table 1 (Diffusion VSR Comparison), Table 2 (REDS4 Quantitative Comparison), Table 6 (Memory & Speed).",
        "arxiv_id": "http://arxiv.org/abs/2512.23709v1"
    },
    {
        "Title": "Edit3r: Instant 3D Scene Editing from Sparse Unposed Images",
        "Field & Subfield": "Computer Vision & 3D Scene Editing / Generative AI",
        "Key Contributions": "- Feed-forward framework for single-pass 3D scene editing from unposed, view-inconsistent images; - SAM2-based recoloring for view-consistent supervision; - Asymmetric input strategy for robustness against inconsistencies; - DL3DV-Edit-Bench benchmark for 3D editing.",
        "Methodology": "A feed-forward network predicts 3D Gaussian splats from sparse, unposed, 2D edited images in one pass. Trained with SAM2-based recoloring for view-consistent supervision, using an asymmetric input scheme (recolored ref. view + original aux. views) & 3D regularization losses.",
        "Strengths": "Orders of magnitude faster inference (0.5s/view vs. 325-584s for baselines). Achieves superior semantic alignment and enhanced 3D consistency. Photorealistic rendering, flexible with various 2D editors, and generalizes well without per-scene optimization.",
        "Limitations": "Recoloring supervision doesn't fully capture large geometric changes or extreme material/illumination shifts. Future work could include generative augmentation & dynamic scenes.",
        "Datasets / Benchmarks": "DL3DV-Edit-Bench (introduced in paper, built on DL3DV test split), featuring 20 scenes and 4 edit types (Add, Remove, Modify, Global).",
        "Results Summary": "Edit3r is significantly faster (0.51s/view vs. 325-584s) and achieves highest CLIPt2i (0.266), indicating better instruction following, while maintaining competitive C-FID (171.3) & C-KID (0.116) for realism/consistency.",
        "Why It Matters": "Enables real-time, interactive 3D scene editing for content creation, visual effects, AR/VR, and game design. Overcomes slow optimization/inconsistency issues of prior methods, paving the way for practical applications.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Overview), Figure 5 (Qualitative comparison), Table 1 (Quantitative comparison on methods).",
        "arxiv_id": "2512.25071v1"
    },
    {
        "Title": "SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time",
        "Field & Subfield": "Computer Vision, Generative AI / Video Diffusion Models",
        "Key Contributions": "- First video diffusion model enabling joint spatial & temporal control from a single video.\n- Novel animation time-embedding for explicit motion sequence control.\n- Temporal-warping training scheme repurposes multi-view datasets for temporal diversity.\n- Cam×Time: New synthetic dataset for dense spatiotemporal supervision.",
        "Methodology": "A latent video diffusion backbone (DiT) with 3D-VAE. Introduces a dedicated 'animation time' embedding (E_ani) and a source-aware camera conditioning (E_cam) that disentangle space and time. Uses temporal warping augmentation on existing datasets & a new synthetic Cam×Time dataset. Employs multi-turn autoregressive inference for long videos.",
        "Strengths": "Achieves fully disentangled control over camera motion and scene dynamics. Supports diverse temporal effects like slow-motion, reverse, bullet-time, and arbitrary initial poses. Significantly outperforms SOTA baselines in temporal control accuracy and visual quality. Enables generation of long, coherent videos via autoregressive scheme.",
        "Limitations": "Relies on synthetic data (CamxTime) for comprehensive temporal supervision. While autoregressive generation extends video length, native generation is limited to 81 frames. The complexity of modeling continuous 4D space-time for dynamic scenes remains a challenging problem.",
        "Datasets / Benchmarks": "Cam×Time (new synthetic), ReCamMaster, SynCamMaster, DL3DV-10K, OpenVideoHD, static-scene datasets (RE10k, Mannequin Challenge). Evaluated using PSNR, SSIM, LPIPS, VBench, RotErr, TransErr, RTA@15, RTA@30.",
        "Results Summary": "SpaceTimePilot outperforms baselines significantly in temporal control (PSNR↑, SSIM↑, LPIPS↓). Achieves comparable visual quality on VBench. Demonstrates superior camera accuracy (lower RotErr, TransErr, higher RTA) and robust disentangled control qualitatively, enabling complex retiming effects.",
        "Why It Matters": "Revolutionizes video editing by offering unprecedented joint control over camera viewpoints and motion dynamics, from a single input video. Enables interactive 4D scene exploration and advanced re-rendering, opening new possibilities for creative content generation and simulation.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Qualitative results of space-time control), Figure 3 (Temporal Wrapping), Table 2 (Quantitative comparison on temporal controls), Figure 4 (Cam×Time visualization), Table 4 (Camera accuracy).",
        "arxiv_id": "http://arxiv.org/abs/2512.25075v1"
    },
    {
        "Title": "ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors",
        "Field & Subfield": "Computer Vision; Deepfake Detection; Generative Models",
        "Key Contributions": "• Self-supervised audio-to-expression diffusion for zero-shot face forgery detection. • Personalized via subject-specific adapters. • Novel 3DMM extraction & content-agnostic authentication. • SOTA performance on traditional/Sora2 deepfakes, robust to corruptions. • Introduced Sora2 Cameo Forensics Preview dataset.",
        "Methodology": "Self-supervised audio-to-expression diffusion model (EXAM) pre-trained on a large dataset. Personalized via subject-specific adapters. Authenticates videos by identity distances from diffusion reconstruction errors using a novel content-agnostic mechanism and 3DMM extraction.",
        "Strengths": "Exhibits SOTA zero-shot generalization to unseen manipulations & Sora2 deepfakes. Highly robust to corruptions (blur, compression). Self-supervised training on pristine data avoids overfitting. Personalized detection for real-world person-of-interest scenarios.",
        "Limitations": "Relies on off-the-shelf models (FLAME, SPECTRE, Wav2Vec). High computational overhead during inference due to iterative 3DMM extraction and multi-timestep diffusion reconstruction. Authentication scores can be unstable in silent video frames.",
        "Datasets / Benchmarks": "Pre-training: Custom audio-expression dataset (VoxCeleb2, AVSpeech, Acappella). Evaluation: DF-TIMIT, DFDCP, KoDF, IDForge. Introduced Sora2 Cameo Forensics Preview (S2CFP) for Sora2-generated video detection.",
        "Results Summary": "Achieved 95.22% avg AUC on DF-TIMIT, DFDCP, KoDF, IDForge, surpassing SOTA by 4.22%. Detected Sora2-generated videos with 94.44% AUC (others failed). Highly robust to corruptions; 2% AUC drop vs. 36.71% for SOTA under severe compression.",
        "Why It Matters": "Addresses key challenge of deepfake generalization to unseen manipulations. Self-supervised design prevents overfitting, enabling robust, real-world detection, even for cutting-edge AI-generated content like Sora2. Crucial for combating misinformation & protecting public trust.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Fig 1: Framework overview. Fig 3: Robustness to corruptions. Table 2: Generalization AUC on deepfake datasets (SOTA comparison).",
        "arxiv_id": "http://arxiv.org/abs/2601.02359v1"
    },
    {
        "Title": "VInO: A Unified Visual Generator with Interleaved OmniModal Context",
        "Field & Subfield": "Computer Vision, Generative Models",
        "Key Contributions": "- Unified diffusion architecture for image/video generation & editing via VLM-MMDiT coupling.\n- Learnable query tokens for improved multimodal conditioning & optimization stability.\n- Token-boundary mechanism for consistent grounding across semantic & latent representations.\n- Progressive training strategy to upgrade a pretrained video model to multi-task generator.",
        "Methodology": "VINO couples a VLM with a Multimodal Diffusion Transformer (MMDiT). VLM processes text, reference images/videos as interleaved conditioning tokens. MMDiT operates as token-based diffusion backbone. Learnable query tokens and token-boundary mechanism for VAE latents enhance conditioning. Progressive multi-stage training from a T2V base model.",
        "Strengths": "Achieves strong visual quality, faithful instruction following, improved reference/attribute preservation, and controllable multi-identity edits across images and videos. Unified framework avoids task-specific models, preserving base model strengths while expanding capabilities. Robust to diverse instruction formats.",
        "Limitations": "Lacks text-rendering capability, limiting performance on text editing benchmarks. Lower quality instruction-based editing datasets can bias model. Quadratic complexity of MMDiT attention increases latency with many references. Modalities constrained by VLM.",
        "Datasets / Benchmarks": "Image/Video collections (e.g., Laion-5B), distillation data. Evaluated on Geneval (T2I/T2V), VBench (T2V, video editing), OpenS2V (reference video gen), ImgEdit (image editing), GEdit (image editing).",
        "Results Summary": "VINO demonstrates strong visual quality and instruction following. It maintains T2V capabilities comparable to its base model while surpassing baselines in diverse editing tasks. Learnable tokens and special VAE latent tokens significantly improve stability, instruction adherence, and identity preservation. User studies confirm preference.",
        "Why It Matters": "VINO offers a scalable and practical pathway towards unified, general-purpose visual generation. It shows the promise of interleaved, in-context computation for multimodal creation, consolidating fragmented pipelines into a single framework for diverse generation and editing tasks.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 3 (VINO pipeline overview), Figure 7 (Qualitative video editing comparison), Table 2 (Geneval T2I/T2V results), Table 7 (VBench video editing quality).",
        "arxiv_id": "2601.02358v1"
    },
    {
        "Title": "A Versatile Multimodal Agent for Multimedia Content Generation",
        "Field & Subfield": "AI, Multimodal Content Generation",
        "Key Contributions": "- Developed plan generation system with tool library & evaluation metrics.\n- Introduced two-stage plan curation (self-reflection, preference-based optimization).\n- Proposed Agent Skill Acquisition (3-stage training pipeline).",
        "Methodology": "MultiMedia-Agent trained via 3 stages of Skill Acquisition Theory (Cognitive, Associative, Autonomous). GPT-4o for plan generation & 2-stage curation (self-correction, preference model). Utilizes a diverse tool library & custom dataset. Minicpm-v2 backbone.",
        "Strengths": "Handles complex, end-to-end multimodal content generation; incorporates human aesthetic/preference alignment; learns skills progressively like humans; comprehensive tool library; robust evaluation metrics.",
        "Limitations": "Tool selection is prompt-based; could benefit from RAG for vast tools. Current system is single-agent; future work may explore multi-agent systems for complex tasks.",
        "Datasets / Benchmarks": "Custom dataset of 18 multimodal tasks (1260 user requests, 3 plans/request). Evaluated against Claude 3.5 Sonnet, GPT-4o, GPT-4o-Mini.",
        "Results Summary": "Agent-2 showed significant success rate improvement. Agent-3 enhanced human preference alignment across metrics, despite a slight dip in execution success for longer plans. Overall, progressive quality improvement from Agent-1 to Agent-3.",
        "Why It Matters": "Enables more versatile, human-aligned AIGC by tackling complex multimodal content creation end-to-end and integrating human preferences, paving the way for advanced automated content production.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Table 1 (Agent comparison), Figure 2 (Plan curation), Table 4 (Success rates), Table 5 (Preference metrics), Figure 4 (Visualizations).",
        "arxiv_id": "http://arxiv.org/abs/2601.03250v1"
    },
    {
        "Title": "Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training",
        "Field & Subfield": "Computer Vision & Graphics; 3D Content Generation",
        "Key Contributions": "- First training-free framework for fantastic 3D creature generation in a feed-forward paradigm. - Novel skeleton-based method for designing and composing diverse skeletal structures. - Geometry and texture generation modules ensure reasonable, harmonious, and style-consistent 3D assets. - Achieves SOTA visual fidelity and efficiency.",
        "Methodology": "Muses employs a 3-stage pipeline: 1) Skeleton-guided concept design using graph-constrained LLM reasoning. 2) SLAT-based content composition via skinning weights and voxel interpolation. 3) Style-consistent texture generation using image-guided appearance modeling (FLUX.1 Kontext).",
        "Strengths": "Training-free, rapid generation (<1 min/instance). Achieves high visual fidelity, structural coherence, and strong textual alignment. Supports diverse, complex compositions and flexible 3D editing. Outperforms SOTA in quality metrics and human preference.",
        "Limitations": "Relies on external 3D generation models (Trellis) for initial assets; performance is sensitive to their quality. Susceptible to failures in skeleton generation (Puppeteer) or initial 3D asset extraction. Not suitable for abstract objects without clear skeletal forms.",
        "Datasets / Benchmarks": "No specific dataset for *training* Muses as it's training-free. Initial 3D assets derived from models trained on large-scale datasets like Objaverse. Evaluated using CLIPScore, VQAScore, and user studies comparing against SOTA baselines.",
        "Results Summary": "Muses significantly outperforms SOTA methods (DreamBeast, Trellis-Text-to-3D, OmniPart) in CLIP (0.2878), VQA (0.9254), visual fidelity (66.67), and textual alignment (85.40). Generates diverse, high-quality, geometrically coherent, and style-consistent fantasy 3D creatures.",
        "Why It Matters": "Addresses challenges in generating creative, out-of-domain 3D content by leveraging 3D skeletons. Provides a flexible, training-free approach that enhances realism and coherence, opening new possibilities for interactive 3D editing, gaming, VR, and animation.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Generated Creatures), Figure 3 (Overview of Muses), Figure 5 (SOTA Comparison), Table 1 (Quantitative Comparison).",
        "arxiv_id": "2601.03256v1"
    },
    {
        "Title": "Choreographing a World of Dynamic Objects",
        "Field & Subfield": "Computer Vision & Graphics, 4D Scene Generation, Robotics",
        "Key Contributions": "- Novel 4D motion representation with Fenwick tree-inspired temporal structure & hierarchical DoF parameterization.\n- Distillation strategy for modern flow-based video generative models (RF-SDS).\n- Robust framework for physically-grounded 4D motions, applicable to diverse objects & robotic manipulation.",
        "Methodology": "Iteratively optimizes a hierarchical 4D scene motion representation (Gaussian splatting, Fenwick tree for temporal coherence, coarse-to-fine control points for spatial hierarchy) using guidance from a rectified flow-based video generative model (Wan 2.2) via a novel Score Distillation Sampling (RF-SDS) target, incorporating spatial/temporal regularizations.",
        "Strengths": "Universal, versatile, & category-agnostic 4D scene motion generation for multiple interacting objects. Produces physically-grounded motions. Supports long-horizon motion & real-world robot manipulation. Achieves high realism and prompt alignment compared to baselines.",
        "Limitations": "Dependent on underlying video generative model capabilities. Cannot generate newly appearing objects not in the initial scene. Suffers from extensive training time due to VAE backpropagation.",
        "Datasets / Benchmarks": "Object assets from Sketchfab/BlenderKit. Video generation model: Wan 2.2 (14B). Baselines: Animate3D, AnimateAnyMesh, MotionDreamer, TrajectoryCrafter. Evaluation: User studies & VideoPhy-2 [5] metrics (Semantic Adherence, Physical Commonsense).",
        "Results Summary": "CHORD achieves state-of-the-art performance in user studies for prompt alignment (87.71% preference) and motion realism (87.37% preference) in multi-object 4D scene generation, significantly outperforming existing methods. Qualitatively, it produces more natural and prompt-aligned motions.",
        "Why It Matters": "Enables scalable, realistic 4D animation of dynamic objects and complex scenes, crucial for building immersive 3D world models, embodied AI, and robotics manipulation. Overcomes limitations of category-specific and data-intensive methods for 4D content creation.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (4D scene motion examples), Figure 2 (Pipeline Overview), Table 1 (Quantitative comparisons with baselines).",
        "arxiv_id": "http://arxiv.org/abs/2601.04194v1"
    },
    {
        "Title": "Embedding Autonomous Agents in Resource-Constrained Robotic Platforms",
        "Field & Subfield": "Robotics & Multi-Agent Systems, Embedded Systems",
        "Key Contributions": "- Demonstrated embedding BDI autonomous agents on resource-constrained microcontrollers.\n- Integrated AgentSpeak logic with Pololu 3pi+ 2040 robot via Embedded-BDI framework.\n- Validated real-time decision-making efficiency for maze exploration on limited hardware.",
        "Methodology": "A BDI agent, programmed in AgentSpeak and compiled to C++ via Embedded-BDI framework, was embedded on a Pololu 3pi+ 2040 robot. The agent used line sensors for perception and actuators for movement, following a left-hand rule to solve a maze autonomously.",
        "Strengths": "Enables high-level agent-based control for real-time autonomous operation on resource-constrained hardware. Shows efficient reasoning cycles (sub-ms for belief/plan selection). Low memory footprint: 5.44% Flash, 6.25% RAM.",
        "Limitations": "Demonstrated on a specific robot for a single maze-solving task. Intention execution dominates cycle time due to sensor/motor operations. Scalability to more complex environments or multi-robot coordination is future work.",
        "Datasets / Benchmarks": "A custom-designed maze for line-following and intersection detection. Performance benchmarks measured belief update, plan selection, and intention execution times during maze solving.",
        "Results Summary": "The robot solved the maze in ~59 seconds, completing 287 reasoning cycles. Average belief update: 0.004 ms; plan selection: 0.024 ms; intention execution: 197 ms. Decision-making phases were efficient enough for real-time.",
        "Why It Matters": "Paves the way for deploying intelligent, autonomous agents on low-cost, resource-limited robots, reducing reliance on centralized control. It advances autonomy for applications like IoT and swarm robotics, enabling local, rational decision-making.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1: Digital design and real-world implementation of the maze. Figure 2: Annotated GPIO activity during agent execution, illustrating cycle phases.",
        "arxiv_id": "http://arxiv.org/abs/2601.04191v1"
    },
    {
        "Title": "Mesh4D: 4D Mesh Reconstruction and Tracking from Monocular Video",
        "Field & Subfield": "Computer Vision, 4D Mesh Reconstruction & Tracking",
        "Key Contributions": "- Novel feed-forward Mesh4D for complete 4D mesh reconstruction.\n- Compact latent space for entire animation sequence via VAE.\n- Skeletal structure guidance during VAE training (not inference).\n- Spatio-temporal attention for stable deformation.\n- New benchmark for 3D shape/motion quality.",
        "Methodology": "A feed-forward model builds on a pre-trained image-to-3D generator (Hunyuan3D 2.1) for static mesh M1. A novel VAE encodes deformation fields into a compact latent space, guided by skeletal structure. A latent diffusion model predicts full animation conditioned on video and M1.",
        "Strengths": "Accurate & complete 3D shape and motion reconstruction from monocular video. Temporally coherent 4D meshes with dense tracking. Generalizes to diverse scenarios without per-scene optimization. Robustness from large-scale pre-trained models.",
        "Limitations": "Relies on high-quality canonical mesh & skeletons for training. Inability to represent topological changes. Difficulty with extremely non-rigid objects. Does not choose optimal reference frame.",
        "Datasets / Benchmarks": "Objaverse-1.0 (filtered for dynamic objects) for training. Custom benchmark of 50 animated 3D assets from Objaverse for evaluation (disjoint from training).",
        "Results Summary": "Outperforms prior SOTA (HY3D 2.1, L4GM, GVFD) in 3D shape (IoU↑, P2S↓, Chamfer↓) and motion reconstruction (l2-Corr↓). Achieves SOTA on novel view synthesis (PSNR↑, SSIM↑, LPIPS↓, FVD↓), with better pose, geometry, and temporal consistency.",
        "Why It Matters": "Addresses the challenging problem of full 4D mesh reconstruction & tracking from monocular video, crucial for applications in computer vision, graphics, & robotics, by providing an efficient, feed-forward, and robust solution.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Method Illustration), Figure 2 (VAE Pipeline), Table 1 (Geometry & Tracking Results), Table 2 (NVS Results)",
        "arxiv_id": "2601.05251v1"
    },
    {
        "Title": "QNeRF: Neural Radiance Fields on a Simulated Gate-based Quantum Computer",
        "Field & Subfield": "Quantum Machine Learning, Computer Vision, Novel-View Synthesis",
        "Key Contributions": "• First hybrid quantum-classical QNeRF for novel-view synthesis. • Dual-Branch quantum embedding for scalability & noise tolerance. • Output scaling and local measurements mitigate quantum challenges. • Outperforms/matches NeRF with fewer parameters.",
        "Methodology": "Hybrid quantum-classical QNeRF. Classical embedding maps coordinates to quantum state, processed by QNN (parametric circuits). Full QNeRF vs. Dual-Branch (separated spatial/view-dependent encoding). Uses amplitude embedding, Ry rotations, entangling layers, parity measurements, output scaling.",
        "Strengths": "Full QNeRF: Outperforms NeRF in PSNR with <50% parameters. Dual-Branch: Comparable PSNR, higher noise tolerance, more scalable, fewer amplitudes. Both offer exponential data compression & hardware compatibility with existing rendering pipelines.",
        "Limitations": "High computational cost for quantum simulations (CPU-bound, slow). Limited by NISQ hardware (qubit count, gate depth, noise). Scalability challenges for Full QNeRF. Conservative noise analysis without advanced transpilation or error mitigation.",
        "Datasets / Benchmarks": "Blender dataset (materials, ficus, lego, drums), LLFF dataset (horns, fern, trex, room). Compared against classical NeRF. Evaluated using PSNR, SSIM, and State Fidelity for noise analysis.",
        "Results Summary": "Full QNeRF (8 qubits) outperforms classical NeRF (31.67 vs 29.53 dB PSNR on Blender) with <50% parameters. Dual-Branch QNeRF achieves comparable performance with higher noise tolerance. Output scaling is crucial for performance.",
        "Why It Matters": "First QML application for novel-view synthesis, demonstrating quantum models can achieve competitive performance with increased compactness. Highlights QML's potential for expressive signal representation and future quantum advantage in 3D computer vision tasks.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (PSNR vs Complexity, Fidelity, Synthesis example), Figure 2 (Architectural Schemes), Table 2 (PSNR Comparison), Figure 5 (Noise Resilience)."
    },
    {
        "Title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
        "Field & Subfield": "Computer Vision & Image Processing (Color Constancy, Reinforcement Learning)",
        "Key Contributions": "- Novel statistical color constancy algorithm (SGP-LRD) for nighttime.\n- First deep reinforcement learning (RL-AWB) for adaptive AWB parameter tuning.\n- Introduces LEVI, the first multi-camera nighttime color constancy dataset.\n- Demonstrates superior cross-sensor generalization with minimal training data.",
        "Methodology": "Hybrid approach combining SGP-LRD (Salient Gray Pixels with Local Reflectance Differences) statistical algorithm with Soft Actor-Critic (SAC) reinforcement learning. Uses two-stage curriculum learning for adaptive parameter optimization (N%, Minkowski p).",
        "Strengths": "Achieves state-of-the-art performance in low-light/nighttime, superior cross-sensor generalization, high data efficiency (5-shot learning), adaptive per-image parameter optimization, robust to noise/complex lighting.",
        "Limitations": "Currently controls only two AWB parameters, potential for over-correction in some challenging scenes, training pipeline not yet fully GPU-resident, increasing action space adds complexity.",
        "Datasets / Benchmarks": "Introduced LEVI (700 images, multi-camera, nighttime, color checker annotations). Evaluated on NCC (existing nighttime) and Gehler-Shi (daytime) datasets.",
        "Results Summary": "RL-AWB achieved best median/mean angular errors in 5-shot setting on NCC/LEVI datasets. Showed significant reduction in cross-dataset generalization errors compared to learning-based baselines, outperforming fully supervised methods.",
        "Why It Matters": "Addresses the critical and challenging problem of AWB in low-light, noisy nighttime scenes, improving image quality for real-world applications like mobile photography and surveillance, with strong cross-sensor adaptability.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (RL-AWB overview), Figure 6 (Cross-dataset qualitative comparison), Table 1 (In-dataset evaluation), Table 2 (Cross-dataset evaluation).",
        "arxiv_id": "http://arxiv.org/abs/2601.05249v1"
    },
    {
        "Title": "SecureCAI: Injection-Resilient LLM Assistants for Cybersecurity Operations",
        "Field & Subfield": "Cybersecurity, AI Safety, Large Language Models",
        "Key Contributions": "• Formalized threat model for LLM injection attacks in SOCs. • SecureCAI: security-aware CAI with adaptive evolution via red-teaming. • DPO for unlearning unsafe responses while preserving task performance. • Extensive validation showing significant ASR reduction & utility.",
        "Methodology": "SecureCAI extends Constitutional AI with security-aware guardrails, adaptive constitution evolution (red-teaming), & DPO for unlearning unsafe patterns. This ensures both safety & task performance in adversarial cybersecurity contexts.",
        "Strengths": "94.7% ASR reduction, 95.1% clean accuracy. High constitutional adherence & strong generalization to unseen attacks. Adaptive defense via continuous red-teaming. Effectively balances security & utility, unlike filtering methods.",
        "Limitations": "Increased inference latency (23%). Residual vulnerability to novel attacks. Effectiveness depends on principle quality. Potential instability/overhead during adaptive constitution updates.",
        "Datasets / Benchmarks": "Security Artifact Corpus (logs, emails, malware) + 51,750 adversarial preference pairs for training. Evaluated on 51,750 adversarial samples (6 categories) & 15,000 benign samples. Held-out 5,000 samples for unseen attack generalization.",
        "Results Summary": "Reduced attack success rate by 94.7% (to 4.3% avg ASR) vs. baseline (80.4%). Maintained 95.1% clean accuracy on security tasks. Achieved >0.92 Constitutional Adherence Score under high adversarial pressure. Generalizes well to unseen attacks.",
        "Why It Matters": "Addresses a critical gap in AI safety for adversarial cybersecurity, enabling trustworthy LLM integration in Security Operations Centers. Provides robust defense against prompt injection, crucial for automating threat analysis in high-stakes environments.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Table 2 (Attack Success Rates), Table 3 (Clean Accuracy), Figure 4 (Constitutional Adherence Score under Adversarial Pressure), Figure 2 (SecureCAI Architecture).",
        "arxiv_id": "http://arxiv.org/abs/2601.07835v1"
    },
    {
        "Title": "Tuning-free Visual Effect Transfer across Videos",
        "Field & Subfield": "Computer Vision, Video Generation & Editing",
        "Key Contributions": "- RefVFX: tuning-free framework for reference-based temporal video effect transfer. \n- Large-scale, effect-aligned dataset: 120K triplets covering 1700+ effects.\n- Multi-source conditioning architecture on diffusion backbones. \n- Extensive evaluation demonstrating superior coherence & generalization.",
        "Methodology": "A diffusion-based video generation model (RefVFX) conditioned on reference video dynamics, input video/image content, and text prompts. Trained on a new large-scale triplet dataset (reference, input, output) compiled from LoRA I2V, custom V2V, and programmatic effects.",
        "Strengths": "Produces visually consistent & temporally coherent edits. Generalizes across unseen effect categories. Outperforms prompt-only baselines in human preference. Operates in a feed-forward manner without inference-time optimization.",
        "Limitations": "Struggles with fine-grained occlusions or complex subject-effect interactions. Dataset is primarily human-centric, limiting generalization to environmental/abstract effects. Inference is computationally expensive (approx. double baseline).",
        "Datasets / Benchmarks": "Introduces a new large-scale triplet dataset (~120K triplets, 1700+ effects). Uses LoRA-based models, Senorita dataset. Evaluated with VideoPrism and CLIP embeddings, human preference studies, and VBench metrics.",
        "Results Summary": "RefVFX achieved 78-98% win rates in human preference over baselines for reference adherence and maintains input video adherence. Achieved higher reference video embedding similarity compared to prompt-only baselines, demonstrating effective temporal effect transfer.",
        "Why It Matters": "Enables transfer of complex, dynamic temporal visual effects between videos using a reference video, a capability challenging for existing text/keyframe-based methods. This broadens video editing possibilities beyond static or semantic edits.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Overview of RefVFX), Figure 2 (Dataset Examples), Figure 5 (Architecture Overview), Tables 1 & 2 (User Study Results)",
        "arxiv_id": "2601.07833v1"
    },
    {
        "Title": "3AM: Segment Anything with Geometric Consistency in Videos",
        "Field & Subfield": "Computer Vision; Video Object Segmentation, 3D Instance Segmentation",
        "Key Contributions": "• Bridges 2D tracking & 3D consistency gap. • Integrates 3D-aware features (MUSt3R) with SAM2's appearance features via Feature Merger. • Employs Field-of-View aware training for 3D correspondence. • Validated substantial improvements on challenging wide-baseline datasets.",
        "Methodology": "3AM enhances SAM2 by integrating multi-level 3D-aware features from MUSt3R via a Feature Merger. It uses a Field-of-View aware sampling strategy during training to learn 3D consistency from posed RGB video. Requires only RGB input at inference, no camera poses or depth.",
        "Strengths": "Achieves geometry-consistent object tracking robust to large viewpoint changes, occlusions, and reappearance. Maintains promptability & efficiency without requiring explicit 3D supervision or camera poses at inference. Outperforms SOTA VOS significantly.",
        "Limitations": "Full FOV-aware sampling can degrade SAM2's original within-view feature-matching. MUSt3R features require precomputation for FOV sampling on some datasets. Original SAM2 memory can be mistrained without MUSt3R grounding.",
        "Datasets / Benchmarks": "Training: ScanNet++, ASE, MOSE. Evaluation: ScanNet++ (Whole/Selected Subset), Replica. Compared to SAM2, SAM2Long, DAM4SAM, SegMASt3R.",
        "Results Summary": "On ScanNet++ Selected Subset, 3AM achieved 90.6% IoU (+15.9 over SOTA) and 71.7% Positive IoU (+30.4 over SOTA). Consistently outperforms SAM2, SAM2Long, and DAM4SAM across ScanNet++ and Replica for VOS and 3D instance segmentation.",
        "Why It Matters": "Enables robust, viewpoint-consistent video object segmentation for embodied AI, robotics, AR, and video editing by combining 2D model efficiency with 3D geometric awareness, without explicit 3D inputs at inference.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (3AM Overview), Figure 3 (Pipeline), Figure 5 (Sampling Strategy), Table 2 (ScanNet++ Results), Table 4 (Replica Results).",
        "arxiv_id": "http://arxiv.org/abs/2601.08831v1"
    },
    {
        "Title": "Modeling LLM Agent Reviewer Dynamics in Elo-Ranked Review System",
        "Field & Subfield": "Artificial Intelligence & Natural Language Processing (LLM Agents, Peer Review Simulation)",
        "Key Contributions": "Introduces LLM agent reviewer simulation w/ Elo-ranked system. Improves Area Chair (AC) decision accuracy. Reveals reviewer adaptive strategies for Elo, not always quality. Offers insights into Elo-ranked peer review benefits & challenges.",
        "Methodology": "Multi-round simulation using Gemini-2.5-Flash LLM agents with 6 personas. Elo-ranked system based on AC quality ratings; reviewers adapt strategies via memory. Compared Baseline, AC Access, & Full Access Elo settings on ICLR 2025 papers.",
        "Strengths": "Simulates complex multi-round peer review, revealing dynamics unobservable in reality. Shows how Elo improves AC decisions & identifies reviewer quality. Offers insights into incentive design for better peer review systems.",
        "Limitations": "Limited review rounds restrict long-term convergence analysis, focusing on short-horizon dynamics. Reviewers adapt strategically for Elo, potentially without substantive review quality improvement, highlighting design sensitivity.",
        "Datasets / Benchmarks": "150 real-world ICLR 2025 paper submissions, sampled uniformly from different average rating intervals and filtered for high rating variance.",
        "Results Summary": "Elo-based feedback greatly improves AC decision accuracy (up to 0.70 Acc). Elo introduces clear reviewer stratification; Experts dominate, low-effort Skimmers are penalized. Visible Elo incentivizes strategic adaptation for ratings, not always quality.",
        "Why It Matters": "Provides a novel framework to study peer review dynamics & incentive mechanisms using LLM agents. Highlights Elo's potential to improve review quality & AC decisions but also reveals challenges of strategic behavior, informing system design.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1: Elo-ranked Review System Dynamic. Figure 3: Elo Rating Dynamics of Personas. Table 1: Decision Performance Across Setups.",
        "arxiv_id": "http://arxiv.org/abs/2601.08829v1"
    },
    {
        "Title": "RAVEN: Erasing Invisible Watermarks via Novel View Synthesis",
        "Field & Subfield": "Computer Vision; AI Security & Image Forensics",
        "Key Contributions": "Novel view synthesis attack vector. Zero-shot diffusion framework with latent viewpoint modulation. View-guided attention for structural consistency. SOTA watermark suppression across 15 methods with high perceptual quality.",
        "Methodology": "Zero-shot diffusion model uses partial inversion, latent viewpoint modulation (geometric shifts), & view-guided attention to synthesize a new view, disrupting watermarks while preserving content. Followed by CIELAB color/contrast transfer. Black-box, no retraining.",
        "Strengths": "Novel approach with SOTA suppression & high perceptual fidelity. Operates black-box without detector access or training data. Efficient (seconds/image on single GPU). Generalizable across diffusion models.",
        "Limitations": "Trade-off between watermark suppression strength and visual quality/artifacts. Primarily focuses on subtle, global viewpoint shifts rather than large geometric changes. Does not reconstruct unseen geometry.",
        "Datasets / Benchmarks": "MS-COCO, SD-Prompts, DiffusionDB. Evaluated against 15 watermarking schemes & 14 baseline attacks (Regen, CtrlGen+, IRA, UnMarker, etc.).",
        "Results Summary": "Achieves SOTA watermark suppression (e.g., 0.026 avg TPR@1%FPR on MS-COCO, >60% better than baselines) with superior perceptual fidelity (low FID, high CLIP-Text scores). Preserves fine details and natural textures without artifacts.",
        "Why It Matters": "Exposes a critical vulnerability in invisible watermarking to semantic-preserving geometric transformations. This necessitates more robust watermarking designs and improves assessment of AI-generated content provenance.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Table 1 (Method Comparison), Figure 1 (RAVEN Pipeline), Table 2/6 (Watermark Removal Performance), Table 3 (Image Quality Metrics), Figures 2/6/7/8 (Qualitative Comparisons)."
    },
    {
        "Title": "Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning",
        "Field & Subfield": "Vision-Language-Action (VLA) Models, Embodied AI, Robotics, Efficient Reasoning",
        "Key Contributions": "Efficient VLA reasoning via verbalizable latent planning. Preference-guided distillation with trajectory alignment. Bridges high-level visual planning to low-level action. Up to 89.3% inference latency reduction over SOTA VLAs.",
        "Methodology": "Teacher-student framework distills explicit CoT into compact latent tokens. Uses preference-guided distillation with a verbalizer LLM. Incorporates action-aligned visual plan distillation and latent reasoning for trajectory prediction. Guides action model with visual latent planning.",
        "Strengths": "Significantly reduces inference latency (up to 89.3%). Maintains/improves SOTA performance across diverse benchmarks. Enables effective long-horizon planning, few-shot adaptation, and robust failure recovery. Compact and expressive latent reasoning.",
        "Limitations": "Verbalizer LLM inherits language model limitations like hallucination. Verbalizer is only for interpretability, not direct action execution, thus not affecting task performance during inference. Future work can improve verbalized reasoning faithfulness.",
        "Datasets / Benchmarks": "Trained on MolmoAct, AIST, PixMo, RoboFAC, RoboVQA, ShareRobot, EgoPlan-Bench, Video-R1-CoT, OXE, ALOHA. Evaluated on EgoPlan-Bench2, RoboVQA, OpenEQA, RoboFAC (reasoning); SimplerEnv-Google, LIBERO, RoboTwin2.0 (manipulation).",
        "Results Summary": "Achieves up to 89.3% inference latency reduction. Outperforms SOTA VLAs on LIBERO (89.7%) and SimplerEnv (68.7%). Higher success rates on RoboTwin2.0 long-horizon tasks. Surpasses GPT-4V/Gemini on embodied reasoning. Demonstrates long-horizon planning, few-shot adaptation, & failure recovery.",
        "Why It Matters": "Addresses critical real-time inference latency for embodied AI. Enables more practical and deployable intelligent agents for robotics/autonomous driving. Provides efficient, robust high-level planning and low-level action execution in complex environments.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Overview), Figure 3 (Latency & Success), Table 5 (Detailed latency/performance).",
        "arxiv_id": "http://arxiv.org/abs/2601.09708v1"
    },
    {
        "Title": "Alterbute: Editing Intrinsic Attributes of Objects in Images",
        "Field & Subfield": "Computer Vision & Image Generation/Editing",
        "Key Contributions": "- Method for editing any intrinsic object attribute (color, texture, material, shape) using a single model, preserving identity.\n- Introduces Visual Named Entities (VNEs) for object identity, extracted automatically using a VLM.\n- Relaxed training objective from intrinsic+extrinsic changes, but restricts to intrinsic at inference.",
        "Methodology": "Diffusion-based model fine-tuned for text-guided intrinsic attribute editing. Conditions on identity reference (VNE cluster), text prompt, background, & object mask. Relaxes training to allow intrinsic/extrinsic changes, but constrains to intrinsic-only at inference. VNEs and attribute descriptions are automatically extracted via a VLM (Gemini) from OpenImages.",
        "Strengths": "Preserves object identity and scene context effectively. Supports editing all intrinsic attributes (color, texture, material, shape) with a single unified model. Automated data curation pipeline using VLM for VNEs enables scalable supervision. Outperforms existing general-purpose and attribute-specific editors.",
        "Limitations": "May produce background artifacts with coarse bounding box masks. Reshaping rigid objects can be challenging, sometimes yielding unrealistic or unintended geometries. Intrinsic attributes can exhibit natural dependencies, limiting some combinations.",
        "Datasets / Benchmarks": "OpenImages dataset for VNE extraction and training. Custom evaluation set of 30 distinct objects (100 samples) for qualitative and quantitative comparisons. User studies and VLM-based evaluations (Gemini, GPT-40, Claude 3.7 Sonnet).",
        "Results Summary": "Alterbute significantly outperforms baselines in user studies and VLM-based evaluations, with users preferring its outputs across all comparisons. Achieves state-of-the-art results for identity-preserving intrinsic attribute editing, demonstrating high-quality, targeted edits.",
        "Why It Matters": "Addresses the challenging problem of editing intrinsic object properties while preserving identity, a key limitation of prior image editing methods. Enables more realistic and controlled image manipulation, pushing capabilities of generative models in practical applications.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Intrinsic attribute edits overview), Figure 2 (Alterbute architecture), Table 1 (Preference rates (%) for Alterbute vs. baselines).",
        "arxiv_id": "http://arxiv.org/abs/2601.10714v1"
    },
    {
        "Title": "∂∞-GRID: A NEURAL DIFFERENTIAL EQUATION SOLVER WITH DIFFERENTIABLE FEATURE GRIDS",
        "Field & Subfield": "Machine Learning & Scientific Computing; Neural PDE Solvers",
        "Key Contributions": "- Novel differentiable grid-based representation for DEs.\n- Combines feature grid efficiency with infinitely differentiable Radial Basis Function (RBF) interpolation.\n- Introduces multi-resolution co-located grids for stable, faster global gradients.\n- Solves higher-order PDEs via implicit training with DEs as loss.",
        "Methodology": "Proposed ∂∞-Grid uses a multi-resolution feature grid with learnable feature vectors at nodes. RBF interpolation, which is infinitely differentiable, extracts query-dependent features. A small decoder maps features to the target signal. Trained implicitly by minimizing DE residuals as loss, using automatic differentiation for derivatives.",
        "Strengths": "5-20x speed-up over coordinate-based MLPs (e.g., Siren) with comparable/better accuracy. Supports higher-order derivatives essential for complex PDEs. Captures high-frequency details. Versatile across diverse physical problems. Efficient computation via localized learning.",
        "Limitations": "RBF interpolation can be computationally intensive and sensitive to shape parameter choice. May introduce boundary errors (Runge's phenomenon). Suffers from curse of dimensionality in higher-dimensional inputs. Not as efficient as traditional numerical solvers.",
        "Datasets / Benchmarks": "Poisson Equation (Image Reconstruction), Helmholtz Equation (Wave Fields), Kirchhoff-Love (Cloth Simulation), Eikonal Equation (SDFs), Heat Equation, Advection Equation. Compared against Siren, K-Planes, Instant-NGP, NeuRBF, PINNs, traditional numerical solvers.",
        "Results Summary": "Achieves 5-20x faster training than MLP-based methods (Siren) for PDEs, while maintaining comparable accuracy. Outperforms linear-interpolation grid methods (K-Planes) for higher-order derivative tasks. Successfully reconstructs high-frequency fields and solves challenging non-linear PDEs.",
        "Why It Matters": "Addresses key limitations of neural PDE solvers (slow training, inability to handle higher-order derivatives for grid methods). Offers a robust, efficient, and versatile framework for physics-informed machine learning and accurate physical field modeling.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Helmholtz/Cloth Sim results & training time), Figure 4 (Image reconstruction comparison & PSNR/training time plots), Table 1 (Quantitative comparison for image reconstruction)."
    },
    {
        "Title": "WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments",
        "Field & Subfield": "Computer Vision & Novel View Synthesis",
        "Key Contributions": "- Self-supervised NVS framework for dynamic scenes w/o camera pose or dynamic mask supervision.\n- Curated Dynamic RealEstate10K (D-RE10K) dataset (15K sequences) & D-RE10K-iPhone benchmark.\n- Outperforms baselines in NVS quality & motion segmentation.",
        "Methodology": "Analysis-by-synthesis approach. Static renderer explains rigid structure; residuals identify transient regions. Pseudo motion masks derived from DINOv3 features & SSIM. Distills motion estimator. Masked 3D scene encoder prevents dynamic content. Progressive training stages & copy-paste augmentation.",
        "Strengths": "Self-supervised, handles dynamic environments (camera & object motion) without 3D or mask ground-truth. Achieves superior NVS quality & motion segmentation. Feed-forward inference. New large-scale dynamic datasets. Robust to open-set distractors.",
        "Limitations": "Pseudo motion masks may not enforce instance-level segmentation, often capturing only moving parts. Can under-segment small objects. Mask quality degrades when moving objects are too large in input images.",
        "Datasets / Benchmarks": "New: Dynamic RealEstate10K (15K dynamic indoor videos), D-RE10K-iPhone (50 real-world paired transient/clean sequences). Used for pretraining/augmentation: RealEstate10K, COCO.",
        "Results Summary": "WildRayZer consistently outperforms optimization-based & feed-forward baselines in NVS (PSNR, SSIM, LPIPS) and motion mask accuracy (mIoU, Recall) on D-RE10K & D-RE10K-iPhone. Achieves sharper background reconstructions & reliable transient removal.",
        "Why It Matters": "Enables scalable, robust novel view synthesis in complex, dynamic real-world environments without requiring extensive manual supervision, bridging a critical gap for \"in-the-wild\" applications in 3D vision.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (WildRayZer Architecture), Figure 4 (Qualitative Comparisons), Table 2 (Main NVS Results), Table 4 (Motion Mask Quality).",
        "arxiv_id": "http://arxiv.org/abs/2601.10716v1"
    },
    {
        "Title": "Implicit Neural Representation Facilitates Unified Universal Vision Encoding",
        "Field & Subfield": "Computer Vision & Representation Learning",
        "Key Contributions": "- Designed and trained HUVR, an INR hyper-network, matching/outperforming DINOv3 for classification, segmentation, and reconstruction.\n- Introduced compressed Tiny Tokens (TinToks), offering +48% ImageNet classification over DINOv3 PCA baseline.\n- Developed improved hyper-network design for image INRs, yielding +2.34 PSNR with 10% training time.\n- Unified recognition and generative task families.",
        "Methodology": "A Hyper-network for Unified Visual Representation (HUVR) predicts Implicit Neural Representation (INR) weights from image inputs for fast, accurate reconstruction. It integrates knowledge distillation from pre-trained vision encoders (like DINOv3) for generalization and semantics. Novel patch-wise processing, a global token for recognition, and learnable downsampling/upsampling layers create compressed \"Tiny Tokens\" for efficient representation.",
        "Strengths": "Unified model for both recognition and generation tasks. Achieves state-of-the-art results in multiple recognition benchmarks (ImageNet, ADE20K) and reconstruction. Enables highly compressed (96x) representations (TinToks) with superior classification. Novel INR hyper-network design is more effective for image INRs.",
        "Limitations": "Pre-training scale/scope not as extensive as SOTA methods (e.g., SigLIP 2, DINOv3). Does not use specialized curated data or engineer distillation from multiple models. Generative capabilities, while present, do not yet match state-of-the-art diffusion models.",
        "Datasets / Benchmarks": "ImageNet, ImageNet22k, DataComp (pre-training). ImageNette, Celeb-A, LSUN Churches (INR comparison). ADE20K (semantic segmentation), NYUv2 (depth estimation). FGVC datasets (Caltech-UCSD Birds, Stanford Cars, DTD, Oxford 102 Flower, Food-101).",
        "Results Summary": "HUVR achieved +0.4% ImageNet classification, +1.2 mIoU ADE20K, and +4.84 PSNR for reconstruction vs. DINOv3. Tiny Tokens (8-dim) show +48% ImageNet classification vs. DINOv3 PCA baseline and +1.26 PSNR vs. Stable Diffusion VAE. Superior dense recognition with TinToks. Achieves SOTA INR reconstruction on ImageNette/LSUN/CelebA.",
        "Why It Matters": "This work unifies two major branches of vision representation learning (recognition and generation) into a single, efficient model. It enables highly compressed yet performant representations, crucial for scaling vision systems and resource-constrained applications, highlighting INR hyper-networks as a viable strategy for universal vision encoding.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Unified modeling & reconstruction), Table 1 (Tiny token results vs. PCA baselines), Table 3 (Standard classification results), Table 4 (Dense recognition results).",
        "arxiv_id": "2601.14256v1"
    },
    {
        "Title": "Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis",
        "Field & Subfield": "Computer Vision, 4D Synthesis, 3D Motion Reconstruction",
        "Key Contributions": "- Novel feed-forward 4D synthesis framework from monocular video.\n- Decomposes 4D synthesis into 3D shape generation and motion reconstruction.\n- Scalable frame-wise transformer architecture robust to varying sequence lengths and mesh resolutions.\n- Introduces new Motion-80 benchmark with ground-truth data.",
        "Methodology": "Feed-forward framework. Decomposes 4D synthesis into static 3D shape generation and dynamic motion reconstruction. Uses a canonical reference mesh (generated/provided) and DINOv2 video features. Learns compact motion latent representation via alternating attention. Predicts per-frame vertex trajectories with a cross-attention decoder. Trained with MSE loss.",
        "Strengths": "Achieves superior fidelity & spatial consistency over prior work. Strong generalization despite limited 4D training data. Robust to varying sequence lengths and mesh resolutions via scalable architecture. Efficient motion representation learning. Can animate artist-created static 3D meshes and handle in-the-wild videos.",
        "Limitations": "Geometry encoder on dense point clouds without explicit mesh topology can cause vertex sticking. Relies on first-frame mesh as reference, struggling with topology changes in later frames.",
        "Datasets / Benchmarks": "New Motion-80 dataset (80 subjects from Objaverse, ground-truth motion/geometry). Consistent4D benchmark (rendering-based metrics). Trained on filtered Objaverse-XL models.",
        "Results Summary": "Outperforms SOTA (L4GM, GVFD, V2M4) in geometric accuracy (CD, F-Score) and appearance (CLIP, DreamSim, FVD) on Motion-80 and Consistent4D. Delivers superior temporal coherence and structural consistency, generating plausible 3D geometry and realistic motion.",
        "Why It Matters": "Addresses the challenging 4D synthesis problem. Enables efficient generation of high-quality, temporally consistent 4D dynamic objects from a single video, crucial for VR, cinematography, robotics, and simulation. Allows motion transfer to static 3D assets.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Table 2 (Quantitative evaluation on Motion-80 set), Table 3 (Quantitative evaluation on Consistent4D), Figure 3 (Geometric comparison), Figure 4 (Qualitative Comparisons).",
        "arxiv_id": "http://arxiv.org/abs/2601.14253v1"
    },
    {
        "Title": "VideoMaMa: Mask-Guided Video Matting via Generative Prior",
        "Field & Subfield": "Computer Vision - Video Matting & Generative Models",
        "Key Contributions": "- VideoMaMa: Diffusion model for high-quality alpha mattes from binary masks, enabling scalable label creation.\n- MA-V Dataset: First large-scale (50K+ videos), high-quality video matting dataset built on real captured footage via pseudo-labeling.",
        "Methodology": "Diffusion-based Video Mask-to-Matte Model (VideoMaMa) built on Stable Video Diffusion. Uses two-stage training (spatial then temporal layers) & semantic knowledge injection (DINOv3 features). Mask augmentation prevents 'copy-paste' behavior.",
        "Strengths": "Strong zero-shot generalization to real-world footage despite synthetic training. Robust to diverse input mask qualities. Enables scalable, high-quality video matting annotation. Bridges synthetic-to-real domain gap.",
        "Limitations": "Cannot generate mattes for regions where the input mask is fundamentally incorrect (e.g., wrong object instance). Relies on input mask to define the target foreground object.",
        "Datasets / Benchmarks": "Introduced MA-V (Matting Anything in Videos) dataset with over 50K real videos. Evaluated on V-HIM60, YouTubeMatte. Used SA-V for MA-V construction. Fine-tuned SAM2 on MA-V.",
        "Results Summary": "VideoMaMa consistently outperforms existing mask-guided matting models. SAM2-Matte, fine-tuned on MA-V, achieves SOTA robustness on in-the-wild videos, validating MA-V's quality. Two-stage training and DINO features are critical.",
        "Why It Matters": "Addresses the severe data scarcity in video matting by providing a scalable pseudo-labeling pipeline and a large, diverse dataset (MA-V), bridging the synthetic-to-real domain gap for more robust, generalizable models.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (VideoMaMa examples), Figure 4 (MA-V examples), Figure 5 (Qualitative comparison), Table 1 (Dataset statistics), Table 2 (All-frame matting comparison), Table 3 (First-frame matting comparison).",
        "arxiv_id": "2601.14255v1"
    },
    {
        "Title": "Attribute-Preserving Pseudo-Labeling for Diffusion-Based Face Swapping",
        "Field & Subfield": "Computer Vision; Image Synthesis & Face Generation",
        "Key Contributions": "- Proposes APPLE: a diffusion-based teacher-student framework for attribute-preserving face swapping.\n- Reformulates face swapping as conditional deblurring instead of inpainting for better attribute preservation.\n- Introduces attribute-aware inversion for enhanced fine-grained attribute preservation.\n- Achieves SOTA attribute preservation & competitive identity transfer, practical for real-world use.",
        "Methodology": "A teacher-student diffusion framework. Teacher uses conditional deblurring on blurred target images + multi-level structural cues (3DMM, gaze, masks) instead of inpainting. Attribute-aware inversion extracts attribute-rich pseudo-labels from the teacher. Student learns from these pseudo-triplets with direct supervision.",
        "Strengths": "Achieves state-of-the-art attribute preservation (pose, expression, skin tone, lighting, makeup, accessories) while maintaining competitive identity transfer. Produces photorealistic, target-faithful results without external conditions at inference. Robust to occlusions when using augmented pseudo-triplets.",
        "Limitations": "Relies on external modules (3DMM, gaze landmarks, segmentation maps) for pseudo-triplet generation; errors can propagate. Diffusion model inference efficiency needs improvement. Not yet extended to video face swapping.",
        "Datasets / Benchmarks": "Trained on VGGFace2-HQ. Evaluated on FFHQ dataset (1,000 source/target pairs). Metrics: FID, ID Similarity, ID Retrieval (Top-1/Top-5), Pose L2 distance, Expression L2 distance.",
        "Results Summary": "APPLE achieves the lowest FID (2.18), superior pose (1.85) and expression (0.64) consistency, and competitive ID similarity (0.54) among all baselines. Qualitatively, it produces more photorealistic images with subtle attribute preservation (e.g., makeup, glasses, gaze) compared to GAN and other diffusion methods.",
        "Why It Matters": "Face swapping is crucial for digital content, privacy, and film. APPLE advances the field by overcoming the challenge of attribute preservation in diffusion models, producing highly realistic and faithful swapped faces without artifacts, which was a major limitation of prior GAN- and diffusion-based methods.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (APPLE outputs), Figure 2 (Deblurring vs. Inpainting), Table 4 (Quantitative SOTA comparison)."
    },
    {
        "Title": "Iterative Refinement Improves Compositional Image Generation",
        "Field & Subfield": "Computer Vision; Text-to-Image Generation",
        "Key Contributions": "- Proposes an iterative test-time strategy for compositional T2I generation.\n- Leverages a VLM critic for feedback and an image editor for sequential refinement.\n- Achieves significant quantitative gains across multiple benchmarks.\n- Draws inspiration from Chain-of-Thought reasoning in LLMs for T2I.",
        "Methodology": "An iterative inference-time scheme with T2I generator, VLM critic (proposes corrections/actions), image editor, and verifier. Operates across T refinement rounds & M parallel streams under a compute budget. Actions include STOP, BACKTRACK, RESTART, CONTINUE.",
        "Strengths": "Simple and training-free, requires no external tools/priors, broadly applicable to diverse T2I models. Consistently outperforms parallel sampling, especially for complex compositional tasks. Offers state-of-the-art performance.",
        "Limitations": "VLM critic/verifier can exhibit faulty reasoning, leading to incorrect corrections or unnecessary refinements. Image editor's inability to make desired changes for complex images can also limit effectiveness.",
        "Datasets / Benchmarks": "ConceptMix (k=1-7), T2I-CompBench (3D-Spatial, numeracy), TIIF-Bench, Visual Jenga scene decomposition task. Evaluated on Qwen-Image, Nano-Banana, GPT-Image models.",
        "Results Summary": "16.9% all-correct rate improvement on ConceptMix (k=7), 13.8% on T2I-CompBench (3D-Spatial), 12.5% on Visual Jenga. Human evaluators preferred our method 58.7% vs. 41.3% for parallel baseline. Full solve rate on Visual Jenga improved from 64.29% to 76.79%.",
        "Why It Matters": "Enables T2I models to perform self-correction, a capability common in LLMs but lacking in T2I. Decomposes complex prompts into sequential, manageable steps, enhancing robustness for compositional generation. Points towards unified reasoning across modalities.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Visual comparison of standard vs. iterative refinement); Table 1 (Quantitative performance across benchmarks and models).",
        "arxiv_id": "http://arxiv.org/abs/2601.15286v1"
    },
    {
        "Title": "Towards Understanding Best Practices for Quantization of Vision-Language Models",
        "Field & Subfield": "Artificial Intelligence & Multimodal Deep Learning, Model Compression",
        "Key Contributions": "- Component sensitivity varies substantially across MLLM architectures (LLM often more critical).\n- SOTA methods (GPTQ, AWQ) preserve performance at 3.5-4.5 bpw, outperforming uniform quantization.\n- Task characteristics dictate optimal bit allocations.\n- Quantization method significantly alters component importance (AWQ favors LLM, GPTQ more balanced).\n- Architectural dependencies and component interplay shape quantization patterns.",
        "Methodology": "Systematic investigation of k-bit uniform quantization and SOTA methods (GPTQ, AWQ) on BLIP-2 and LLaVA. Explored quantization across model components (ViT, Q-Former, LLM), block groups, and layer types. Employed Random Forest, Permutation Importance, and SHAP for component importance analysis with consensus ranking.",
        "Strengths": "Comprehensive and systematic ablation studies on MLLM quantization. Compares multiple SOTA methods across diverse tasks & architectures. Provides practical insights into component sensitivities and optimal bit allocation. Uses robust feature importance techniques.",
        "Limitations": "Study limited to simulated quantization; does not capture end-to-end latency or hardware-specific optimizations. Focuses on specific MLLM architectures and calibration-based PTQ methods.",
        "Datasets / Benchmarks": "COCO Captions, Flickr (text-to-image retrieval), VQAv2, GQA (Visual Question Answering). Evaluated BLIP-2 ViT-g and LLaVA 1.5 7B.",
        "Results Summary": "SOTA methods achieve high accuracy at 3.5-4.5 bpw. LLM is often most sensitive, especially for reasoning tasks. AWQ prioritizes LLM preservation, while GPTQ distributes importance more evenly. ViT and LLM show comparable performance impact despite size differences. Quantizing multiple components yields worse performance than single component.",
        "Why It Matters": "Provides crucial guidance for efficient deployment of MLLMs on resource-constrained devices by optimizing quantization strategies. Highlights the need for holistic pipeline analysis and task-aware bit allocation to improve accessibility and reduce costs of advanced AI models.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 4 (SOTA Quantization Performance-Size Tradeoff), Figure 7 (BLIP-2 Component Performance), Table 1 (Consensus feature-importance (%) of model components).",
        "arxiv_id": "http://arxiv.org/abs/2601.15287v1"
    },
    {
        "Title": "CamPilot: Improving Camera Control in Video Diffusion Model with Efficient Camera Reward Feedback",
        "Field & Subfield": "Computer Vision & Video Generation, 3D Reconstruction",
        "Key Contributions": "- Efficient camera-aware 3D decoder mapping video latent + camera pose to 3D Gaussians for reward.\n- Camera reward optimization minimizing mask-aware pixel-level differences for video-camera alignment.\n- Extensive experiments demonstrating effectiveness & improved camera controllability.",
        "Methodology": "CamPilot leverages an efficient camera-aware 3D decoder that maps video latents and camera poses to 3D Gaussians. It uses Reward Feedback Learning (ReFL) with a visibility-aware pixel-level consistency reward to optimize camera controllability in video diffusion models.",
        "Strengths": "Significantly improves camera controllability & visual quality. Efficient 3D decoder for reward computation. Addresses 2D-to-3D gaps. Outperforms existing methods on various metrics. Visibility mask handles stochasticity.",
        "Limitations": "Performance is bounded by the 3D decoder. Currently trained only on static scene datasets (RE10K). Not suitable for dynamic scene reconstruction due to 3DGS limitations. Higher computational overhead if Class-Free Guidance (CFG) is used.",
        "Datasets / Benchmarks": "RealEstate10K (training & testing), WorldScore (static benchmark for out-of-domain evaluation).",
        "Results Summary": "Outperforms baselines (MotionCtrl, CameraCtrl, ViewCrafter, FlexWorld) in video generation (lower FID, FVD, Rerr, Terr) and 3D scene generation (higher PSNR, SSIM, lower LPIPS), demonstrating improved camera controllability & visual quality.",
        "Why It Matters": "Enables more precise & controllable video generation, crucial for virtual reality, robotics, and content creation. Bridges the gap between 2D generation and 3D/4D reconstruction, offering an efficient ReFL approach for camera control.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Table 1 (Quantitative comparison on video and 3D scene generation), Table 2 (Quantitative comparison across control and consistency metrics), Figure 4 (Qualitative comparison of video generation).",
        "arxiv_id": "http://arxiv.org/abs/2601.16214v1"
    },
    {
        "Title": "PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation",
        "Field & Subfield": "Computer Vision & Natural Language Processing, Multimodal AI",
        "Key Contributions": "- Novel Language-aligned Pyramidal Quantization (LaPQ) for multi-scale video representation.\n- Dual semantic alignment strategy for local/global text-video coherence.\n- Hierarchical semantic codebook loss for stable, semantically consistent quantization.\n- Achieves SOTA in video reconstruction, T2V generation, and zero-shot understanding tasks.",
        "Methodology": "PyraTok uses a pretrained video VAE with a Language-aligned Pyramidal Quantization (LaPQ) module to discretize encoder features at multiple depths. It employs dual semantic alignment via multi-scale text-guided quantization (local) and an autoregressive objective (global). A shared binary codebook and hierarchical semantic codebook loss ensure consistency.",
        "Strengths": "SOTA performance across 10 diverse video benchmarks, scaling to 4K/8K resolutions. Improves text-to-video quality, zero-shot video segmentation, temporal action localization, and video understanding. Mitigates posterior collapse and semantic drift with high codebook utilization.",
        "Limitations": "The paper does not explicitly state limitations of PyraTok itself, but rather addresses limitations of prior methods (single-scale semantics, small codebooks, shallow language supervision leading to semantic drift). Implicitly, the complexity of its multi-stage training could be a limitation.",
        "Datasets / Benchmarks": "WebVid-10M, COCO-Val, Droplet-10M, OpenVid-1M, UltraVideo, YouTube-VIS 2021, OVIS, THUMOS14, ActivityNet v1.3, MVBench, Kinetics, UCF-101, MCL-JCV.",
        "Results Summary": "PyraTok achieves SOTA in video reconstruction (e.g., +10.51% PSNR on WebVid-10M), T2V generation (9-22 pts FVD reduction), and zero-shot tasks: +68.8% mAP on YouTube-VIS and +217.9% mAP on OVIS for segmentation, +5.75 mAP for action localization, and 86.03% accuracy on MVBench.",
        "Why It Matters": "PyraTok advances video understanding and generation by creating semantically rich, multi-scale discrete video tokens that align tightly with language. This enables more precise, coherent, and controllable video AI systems, crucial for high-fidelity content creation and complex multimodal reasoning.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Table 1 (Reconstruction Quality), Table 3 (Video Segmentation), Table 4 (Action Localization), Table 5 (Video Understanding), Figure 3 (Architecture Overview), Figure 5 (Reconstruction Comparison).",
        "arxiv_id": "http://arxiv.org/abs/2601.16210v1"
    },
    {
        "Title": "Why Can't I Open My Drawer? Mitigating Object-Driven Shortcuts in Zero-Shot Compositional Action Recognition",
        "Field & Subfield": "Computer Vision, Action Recognition",
        "Key Contributions": "• Identified object-driven verb shortcuts as core ZS-CAR failure.\n• Diagnosed ZS-CAR evaluation protocol limitations, proposing unbiased metrics.\n• Introduced RCORE, a framework to mitigate shortcut learning and improve generalization.",
        "Methodology": "Proposed RCORE framework with: (i) VOCAMix (composition-aware augmentation) to diversify verb-object combinations; (ii) Temporal Order Regularization Loss (TORC) to enforce temporal-grounded verb learning; (iii) Margin loss to penalize frequent-but-incorrect predictions.",
        "Strengths": "RCORE significantly improves unseen composition accuracy, reduces reliance on co-occurrence bias, achieves positive compositional gaps, and learns robust, temporally grounded verb representations across datasets.",
        "Limitations": "The paper primarily diagnoses limitations of existing ZS-CAR models (data sparsity, asymmetric learning difficulty, flawed evaluation). It doesn't explicitly discuss new limitations introduced by RCORE.",
        "Datasets / Benchmarks": "Sth-com (Something-Something V2, Something-Else), EK100-com (newly curated from EPIC-KITCHENS-100).",
        "Results Summary": "RCORE outperforms SOTA (C2C) on Sth-com & EK100-com in unseen composition accuracy (e.g., +1.89% H.M. on Sth-com), achieves positive compositional gaps where baselines fail, and reduces False Co-occurrence Prediction.",
        "Why It Matters": "Addresses a critical flaw in compositional video understanding, leading to AI models that genuinely reason compositionally instead of relying on shortcuts, crucial for robust and generalizable action recognition.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Shortcut illustration), Figure 2 (Object-driven shortcut diagnosis), Figure 5 (RCORE's impact on FCP & temporal sensitivity), Table 2 & 3 (Sth-com & EK100-com results).",
        "arxiv_id": "http://arxiv.org/abs/2601.16211v1"
    },
    {
        "Title": "MEGnifying Emotion: Sentiment Analysis from Annotated Brain Data",
        "Field & Subfield": "Neuroscience & Machine Learning (MEG, NLP)",
        "Key Contributions": "- Novel method to annotate existing MEG datasets with sentiment labels via pre-trained NLP models.\n- Proof-of-concept for directly decoding sentiment from MEG brain activity.\n- Demonstrated improved balanced accuracy over baseline using deep learning for Brain-to-Sentiment.",
        "Methodology": "Pre-trained Text-to-Sentiment models (CardiffNLP selected) labeled text from audiobooks aligned with MEG data. Force-alignment linked sentiment labels to brain recordings. MLP and LSTM models were trained on MEG data to predict sentiment probabilities.",
        "Strengths": "Addresses a critical gap in sentiment-labeled MEG datasets. Proposes an objective, reproducible labeling method using NLP. Validates direct sentiment decoding from brain activity, offering a new avenue for understanding emotion.",
        "Limitations": "Small dataset (3 subjects, 30 hrs). Class imbalance biased accuracy. Vague sentiment labels. Limited model architectures explored (MLP, LSTM). No interpretability analysis. Doesn't account for individual differences.",
        "Datasets / Benchmarks": "MEG dataset from *The Adventures of Sherlock Holmes* (Armeni et al., 2022). Pre-trained sentiment models: CardiffNLP, FiniteAutomata, LXYuan, NickWong. Baseline: 33.3% balanced accuracy.",
        "Results Summary": "LSTM (35.745% balanced accuracy) and MLP (35.878%) outperformed baseline (33.333%). LSTM showed more consistent results (lower dispersion, higher t-statistic), indicating potential for sequential models in MEG data.",
        "Why It Matters": "Enables new research into emotional processing in the brain by creating sentiment-labeled neuroimaging data. Opens pathways for brain-computer interfaces that decode emotional states, deepening understanding of the human experience.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Table 2 (Brain-to-Sentiment results), Figure 2 (Balanced accuracy distribution)",
        "arxiv_id": "2601.18792v1"
    },
    {
        "Title": "Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes",
        "Field & Subfield": "Reinforcement Learning; Large Language Models; Reasoning",
        "Key Contributions": "- Introduces PrefixRL: conditions on successful off-policy prefixes for on-policy RL, avoiding instabilities.\n- Proves PrefixRL is objective-consistent & more sample-efficient than standard RL.\n- Discovers 'back-generalization': training on prefixed problems improves no-prefix performance.\n- Shows PrefixRL learns strategies beyond those in prefixes & maintains stable training.",
        "Methodology": "PrefixRL extracts prefixes from correct off-policy traces (via rejection sampling) & appends them to original problems, creating 'prefixed problems.' On-policy RL is then run on both original & prefixed problems, masking gradients on the prefix. This guides exploration & boosts learning signal.",
        "Strengths": "Theoretically consistent & more sample-efficient. Achieves faster learning & higher final rewards on hard problems. Transfers gains to held-out benchmarks. More stable training, preserves entropy, and effective even with off-policy prefixes from different model families.",
        "Limitations": "Cross-family prefix sourcing is less effective if the source model is weak or prefixes don't align well with target model's internal representations. Back-generalization can be slower with very long prefixes or severe train/test mismatch.",
        "Datasets / Benchmarks": "Hard training problems (1k from DAPO & OMNI-MATH levels 6-8). Held-out benchmarks: AIME '25, HMMT '25, IMO-AnswerBench. Llama3.1-8B-instruct & Qwen3-4B-instruct models.",
        "Results Summary": "PrefixRL achieves same training reward 2x faster than strong baselines (SFT+RL) & increases final reward by 3x. Improves training accuracy >45% (Llama) & >30% (Qwen). Boosts pass@1 on AIME '25 by 12% absolute.",
        "Why It Matters": "Addresses the challenge of RL stalling on hard LLM reasoning tasks by effectively reusing 'wasted' compute from prior sampling. Improves RL sample efficiency and stability, making LLM training more practical and scalable for complex problem-solving.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 2 (Compute-matched performance), Figure 5 (Back-generalization across prefix lengths), Figure 6 (Strategy discovery beyond prefix), Figure 10 (Held-out benchmark results).",
        "arxiv_id": "2601.18795v1"
    },
    {
        "Title": "ctELM: Decoding and Manipulating Embeddings of Clinical Trials with Embedding Language Models",
        "Field & Subfield": "Biomedical NLP & Machine Learning, Interpretable AI",
        "Key Contributions": "• First open-source ELM architecture/training framework.\n• Expert-validated dataset for clinical trials.\n• ctELM model for interpreting/generating clinical trial abstracts.\n• Ablation studies for optimal ELM training.",
        "Methodology": "Aligns Llama 3.1-8B-Instruct LLM to clinical trial embeddings (bge-large-en-v1.5) via a 2-layer MLP adapter. Trained on 5 diverse tasks (abstract decode, section, summary, commonality, differences) using LoRA. Evaluated via Semantic Consistency (SC) and human experts. Uses Concept Activation Vectors (CAVs) for controlled generation.",
        "Strengths": "ctELM consistently outperforms Vec2Text baselines in semantic consistency. Generates plausible clinical trial abstracts that can deceive human experts. Responsive to clinically meaningful embedding directions (age, sex). Provides an open-source framework, enhancing accessibility.",
        "Limitations": "Domain-specific to clinical abstracts; generalizability to other biomedical data/domains is unclear. Vec2Text baseline used a smaller T5 model. Ethical concerns regarding synthetic data generation for real-world medical studies.",
        "Datasets / Benchmarks": "PubMed 200K RCT dataset for training. Synthetic data for plain language summaries, commonality, and difference tasks generated by GPT-40-mini. Embedding models: BAAI/bge-large-en-v1.5, Alibaba-NLP/gte-large-en-v1.5, NeuML/pubmedbert-base-embeddings.",
        "Results Summary": "ctELM achieved 0.87 SC for abstract reconstruction (vs. 0.82 for best Vec2Text). It fooled human experts 44% of the time on plausibility. Generated abstracts were responsive to age/sex CAV modifications while maintaining high SC. Performance improved with data scale.",
        "Why It Matters": "Enhances transparency and interpretability of embedding spaces. Enables controlled, language-based generation of novel, plausible clinical trial hypotheses. Opens avenues for ideation and discovery in biomedical research and makes ELMs more accessible.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Table 2 (Semantic Consistency for 5 tasks), Table 3 (Human expert win rates), Figures 2 & 3 (CAV effects on subject age/sex).",
        "arxiv_id": "http://arxiv.org/abs/2601.18796v1"
    },
    {
        "Title": "DuwatBench: Bridging Language and Visual Heritage through an Arabic Calligraphy Benchmark for Multimodal Understanding",
        "Field & Subfield": "Artificial Intelligence; Computer Vision & Natural Language Processing",
        "Key Contributions": "- Introduces DuwatBench: 1.27K curated samples across 6 Arabic calligraphy styles & 1,475 unique words.\n- Provides detection-level annotations for word localization, complementing existing classification/OCR datasets.\n- Establishes systematic baselines on 13 state-of-the-art Arabic & multilingual LMMs, revealing failure cases.",
        "Methodology": "Dataset collected from digital archives & community repositories, filtered for quality, then manually transcribed by 3 native Arabic speakers with bounding box annotations. A two-tier verification by calligraphy experts ensured accuracy. Models evaluated using CER, WER, chrF, ExactMatch, NLD metrics.",
        "Strengths": "First benchmark integrating diverse calligraphy styles, semantic depth, real-world artistic backgrounds, and detection-level annotations. Enables evaluation of LMMs on visually complex Arabic script beyond plain text, fostering culturally-grounded AI.",
        "Limitations": "Dataset size is smaller (1.27K samples) compared to general-purpose Arabic corpora, which might limit generalizability. Some styles are underrepresented, impacting model robustness.",
        "Datasets / Benchmarks": "DuwatBench (new); compared against existing Calliar, HICMA, Allaf, Adam, Sal&King, Kaoudja, MOJ-DB. Evaluated: Llava-v1.6, EasyOCR, InternVL3, Qwen2.5-VL (7B/72B), Gemma-3-27B-IT, trocr-base-arabic, MBZUAI/AIN, Claude-sonnet-4.5, Gemini (1.5/2.5-flash), GPT-4o (mini/full).",
        "Results Summary": "Gemini-2.5-Flash (closed-source) and Gemma-3-27B-IT (open-source) perform best. Traditional OCR and general LMMs struggle with stylized calligraphy, highlighting a gap. Performance drops significantly with increasing calligraphic complexity (e.g., Kufic, Diwani styles are harder than Naskh, Ruq'ah). Bounding box guidance improves recognition.",
        "Why It Matters": "Addresses a critical gap in multimodal understanding of Arabic calligraphy, a rich cultural heritage. Promotes responsible AI that is inclusive of non-Latin scripts and artistic forms. Supports applications in cultural preservation, education, and digital humanities.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (DuwatBench Taxonomy), Table 1 (Dataset Comparison), Table 2 (Overall Model Performance), Table 3 (WER per Style), Figure 5 (Qualitative Comparison).",
        "arxiv_id": "http://arxiv.org/abs/2601.19898v1"
    },
    {
        "Title": "SELF-DISTILLATION ENABLES CONTINUAL LEARNING",
        "Field & Subfield": "Artificial Intelligence & Machine Learning / Continual Learning",
        "Key Contributions": "- Introduces Self-Distillation Fine-Tuning (SDFT) for on-policy learning from demonstrations.\n- Achieves higher new-task accuracy and substantially reduces catastrophic forgetting.\n- Enables stable accumulation of multiple skills over time without performance regression.\n- Allows models to acquire reasoning skills without explicit reasoning traces.",
        "Methodology": "SDFT uses an in-context learning model as its own teacher. The student model (conditioned only on task input) is distilled from the teacher (same model, conditioned on task + expert demonstration) via on-policy updates minimizing reverse KL divergence. This generates on-policy training signals.",
        "Strengths": "Significantly reduces catastrophic forgetting compared to SFT. Improves generalization both in- and out-of-distribution. Enables models to acquire new skills & knowledge without explicit reward functions or reasoning traces. Scales well with model size due to stronger ICL abilities.",
        "Limitations": "Higher computational cost (2.5x FLOPs, 4x wall-clock time vs. SFT). Student can inherit spurious linguistic patterns from teacher. Relies critically on base model's in-context learning capabilities. Struggles with fundamental shifts in model's generation patterns.",
        "Datasets / Benchmarks": "Skill Learning: SciKnowEval (Science Q&A), ToolAlpaca (Tool Use), HuatuoGPT-01 (Medical). Knowledge Acquisition: Custom Wikipedia corpus (2025 events). Prior Capabilities: HellaSwag, TruthfulQA, MMLU, IFEval, Winogrande, HumanEval.",
        "Results Summary": "SDFT consistently outperforms SFT, achieving higher new-task accuracy while reducing catastrophic forgetting. It enables stable multi-task skill accumulation, unlike SFT's oscillatory behavior. SDFT effectively integrates new factual knowledge and retains reasoning depth without explicit reasoning data.",
        "Why It Matters": "SDFT provides a practical path for foundation models to continually learn new skills and knowledge from demonstrations post-deployment, addressing a fundamental challenge in AI development by mitigating catastrophic forgetting and enabling more adaptable, general-purpose models.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (SDFT vs SFT performance), Figure 3 (Sequential learning comparison), Figure 4 (Pareto efficiency across tasks), Table 1 (Knowledge acquisition results), Table 5 (Skill Learning task results).",
        "arxiv_id": "http://arxiv.org/abs/2601.19897v1"
    },
    {
        "Title": "Evolutionary Strategies lead to Catastrophic Forgetting in LLMs",
        "Field & Subfield": "Artificial Intelligence & Large Language Models",
        "Key Contributions": "• ES achieves comparable performance to GRPO on math/reasoning tasks.\n• ES training causes significant catastrophic forgetting of prior abilities.\n• ES updates are less sparse with orders of magnitude larger L2 norms than GRPO.",
        "Methodology": "Empirical comparison of Evolutionary Strategies (ES) and Group Relative Policy Optimization (GRPO) for LLM fine-tuning. Analyzed forgetting curves, parameter update sparsity, and Frobenius norms over iterations on multiple math & reasoning benchmarks.",
        "Strengths": "Comprehensive empirical analysis of ES for LLM fine-tuning. Highlights critical issues (catastrophic forgetting) overlooked in prior ES works. Provides insights into update characteristics (norm, sparsity) causing forgetting. Open-sourced code & models.",
        "Limitations": "ES has inherent randomness, leading to high variance; larger population size could reduce this. Forgetting analysis focused on one prior task, not fully capturing multi-faceted performance loss.",
        "Datasets / Benchmarks": "Countdown, GSM8K, MATH, Olympiad-Bench (new tasks). HellaSwag (prior task). Models: Qwen2.5-1.5B-Instruct, Llama-3.2-1B-Instruct.",
        "Results Summary": "ES reaches comparable performance to GRPO (within 3-4% accuracy) on new tasks. However, ES causes significant catastrophic forgetting of prior abilities, unlike GRPO. ES updates have orders of magnitude larger L2 norms and are much less sparse than GRPO updates.",
        "Why It Matters": "Identifies a critical flaw in current Evolutionary Strategies for continual learning in LLMs: catastrophic forgetting. Despite memory efficiency, ES leads to severe model degradation, hindering its adoption for online adaptation and task generalization. Informs future research on robust gradient-free methods.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Table 1 (Peak performance of ES vs. GRPO), Figure 1 (ES catastrophic forgetting on HellaSwag), Figure 3 (ES updates have higher Frobenius norm), Figure 4 (ES updates are less sparse).",
        "arxiv_id": "2601.20861v1"
    },
    {
        "Title": "FreeFix: Boosting 3D Gaussian Splatting via Fine-Tuning-Free Diffusion Models",
        "Field & Subfield": "Computer Vision & Novel View Synthesis, 3D Reconstruction",
        "Key Contributions": "- Fine-tuning-free approach using pretrained image DMs for 3DGS extrapolation.\n- Interleaved 2D-3D refinement for multi-frame consistency.\n- Per-pixel confidence mask via Fisher information for targeted artifact removal.\n- Multi-level confidence maps for denoising guidance.",
        "Methodology": "Interleaved 2D-3D refinement strategy, applying pretrained Image Diffusion Models (IDMs) without fine-tuning. Utilizes Fisher information-derived per-pixel confidence masks and multi-level confidence maps for denoising guidance, combined with an affine transform to mitigate color bias.",
        "Strengths": "Achieves SOTA/comparable performance without fine-tuning DMs, strong generalization across diverse datasets, ensures multi-frame consistency, computationally efficient compared to Video DMs.",
        "Limitations": "May fail with excessive artifacts lacking credible guidance. 3DGS updating process is relatively slow and convergence can be challenging across many refining steps.",
        "Datasets / Benchmarks": "LLFF [18], Mip-NeRF 360 [1], Waymo [29] for evaluation. Uses SDXL [22] and Flux [12] as IDM backbones.",
        "Results Summary": "Outperforms/matches fine-tuned methods (Difix3D+) on LLFF/MipNeRF 360, comparable on Waymo, with strong generalization. Confidence guidance significantly improves over opacity masks, demonstrating higher fidelity.",
        "Why It Matters": "Addresses artifact generation in extrapolated views for 3DGS, enabling practical NVS from sparse inputs. Leverages powerful DMs without costly fine-tuning, enhancing realism for AR/VR, robotics, and autonomous driving applications.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Overview), Figure 5 (Qualitative Comparisons), Table 1 (Quantitative Comparison with Baselines).",
        "arxiv_id": "2601.20857v1"
    },
    {
        "Title": "SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models",
        "Field & Subfield": "Artificial Intelligence & Automated Planning, Large Language Models",
        "Key Contributions": "• Proposes SokoBench, a simplified Sokoban benchmark for long-horizon planning.\n• Systematically assesses planning abilities of SOTA LRMs (DeepSeek R1, GPT-5 models).\n• Reveals consistent planning degradation in LRMs for tasks >25 moves.\n• Shows modest improvements with PDDL parsing/solving tools, highlighting architectural limits.",
        "Methodology": "Two setups: 1) 1-shot inference for LRMs to solve simplified Sokoban puzzles (linear corridor, 1 box, 1 goal) based on ASCII maps. 2) LLM-Modulo, where LRMs generate PDDL problems solved by external PDDL tools (parsers, solvers). Evaluated with Accuracy, Prefix Accuracy, and Manhattan Distance.",
        "Strengths": "Systematic assessment in a controlled environment isolating long-horizon planning; novel simplified Sokoban benchmark (SokoBench) publicly released; rigorous evaluation with multiple metrics and models; highlights fundamental architectural limitations.",
        "Limitations": "Narrow scope (1-box linear corridors); doesn't cover full multi-box Sokoban complexity/deadlocks; sensitivity to prompt formatting; inherent instability in API-based evaluations; potential pretraining contamination.",
        "Datasets / Benchmarks": "SokoBench (new, publicly released on HuggingFace); PlanBench (referenced for related work).",
        "Results Summary": "LRMs show consistent planning degradation for tasks requiring >25 moves, attributed to internal counting issues & state tracking failures. LLM-Modulo with PDDL tools offers only modest improvements, confirming LLMs' inherent spatial grounding and constraint representation limitations.",
        "Why It Matters": "Provides critical insights into fundamental long-horizon planning and reasoning limitations of LRMs, even in simplified settings. Suggests architectural innovations beyond test-time scaling are needed for robust planning capabilities, particularly in state tracking and constraint adherence.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 3 (Accuracy & Reasoning Tokens for LRM experiment), Figure 5 (Prefix Accuracy & Manhattan Distance), Figure 6 (LLM-Modulo results for GPT-5-mini)",
        "arxiv_id": "http://arxiv.org/abs/2601.20856v1"
    },
    {
        "Title": "Discovering Hidden Gems in Model Repositories",
        "Field & Subfield": "Machine Learning, Model Discovery & Evaluation",
        "Key Contributions": "• Identified \"hidden gems\": unpopular fine-tuned models outperforming popular ones across tasks.\n• Formulated model discovery as a Multi-Armed Bandit problem.\n• Introduced an accelerated Sequential Halving algorithm (50x faster).\n• Showed model documentation is often missing/irrelevant.",
        "Methodology": "Formulated model discovery as a Fixed-Budget Best-Arm Identification problem. Adapted Sequential Halving with 1) Correlated Sampling for variance reduction and 2) Aggressive Elimination Schedule for faster pruning of low-quality models.",
        "Strengths": "• Significantly accelerates model discovery (>50x speedup).\n• Consistently identifies superior, overlooked models (\"hidden gems\").\n• Improves average model performance (>4.5%).\n• Robust across various model families & tasks.",
        "Limitations": "Still requires evaluating models (not zero-cost). Focused on predefined tasks; new tasks require re-evaluation. Alternative weight-space learning is currently limited to small-scale networks/benchmarks.",
        "Datasets / Benchmarks": "RouterBench (aggregates ARC-C, Winogrande, MMLU, MBPP, GSM8K). Evaluated >2,000 models from Qwen2.5 (3B, 7B), Mistral-7B, Llama3.1-8B model trees.",
        "Results Summary": "Discovered hidden gems achieving up to +40.1% performance gain (e.g., Llama 8B math +12.8%). Our method achieved >50x speedup, finding top-3 models with 50 queries/model and >4.5% avg. performance improvement.",
        "Why It Matters": "Addresses the critical problem of model selection in vast repositories. Enables users to find truly best-performing models instead of defaulting to popular, often suboptimal, choices, unlocking the potential of overlooked models.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Table 1 (Model Discovery Results), Table 5 (Ablation Studies), Figure 1 (Hidden Gems & Repository Inefficiency)",
        "arxiv_id": "2601.22157v1"
    },
    {
        "Title": "RedSage: A Cybersecurity Generalist LLM",
        "Field & Subfield": "Artificial Intelligence & Cybersecurity",
        "Key Contributions": "- Curated 11.8B-token cybersecurity pretraining data (CyberFineWeb, RedSage-Seed).\n- Designed agentic augmentation for 266K multi-turn SFT dialogues (RedSage-Conv).\n- Introduced RedSage-Bench: 30K MCQs & 240 open-ended Q&A for knowledge, skills, tools.\n- Developed RedSage, an open-source 8B LLM, achieving SOTA on cybersecurity & general benchmarks.",
        "Methodology": "Multi-stage training: (1) Continual pretraining on filtered web data (CyberFineWeb) & curated resources (RedSage-Seed). (2) Supervised fine-tuning via agentic augmentation (RedSage-Conv) & general instruction data. (3) Direct Preference Optimization (DPO). Evaluated on custom and established benchmarks.",
        "Strengths": "Open-source, locally deployable 8B model. Comprehensive evaluation across knowledge, skills, and tools. Agentic augmentation creates high-quality, diverse dialogues. Achieves SOTA on cybersecurity tasks while improving general reasoning, avoiding post-tuning degradation.",
        "Limitations": "LLM-generated content in pipeline may propagate biases/inaccuracies. Inherent risk of misuse due to offensive security knowledge. Data transparency limited for some copyrighted materials used in curation.",
        "Datasets / Benchmarks": "Custom: CyberFineWeb (11.7B tokens), RedSage-Seed (28.6K docs, 0.15B tokens), RedSage-Conv (266K multi-turn dialogues, 352M tokens), RedSage-Bench (30K MCQs, 240 Open-ended Q&A). External: CTI-Bench, CyberMetric, SECURE, MMLU, ARC-C, HellaSwag, TruthfulQA, GSM8K, IFEval.",
        "Results Summary": "RedSage-8B-DPO achieved SOTA, surpassing baselines by +5.59 points on cybersecurity benchmarks and +5.05 points on Open LLM Leaderboard tasks, demonstrating enhanced domain expertise and general reasoning. Outperformed Qwen3-32B on general tasks despite smaller size.",
        "Why It Matters": "Addresses the need for privacy-preserving, locally deployable, domain-adapted LLM assistants for cybersecurity operations. Provides comprehensive open-source resources (models, data, code) to accelerate research and fill skill gaps in a critical field.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (RedSage pipeline overview), Table 1 (Benchmark comparison), Table 3 (RedSage-Seed vs. RedSage-Conv stats), Table 4 (RedSage-MCQ results), Table 5 (Cybersecurity Benchmark results)."
    },
    {
        "Title": "MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training",
        "Field & Subfield": "Machine Learning, Neuroscience; Brain-Computer Interfaces, Neural Decoding, Self-supervised Learning",
        "Key Contributions": "- Scaled neural pre-training to 2.5-minute MEG contexts (5-300x longer).\n- Achieved data-efficient transfer for cross-subject generalization with less data.\n- Demonstrated improved brain-to-text & zero-shot prediction via learned hierarchical attention patterns.",
        "Methodology": "MEG-XL uses masked token prediction for self-supervised pre-training on 2.5 min MEG samples. It employs a BioCodec tokenizer and a criss-cross transformer for efficient long-context modeling. Fine-tuned for word decoding by predicting T5-large word embeddings.",
        "Strengths": "Achieves SOTA data efficiency in low-data regimes, outperforming supervised & brain FMs. Leverages 5-300x longer neural contexts. Improves cross-subject transfer. Reveals learned hierarchical attention. Uses non-invasive MEG for scalability & safety in BCI.",
        "Limitations": "Primarily focuses on perceived speech, not imagined. Evaluated on a limited 50-word vocabulary. GPU VRAM constrains context length scaling. Understanding of learned neural structure is unresolved. Potential aliasing artifacts from preprocessing.",
        "Datasets / Benchmarks": "Pre-training: CamCAN, MOUS, SMN4Lang (~300 hrs, ~800 subjects). Evaluation: MEG-MASC, Armeni, LibriBrain. Benchmarks: SOTA supervised method (d'Ascoli et al., 2025) & brain FMs (BioCodec, EEGPT, BIOT, BBL, BrainOmni, LaBraM).",
        "Results Summary": "MEG-XL, pre-trained on 2.5 min MEG contexts, achieves SOTA performance in brain-to-text decoding, especially with limited data. Outperforms brain FMs & matches supervised methods. Longer pre-training context improves representations for word decoding via learned hierarchical attention.",
        "Why It Matters": "Enables data-efficient brain-to-text interfaces for paralyzed patients with minimal training data. Advances non-invasive BCI by showing long-context pre-training can substitute for deep individual subject data, accelerating clinical deployment. Offers insights into brain encoding.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (MEG-XL overview, context length, data-efficiency), Figure 3 (Pre-training enables generalization), Table 1 (Foundation Model comparison), Figure 4 (Longer context improves representations), Figure 5 (Zero-shot prediction & attention patterns).",
        "arxiv_id": "http://arxiv.org/abs/2602.02494v1"
    },
    {
        "Title": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss",
        "Field & Subfield": "Computer Vision; Generative Models, Diffusion Models, Image Synthesis",
        "Key Contributions": "Proposes PixelGen, an end-to-end pixel diffusion with LPIPS for local & P-DINO for global semantics. Achieves SOTA FID 5.11 on ImageNet-256 (no CFG, 80 epochs), outperforming latent diffusion. Simplifies generation by removing VAEs & latent representations.",
        "Methodology": "End-to-end pixel diffusion predicting clean image. Incorporates LPIPS loss for local textures and P-DINO (DINOv2-based) for global semantics. Uses a flow-matching objective and a noise-gating strategy to apply perceptual losses only at low-noise steps.",
        "Strengths": "Simpler pipeline (no VAEs/latent representations, no artifacts), achieves superior image quality (FID, IS), faster training convergence. Enhances local texture fidelity & global semantic consistency. Provides a powerful, end-to-end generative paradigm.",
        "Limitations": "Has a performance gap with *leading* latent models *with* CFG. Perceptual losses can reduce sample diversity if applied at high-noise steps. Future work on pixel-based samplers, CFG methods, and incorporating richer perceptual objectives (e.g., adversarial).",
        "Datasets / Benchmarks": "ImageNet-256 (class-to-image), GenEval (object-focused text-to-image), pretraining on 36M images + 60k instruction-tuning data for text-to-image.",
        "Results Summary": "Achieved FID 5.11 on ImageNet-256 (no CFG, 80 epochs), surpassing REPA-XL/2 (FID 5.90, 800 epochs). Achieved GenEval 0.79 for text-to-image, matching/surpassing large-scale models. Outperforms other pixel diffusion methods.",
        "Why It Matters": "PixelGen redefines the generative AI pipeline by proving pixel diffusion can outperform VAE-based latent models without their limitations. It offers a simpler, more powerful, and artifact-free approach to high-fidelity image synthesis.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Fig 1 (PixelGen overview vs latent diffusion), Fig 4 (Perceptual supervision effect), Table 1 (ImageNet w/o CFG comparison), Table 4 (Text-to-Image GenEval results).",
        "arxiv_id": "http://arxiv.org/abs/2602.02493v1"
    }
]