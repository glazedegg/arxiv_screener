[
    {
        "Title": "Beyond Simple Edits: Composed Video Retrieval with Dense Modifications",
        "Field & Subfield": "Computer Science > Computer Vision",
        "Key Contributions": "• Introduces Dense-WebVid-CoVR dataset with 1.6M samples & dense modification text. • Proposes a robust CoVR model with unified grounding encoder for input video, description, & modification text fusion. • Achieves SOTA on Dense-WebVid-CoVR (71.3% Recall@1), outperforming existing methods by 3.4%. • Shows improved performance on CoIR datasets (CIRR, FashionIQ) & Ego-CVR.",
        "Methodology": "Dense-WebVid-CoVR created using Gemini-Pro for video descriptions & GPT-40 for dense modification texts, with manual verification. The model uses ViT-L for vision, BLIP-pretrained text encoder, & a BLIP-2 based grounding text encoder for unified fusion of query video, description, & modification text via cross-attention, trained with contrastive loss.",
        "Strengths": "• Introduces a large-scale, high-quality dataset with dense modification texts (7x longer). • Novel unified fusion strategy for better multimodal alignment. • Achieves SOTA performance on multiple CoVR & CoIR benchmarks. • Enhanced fine-grained retrieval capturing subtle visual/temporal changes. • Robust quality control for dataset generation.",
        "Limitations": "• Minor inaccuracies (2-3%) in training set, claimed minimal impact. • Future work: multilingual CoVR for low-resource languages. • Future work: efficient techniques for processing very long videos.",
        "Datasets / Benchmarks": "Introduced: Dense-WebVid-CoVR (1.6M samples). Used/Compared against: WebVid-CoVR, EgoCVR, CIRR, FashionIQ.",
        "Results Summary": "Achieves SOTA on Dense-WebVid-CoVR with 71.3% Recall@1 (3.4% gain over prior SOTA). Consistently outperforms baselines on Ego-CVR, CIRR (56.30% R@1), and FashionIQ, demonstrating superior fine-grained retrieval accuracy across all settings.",
        "Why It Matters": "Advances composed video retrieval by providing a richer, contextually aware dataset & an effective model. Enables precise video retrieval based on subtle modifications, crucial for applications like video editing & media production. Dense modification texts are a key innovation.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Fig 1: Example CoVR triplets comparing modification texts. Fig 3: Proposed CoVR architecture. Table 1: Comparative analysis of CoVR benchmarks. Table 2: Main results on Dense-WebVid-CoVR. Fig 13: Detailed modification-text comparison."
    },
    {
        "Title": "COMPUTERRL: SCALING END-TO-END ONLINE REINFORCEMENT LEARNING FOR COMPUTER USE AGENTS",
        "Field & Subfield": "Artificial Intelligence > Reinforcement Learning",
        "Key Contributions": "- Introduces API-GUI paradigm for machine-oriented desktop interaction, unifying API calls and GUI actions.\n- Establishes a large-scale, distributed RL infrastructure supporting thousands of parallel virtual desktop environments for scalable training.\n- Proposes Entropulse, a novel training strategy alternating RL with supervised fine-tuning to mitigate entropy collapse and ensure sustained learning.\n- Achieves new state-of-the-art accuracy of 48.1% on the OSWorld benchmark for general desktop automation agents.",
        "Methodology": "COMPUTERRL combines a novel API-GUI paradigm unifying programmatic control and GUI interaction, a distributed RL infrastructure using Docker and gRPC for scalable parallel environments, and Entropulse, a training strategy that alternates RL with SFT to maintain exploration and prevent entropy collapse. It leverages LLMs for API construction and employs a step-level GRPO algorithm with rule-based verifiable rewards.",
        "Strengths": [
            "Novel API-GUI paradigm offers superior operational efficiency and generalization.",
            "Highly scalable distributed RL infrastructure supports thousands of parallel environments.",
            "Entropulse strategy ensures robust and sustained performance gains in extended RL training.",
            "Achieves state-of-the-art performance on a challenging real-world desktop automation benchmark (OSWorld).",
            "Significantly reduces steps required for task completion compared to baselines (1/3)."
        ],
        "Limitations": [
            "Errors observed in visual perception and multi-application coordination.",
            "Challenges with operational illusions and other miscellaneous errors.",
            "Genuine universality and adaptation to unfamiliar applications remain open questions.",
            "Long-horizon autonomy and complex, multi-step objectives are still being explored."
        ],
        "Datasets / Benchmarks": "Evaluated on the OSWorld benchmark and OSWorld-Verified benchmark. No new datasets were introduced by the paper.",
        "Results Summary": "COMPUTERRL-trained AutoGLM-OS-9B achieved 48.1% success on OSWorld, surpassing SOTA models like OpenAI CUA (42.9%), UI-TARS-1.5 (42.5%), and Claude Sonnet 4 (30.7%). The API-GUI paradigm showed a 134% improvement over GUI-only approaches. Entropulse increased average training rewards and improved learning efficiency by mitigating entropy collapse.",
        "Why It Matters": "This work addresses critical challenges in developing autonomous agents for complex digital workspaces, offering a scalable and robust framework for end-to-end online RL. By bridging the gap between machine and human-centric GUIs, it lays a foundational step towards truly intelligent desktop automation, improving efficiency and paving the way for more capable generalist agents.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1: Shows state-of-the-art success rates on OSWorld (1a) and COMPUTERRL training reward curves (1b), demonstrating Entropulse's benefits. Table 1: Provides a comprehensive comparison of AUTOGLM-OS performance against various proprietary and open models on OSWorld benchmarks."
    },
    {
        "Title": "Relational Visual Similarity",
        "Field & Subfield": "Computer Vision & Image Understanding. Focuses on a novel visual similarity metric leveraging Vision-Language Models for abstract relational reasoning, moving beyond surface attributes to capture human-like conceptual similarities.",
        "Key Contributions": [
            "Introduced 'relational visual similarity' as a new dimension.",
            "Curated a 114k {image-anonymous caption} dataset for relational training.",
            "Developed 'relsim,' a new metric for relational visual similarity.",
            "Demonstrated relsim's utility in image retrieval & generation.",
            "Analyzed relationship between attribute and relational similarity."
        ],
        "Methodology": "Proposed `relsim` by: 1) Filtering LAION-2B to select images with relational cues. 2) Generating anonymous captions (describing relational logic, not surface content) from image groups using a fine-tuned VLM. 3) Training `relsim` (Qwen2.5-VL-7B) with InfoNCE loss to align image/caption embeddings.",
        "Strengths": "Captures human-like abstract relational reasoning, a key missing dimension in visual AI. `relsim` significantly outperforms existing attribute-based metrics in recognizing deeper conceptual similarities, validated by user studies & GPT-40 evaluation. Enables novel image retrieval/generation applications.",
        "Limitations": "Anonymous caption dataset curated manually (532 groups), limiting scalability & prone to bias. VLMs can hallucinate captions. Difficulty in specifying user intent for multi-relational images. Expanding dataset/pipeline is a future direction.",
        "Datasets / Benchmarks": "New: Relational Dataset (114k {image, anonymous caption} pairs derived from LAION-2B). Baselines: LPIPS, DINO, CLIP, dreamsim (image-to-image); CLIP-T, Qwen-T (caption-based). Evaluation: GPT-40 automated judge & human user study.",
        "Results Summary": "`relsim` achieved a GPT score of 6.77 (vs. LPIPS 4.56, CLIP-I 5.91), indicating superior relational similarity detection. User studies showed consistent human preference for `relsim` (42.5-60.7%). Proprietary models excel in analogical generation (GPT40 relsim 0.82) over open-source models.",
        "Why It Matters": "Addresses a critical gap in AI's visual understanding by moving beyond surface attributes to capture abstract, relational logic, mirroring human cognition. This unlocks new possibilities for creative AI applications, enhancing image retrieval, and analogical content generation.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Illustrates relational vs. attribute similarity), Figure 2 (Overall Pipeline), Figure 6 (relsim performance vs. baselines), Figure 8 (User study results), Table 2 (Analogical image generation benchmarks).",
        "arxiv_id": "http://arxiv.org/abs/2512.07833v1"
    },
    {
        "Title": "ASTRA: General INTERACTIVE WORLD MODEL WITH AUTOREGRESSIVE DENOISING",
        "Field & Subfield": "Computer Vision, Generative Models, World Models",
        "Key Contributions": "Introduces Astra, an interactive general world model. Key contributions: Autoregressive denoising with temporal causal attention; Action-aware adapter for precise control; Noise-augmented history memory for consistency; Mixture of Action Experts for diverse modalities.",
        "Methodology": "Autoregressive denoising framework based on a pre-trained video diffusion backbone. Utilizes an Action-Aware Flow Transformer (AFT) with ACT-Adapter for action injection, a noise-as-mask strategy for history memory to reduce visual inertia, and a Mixture of Action Experts (MoAE) for multimodal actions.",
        "Strengths": "Achieves high visual fidelity, strong temporal consistency, and precise action responsiveness. Versatile across diverse tasks (robotics, AD, exploration) and generalizes well to out-of-domain scenes. Parameter-efficient design with low compute overhead.",
        "Limitations": "Inference efficiency is limited, requiring multiple denoising steps per frame. This makes real-time deployment challenging for latency-sensitive applications like online control or interactive robotics.",
        "Datasets / Benchmarks": "nuScenes, Sekai, SpatialVID, RT-1, Multi-Cam Video. Evaluated on Astra-Bench (custom benchmark) and CityWalker for out-of-domain generalization.",
        "Results Summary": "Outperforms SOTA models (Wan-2.1, MatrixGame, YUME) across all metrics: instruction following, subject/background consistency, motion smoothness, and visual quality. Achieves lower rotation/translation errors and superior action-following accuracy on diverse benchmarks.",
        "Why It Matters": "Astra advances interactive world modeling, enabling more general, scalable simulators for exploration, robotics, autonomous driving, and embodied intelligence, bridging the gap to true interactive world simulation.",
        "Should Read Fully?": "Yes, for researchers and practitioners interested in interactive world models, diffusion-based video generation, robotics, autonomous driving, or embodied AI.",
        "Key Figures or Tables": "Fig. 1 (diverse applications), Fig. 3 (Astra framework), Fig. 5 & 6 (qualitative results), Table 2 (SOTA comparison), Table 3 (ablation studies).",
        "arxiv_id": "http://arxiv.org/abs/2512.08931v1"
    },
    {
        "Title": "Efficiently Reconstructing Dynamic Scenes One D4RT at a Time",
        "Field & Subfield": "Computer Vision; 4D Reconstruction & Tracking",
        "Key Contributions": "Novel query-based feedforward model for efficient 4D scene reconstruction. Unified transformer architecture for depth, correspondence, point clouds, camera params. SOTA accuracy & speed in dynamic 4D reconstruction & tracking. Enables efficient dense, holistic scene reconstruction.",
        "Methodology": "D4RT uses an encoder (ViT-g) to create a global scene representation (F) from video. A lightweight cross-attention decoder queries F with spatio-temporal parameters (u,v, t_src, t_tgt, t_cam) and local RGB patch to predict 3D point positions.",
        "Strengths": "Highly efficient (18-300x faster, 200+ FPS), scalable, unified interface for multiple 4D tasks, SOTA accuracy across diverse benchmarks, handles dynamic scenes, supports subpixel precision & high-res decoding with novel querying.",
        "Limitations": "No explicit limitations for D4RT are stated, as the paper focuses on overcoming prior methods' drawbacks. Implicitly, a large encoder requires pre-training. Dense inference still has computational cost, partially addressed by occupancy grid optimization.",
        "Datasets / Benchmarks": "BlendedMVS, Co3Dv2, Dynamic Replica, Kubric, MVS-Synth, PointOdyssey, ScanNet, ScanNet++, Tartanair, VirtualKitti, Waymo Open (training). Sintel, ScanNet, KITTI, Bonn, TapVid-3D (evaluation).",
        "Results Summary": "D4RT sets SOTA across 4D tasks (depth, point cloud, 3D tracking, camera pose) on various benchmarks. Achieves significantly higher throughput (e.g., 200+ FPS, 100x faster than MegaSaM) while delivering superior accuracy. Generalizes to long videos & high-res decoding.",
        "Why It Matters": "Offers a unified, efficient, and highly scalable framework for 4D reconstruction, overcoming fragmentation and computational bottlenecks of prior methods, paving the way for next-generation 4D perception in complex dynamic environments.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Fig. 1 (Model overview), Fig. 3 (Pose accuracy vs. speed), Fig. 4 (Qualitative reconstruction), Table 3 (3D tracking throughput), Table 4 (4D tracking metrics).",
        "arxiv_id": "http://arxiv.org/abs/2512.08924v1"
    },
    {
        "Title": "Selfi: Self Improving Reconstruction Engine via 3D Geometric Feature Alignment",
        "Field & Subfield": "Computer Vision & 3D Reconstruction, Novel View Synthesis",
        "Key Contributions": "• Self-improving geometric feature learning from unposed RGB via reprojection-based consistency loss. • Achieves state-of-the-art NVS and pose estimation using a frozen VFM backbone. • Enhances NVS quality and pose accuracy via dense bundle adjustment with depth shift.",
        "Methodology": "Selfi uses a pre-trained VGGT (3D VFM) as a backbone. A DPT adapter learns geometrically-aligned features using a self-supervised reprojection-based consistency loss with pseudo-ground-truth from VGGT outputs. These features predict 3D Gaussian parameters via a U-Net decoder. Poses are refined using dense bundle adjustment, with Gaussian centers adjusted via an affine depth shift.",
        "Strengths": "State-of-the-art NVS and pose estimation from unposed, uncalibrated images. Achieves high-fidelity rendering, including thin structures and fine details. Robust to varying input views/overlap. Self-supervised, efficiently leveraging VFMs.",
        "Limitations": "Relies on VGGT's normalized scale, leading to depth inaccuracies in distant regions (e.g., sky). Sensitive to exposure discrepancies between input and target images. Currently restricted to static scenes, failing on dynamic environments.",
        "Datasets / Benchmarks": "Trained on DL3DV and RealEstate10K. Evaluated on DL3DV, RealEstate10K, MipNeRF, Tanks&Temples, and RayZer split. Metrics include PSNR, SSIM, LPIPS for NVS; AUC@3, AUC@5, AUC@15 for pose estimation.",
        "Results Summary": "Selfi consistently outperforms existing pose-free feed-forward Gaussian methods (e.g., AnySplat, WorldMirror) and achieves performance comparable to/exceeding 3DGS with ground-truth poses. Achieves SOTA on NVS & pose estimation benchmarks across various input settings (varying sequence lengths, overlap).",
        "Why It Matters": "Selfi advances novel view synthesis and 3D reconstruction by bypassing the fragile and computationally intensive SfM pipeline. It enables robust, high-fidelity 3D scene understanding from unposed images, making 3D VFMs more practical for real-world applications without explicit 3D ground truth.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Selfi pipeline overview), Figure 3 & 5 (Qualitative NVS comparisons), Table 1 & 2 (NVS performance with varying inputs/overlap), Table 5 (BA for NVS/Pose).",
        "arxiv_id": "http://arxiv.org/abs/2512.08930v1"
    },
    {
        "Title": "Closing the Train-Test Gap in World Models for Gradient-Based Planning",
        "Field & Subfield": "Reinforcement Learning & Robotics; Model-Based Planning",
        "Key Contributions": "- Proposes Online World Modeling (OWM) to expand WM's reliable state space via simulator-corrected trajectories.\n- Introduces Adversarial World Modeling (AWM) to smooth loss landscape & promote robustness by finetuning on perturbed inputs.\n- Achieves CEM-level performance with 10x speedup for gradient-based planning.",
        "Methodology": "Two finetuning methods (OWM, AWM) for latent world models (DINO-WM) used in gradient-based planning (GBP). OWM generates and finetunes on simulator-corrected GBP trajectories. AWM perturbs inputs to maximize prediction error, smoothing the planning loss landscape.",
        "Strengths": "Significantly improves gradient-based planning reliability and performance, often matching or exceeding CEM. Achieves a 10x computational speedup over CEM. Smooths the planning loss landscape, reducing local minima. Narrows the train-test gap in world model error.",
        "Limitations": "Online World Modeling relies on access to an environment simulator, which might be costly or infeasible in real-world settings. Performance benefits vary by task and specific gradient optimizer used (e.g., Adam generally outperforms GD).",
        "Datasets / Benchmarks": "PushT, PointMaze, Wall (main experiments); Rope, Granular (additional robotic manipulation tasks). Utilizes existing DINO-WM datasets and architectures.",
        "Results Summary": "AWM with Adam GBP outperforms or matches CEM on PushT, PointMaze, Wall tasks, achieving up to +30% success rate increase over DINO-WM (open-loop) and up to 10x faster execution. Both OWM and AWM narrow the train-test gap.",
        "Why It Matters": "Enables practical and efficient gradient-based planning with world models, a critical step for real-world robotics. Addresses a fundamental train-test gap, making learned dynamics models more reliable for long-horizon control.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Table 1 (Planning Results), Figure 2 (Optimization landscape of DINO-WM vs. AWM), Figure 3 (Planning efficiency)",
        "arxiv_id": "2512.09929v1"
    },
    {
        "Title": "GAINS: Gaussian-based Inverse Rendering from Sparse Multi-View Captures",
        "Field & Subfield": "Computer Vision & Graphics, Inverse Rendering",
        "Key Contributions": "- Two-stage inverse rendering for sparse multi-view captures.\n- Synergizes learning-based priors (depth/normal, diffusion) for robust geometry.\n- Combines segmentation, IID, and diffusion priors for stable material recovery.\n- Achieves SOTA relighting, NVS, and material accuracy on sparse inputs.",
        "Methodology": "Two-stage pipeline using 2DGS. Stage I: Geometry refinement with monocular depth/normal and diffusion priors. Stage II: Material properties & lighting estimation using segmentation, intrinsic image decomposition (IID), and diffusion priors, all combined with PBR.",
        "Strengths": "Significantly improves material parameter accuracy, relighting quality, and novel-view synthesis, especially under sparse-view conditions. More robust against overfitting and ambiguity than prior methods. Better intrinsic separation and reduced reflection baking.",
        "Limitations": "Segmentation guidance can trade off robustness for high-frequency detail under dense supervision. IID prior can exhibit view-inconsistency, though mitigated by a weighted loss.",
        "Datasets / Benchmarks": "Synthetic4Relight [44], TensorIR [10], Ref-Real [29]. Evaluated using PSNR, SSIM, LPIPS for NVS/Albedo/Relighting, and MSE for Roughness.",
        "Results Summary": "GAINS consistently outperforms SOTA Gaussian-based IR methods (Ref-GS, GI-GS) on all datasets and metrics, especially with sparse (4-8) views. Achieves higher PSNR, SSIM, lower LPIPS/MSE for NVS, albedo, roughness, and relighting.",
        "Why It Matters": "Addresses the challenging problem of inverse rendering from sparse inputs, which is critical for real-world applications where dense multi-view captures are impractical or impossible, enabling more robust 3D scene understanding.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Table 1, Table 2 (Quantitative evaluation across datasets), Table 3 (Ablation study of priors), Figure 1 (Qualitative intrinsics/relighting), Figure 6 (View-dependent performance)."
    },
    {
        "Title": "ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning",
        "Field & Subfield": "Computer Vision & Generative AI: Video Editing",
        "Key Contributions": "- Introduces Reason-Informed Video Editing (RVE) task.\n- Proposes RVE-Bench, a comprehensive benchmark for RVE.\n- Develops ReViSE, a Self-Reflective Reasoning (SRF) framework.\n- Leverages internal VLM for intrinsic, differential feedback.\n- Achieves SOTA on RVE-Bench.",
        "Methodology": "ReViSE employs a Self-Reflective Reasoning (SRF) framework, unifying generation & internal evaluation. An internal VLM acts as a critic, providing differential feedback. Training uses Unified Semantic Optimization (USO) & Reward Weighted Optimization (RWO) with flow-matching loss.",
        "Strengths": "Bridges the reasoning-editing gap in video models, provides self-supervised intrinsic feedback, significantly improves reasoning accuracy & visual fidelity, and introduces a robust, reasoning-aware evaluation framework using GPT-4o.",
        "Limitations": "Performance is primarily constrained by the capabilities of the base models. Cannot fully exploit potential without access to more powerful foundational models due to resource limitations.",
        "Datasets / Benchmarks": "RVE-Bench (new, 1,000 unique triplets), comprising two subsets: Reasoning-Informed Video Editing (809 samples) & In-Context Video Generation (200 samples). Leverages Ditto-1M and collected movie data.",
        "Results Summary": "ReViSE significantly enhances editing accuracy and visual fidelity on RVE-Bench, achieving a 32% improvement in Overall score on the reasoning-informed video editing subset over state-of-the-art methods across all reasoning categories.",
        "Why It Matters": "Addresses a crucial gap in video editing by enabling models to understand & incorporate complex reasoning (physical plausibility, causal dynamics), moving beyond literal transformations to generate more realistic, logically coherent, and contextually rich videos.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (ReViSE framework overview), Figure 2 (RVE-Bench overview & statistics), Table 1 (Quantitative Results on RVE-Bench).",
        "arxiv_id": "http://arxiv.org/abs/2512.09924v1"
    }
]