[
    {
        "Title": "Beyond Simple Edits: Composed Video Retrieval with Dense Modifications",
        "Field & Subfield": "Computer Science > Computer Vision",
        "Key Contributions": "• Introduces Dense-WebVid-CoVR dataset with 1.6M samples & dense modification text. • Proposes a robust CoVR model with unified grounding encoder for input video, description, & modification text fusion. • Achieves SOTA on Dense-WebVid-CoVR (71.3% Recall@1), outperforming existing methods by 3.4%. • Shows improved performance on CoIR datasets (CIRR, FashionIQ) & Ego-CVR.",
        "Methodology": "Dense-WebVid-CoVR created using Gemini-Pro for video descriptions & GPT-40 for dense modification texts, with manual verification. The model uses ViT-L for vision, BLIP-pretrained text encoder, & a BLIP-2 based grounding text encoder for unified fusion of query video, description, & modification text via cross-attention, trained with contrastive loss.",
        "Strengths": "• Introduces a large-scale, high-quality dataset with dense modification texts (7x longer). • Novel unified fusion strategy for better multimodal alignment. • Achieves SOTA performance on multiple CoVR & CoIR benchmarks. • Enhanced fine-grained retrieval capturing subtle visual/temporal changes. • Robust quality control for dataset generation.",
        "Limitations": "• Minor inaccuracies (2-3%) in training set, claimed minimal impact. • Future work: multilingual CoVR for low-resource languages. • Future work: efficient techniques for processing very long videos.",
        "Datasets / Benchmarks": "Introduced: Dense-WebVid-CoVR (1.6M samples). Used/Compared against: WebVid-CoVR, EgoCVR, CIRR, FashionIQ.",
        "Results Summary": "Achieves SOTA on Dense-WebVid-CoVR with 71.3% Recall@1 (3.4% gain over prior SOTA). Consistently outperforms baselines on Ego-CVR, CIRR (56.30% R@1), and FashionIQ, demonstrating superior fine-grained retrieval accuracy across all settings.",
        "Why It Matters": "Advances composed video retrieval by providing a richer, contextually aware dataset & an effective model. Enables precise video retrieval based on subtle modifications, crucial for applications like video editing & media production. Dense modification texts are a key innovation.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Fig 1: Example CoVR triplets comparing modification texts. Fig 3: Proposed CoVR architecture. Table 1: Comparative analysis of CoVR benchmarks. Table 2: Main results on Dense-WebVid-CoVR. Fig 13: Detailed modification-text comparison."
    },
    {
        "Title": "COMPUTERRL: SCALING END-TO-END ONLINE REINFORCEMENT LEARNING FOR COMPUTER USE AGENTS",
        "Field & Subfield": "Artificial Intelligence > Reinforcement Learning",
        "Key Contributions": "- Introduces API-GUI paradigm for machine-oriented desktop interaction, unifying API calls and GUI actions.\n- Establishes a large-scale, distributed RL infrastructure supporting thousands of parallel virtual desktop environments for scalable training.\n- Proposes Entropulse, a novel training strategy alternating RL with supervised fine-tuning to mitigate entropy collapse and ensure sustained learning.\n- Achieves new state-of-the-art accuracy of 48.1% on the OSWorld benchmark for general desktop automation agents.",
        "Methodology": "COMPUTERRL combines a novel API-GUI paradigm unifying programmatic control and GUI interaction, a distributed RL infrastructure using Docker and gRPC for scalable parallel environments, and Entropulse, a training strategy that alternates RL with SFT to maintain exploration and prevent entropy collapse. It leverages LLMs for API construction and employs a step-level GRPO algorithm with rule-based verifiable rewards.",
        "Strengths": [
            "Novel API-GUI paradigm offers superior operational efficiency and generalization.",
            "Highly scalable distributed RL infrastructure supports thousands of parallel environments.",
            "Entropulse strategy ensures robust and sustained performance gains in extended RL training.",
            "Achieves state-of-the-art performance on a challenging real-world desktop automation benchmark (OSWorld).",
            "Significantly reduces steps required for task completion compared to baselines (1/3)."
        ],
        "Limitations": [
            "Errors observed in visual perception and multi-application coordination.",
            "Challenges with operational illusions and other miscellaneous errors.",
            "Genuine universality and adaptation to unfamiliar applications remain open questions.",
            "Long-horizon autonomy and complex, multi-step objectives are still being explored."
        ],
        "Datasets / Benchmarks": "Evaluated on the OSWorld benchmark and OSWorld-Verified benchmark. No new datasets were introduced by the paper.",
        "Results Summary": "COMPUTERRL-trained AutoGLM-OS-9B achieved 48.1% success on OSWorld, surpassing SOTA models like OpenAI CUA (42.9%), UI-TARS-1.5 (42.5%), and Claude Sonnet 4 (30.7%). The API-GUI paradigm showed a 134% improvement over GUI-only approaches. Entropulse increased average training rewards and improved learning efficiency by mitigating entropy collapse.",
        "Why It Matters": "This work addresses critical challenges in developing autonomous agents for complex digital workspaces, offering a scalable and robust framework for end-to-end online RL. By bridging the gap between machine and human-centric GUIs, it lays a foundational step towards truly intelligent desktop automation, improving efficiency and paving the way for more capable generalist agents.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1: Shows state-of-the-art success rates on OSWorld (1a) and COMPUTERRL training reward curves (1b), demonstrating Entropulse's benefits. Table 1: Provides a comprehensive comparison of AUTOGLM-OS performance against various proprietary and open models on OSWorld benchmarks."
    },
    {
        "Title": "Relational Visual Similarity",
        "Field & Subfield": "Computer Vision & Image Understanding. Focuses on a novel visual similarity metric leveraging Vision-Language Models for abstract relational reasoning, moving beyond surface attributes to capture human-like conceptual similarities.",
        "Key Contributions": [
            "Introduced 'relational visual similarity' as a new dimension.",
            "Curated a 114k {image-anonymous caption} dataset for relational training.",
            "Developed 'relsim,' a new metric for relational visual similarity.",
            "Demonstrated relsim's utility in image retrieval & generation.",
            "Analyzed relationship between attribute and relational similarity."
        ],
        "Methodology": "Proposed `relsim` by: 1) Filtering LAION-2B to select images with relational cues. 2) Generating anonymous captions (describing relational logic, not surface content) from image groups using a fine-tuned VLM. 3) Training `relsim` (Qwen2.5-VL-7B) with InfoNCE loss to align image/caption embeddings.",
        "Strengths": "Captures human-like abstract relational reasoning, a key missing dimension in visual AI. `relsim` significantly outperforms existing attribute-based metrics in recognizing deeper conceptual similarities, validated by user studies & GPT-40 evaluation. Enables novel image retrieval/generation applications.",
        "Limitations": "Anonymous caption dataset curated manually (532 groups), limiting scalability & prone to bias. VLMs can hallucinate captions. Difficulty in specifying user intent for multi-relational images. Expanding dataset/pipeline is a future direction.",
        "Datasets / Benchmarks": "New: Relational Dataset (114k {image, anonymous caption} pairs derived from LAION-2B). Baselines: LPIPS, DINO, CLIP, dreamsim (image-to-image); CLIP-T, Qwen-T (caption-based). Evaluation: GPT-40 automated judge & human user study.",
        "Results Summary": "`relsim` achieved a GPT score of 6.77 (vs. LPIPS 4.56, CLIP-I 5.91), indicating superior relational similarity detection. User studies showed consistent human preference for `relsim` (42.5-60.7%). Proprietary models excel in analogical generation (GPT40 relsim 0.82) over open-source models.",
        "Why It Matters": "Addresses a critical gap in AI's visual understanding by moving beyond surface attributes to capture abstract, relational logic, mirroring human cognition. This unlocks new possibilities for creative AI applications, enhancing image retrieval, and analogical content generation.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Illustrates relational vs. attribute similarity), Figure 2 (Overall Pipeline), Figure 6 (relsim performance vs. baselines), Figure 8 (User study results), Table 2 (Analogical image generation benchmarks).",
        "arxiv_id": "http://arxiv.org/abs/2512.07833v1"
    },
    {
        "Title": "ASTRA: General INTERACTIVE WORLD MODEL WITH AUTOREGRESSIVE DENOISING",
        "Field & Subfield": "Computer Vision, Generative Models, World Models",
        "Key Contributions": "Introduces Astra, an interactive general world model. Key contributions: Autoregressive denoising with temporal causal attention; Action-aware adapter for precise control; Noise-augmented history memory for consistency; Mixture of Action Experts for diverse modalities.",
        "Methodology": "Autoregressive denoising framework based on a pre-trained video diffusion backbone. Utilizes an Action-Aware Flow Transformer (AFT) with ACT-Adapter for action injection, a noise-as-mask strategy for history memory to reduce visual inertia, and a Mixture of Action Experts (MoAE) for multimodal actions.",
        "Strengths": "Achieves high visual fidelity, strong temporal consistency, and precise action responsiveness. Versatile across diverse tasks (robotics, AD, exploration) and generalizes well to out-of-domain scenes. Parameter-efficient design with low compute overhead.",
        "Limitations": "Inference efficiency is limited, requiring multiple denoising steps per frame. This makes real-time deployment challenging for latency-sensitive applications like online control or interactive robotics.",
        "Datasets / Benchmarks": "nuScenes, Sekai, SpatialVID, RT-1, Multi-Cam Video. Evaluated on Astra-Bench (custom benchmark) and CityWalker for out-of-domain generalization.",
        "Results Summary": "Outperforms SOTA models (Wan-2.1, MatrixGame, YUME) across all metrics: instruction following, subject/background consistency, motion smoothness, and visual quality. Achieves lower rotation/translation errors and superior action-following accuracy on diverse benchmarks.",
        "Why It Matters": "Astra advances interactive world modeling, enabling more general, scalable simulators for exploration, robotics, autonomous driving, and embodied intelligence, bridging the gap to true interactive world simulation.",
        "Should Read Fully?": "Yes, for researchers and practitioners interested in interactive world models, diffusion-based video generation, robotics, autonomous driving, or embodied AI.",
        "Key Figures or Tables": "Fig. 1 (diverse applications), Fig. 3 (Astra framework), Fig. 5 & 6 (qualitative results), Table 2 (SOTA comparison), Table 3 (ablation studies).",
        "arxiv_id": "http://arxiv.org/abs/2512.08931v1"
    },
    {
        "Title": "Efficiently Reconstructing Dynamic Scenes One D4RT at a Time",
        "Field & Subfield": "Computer Vision; 4D Reconstruction & Tracking",
        "Key Contributions": "Novel query-based feedforward model for efficient 4D scene reconstruction. Unified transformer architecture for depth, correspondence, point clouds, camera params. SOTA accuracy & speed in dynamic 4D reconstruction & tracking. Enables efficient dense, holistic scene reconstruction.",
        "Methodology": "D4RT uses an encoder (ViT-g) to create a global scene representation (F) from video. A lightweight cross-attention decoder queries F with spatio-temporal parameters (u,v, t_src, t_tgt, t_cam) and local RGB patch to predict 3D point positions.",
        "Strengths": "Highly efficient (18-300x faster, 200+ FPS), scalable, unified interface for multiple 4D tasks, SOTA accuracy across diverse benchmarks, handles dynamic scenes, supports subpixel precision & high-res decoding with novel querying.",
        "Limitations": "No explicit limitations for D4RT are stated, as the paper focuses on overcoming prior methods' drawbacks. Implicitly, a large encoder requires pre-training. Dense inference still has computational cost, partially addressed by occupancy grid optimization.",
        "Datasets / Benchmarks": "BlendedMVS, Co3Dv2, Dynamic Replica, Kubric, MVS-Synth, PointOdyssey, ScanNet, ScanNet++, Tartanair, VirtualKitti, Waymo Open (training). Sintel, ScanNet, KITTI, Bonn, TapVid-3D (evaluation).",
        "Results Summary": "D4RT sets SOTA across 4D tasks (depth, point cloud, 3D tracking, camera pose) on various benchmarks. Achieves significantly higher throughput (e.g., 200+ FPS, 100x faster than MegaSaM) while delivering superior accuracy. Generalizes to long videos & high-res decoding.",
        "Why It Matters": "Offers a unified, efficient, and highly scalable framework for 4D reconstruction, overcoming fragmentation and computational bottlenecks of prior methods, paving the way for next-generation 4D perception in complex dynamic environments.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Fig. 1 (Model overview), Fig. 3 (Pose accuracy vs. speed), Fig. 4 (Qualitative reconstruction), Table 3 (3D tracking throughput), Table 4 (4D tracking metrics).",
        "arxiv_id": "http://arxiv.org/abs/2512.08924v1"
    },
    {
        "Title": "Selfi: Self Improving Reconstruction Engine via 3D Geometric Feature Alignment",
        "Field & Subfield": "Computer Vision & 3D Reconstruction, Novel View Synthesis",
        "Key Contributions": "• Self-improving geometric feature learning from unposed RGB via reprojection-based consistency loss. • Achieves state-of-the-art NVS and pose estimation using a frozen VFM backbone. • Enhances NVS quality and pose accuracy via dense bundle adjustment with depth shift.",
        "Methodology": "Selfi uses a pre-trained VGGT (3D VFM) as a backbone. A DPT adapter learns geometrically-aligned features using a self-supervised reprojection-based consistency loss with pseudo-ground-truth from VGGT outputs. These features predict 3D Gaussian parameters via a U-Net decoder. Poses are refined using dense bundle adjustment, with Gaussian centers adjusted via an affine depth shift.",
        "Strengths": "State-of-the-art NVS and pose estimation from unposed, uncalibrated images. Achieves high-fidelity rendering, including thin structures and fine details. Robust to varying input views/overlap. Self-supervised, efficiently leveraging VFMs.",
        "Limitations": "Relies on VGGT's normalized scale, leading to depth inaccuracies in distant regions (e.g., sky). Sensitive to exposure discrepancies between input and target images. Currently restricted to static scenes, failing on dynamic environments.",
        "Datasets / Benchmarks": "Trained on DL3DV and RealEstate10K. Evaluated on DL3DV, RealEstate10K, MipNeRF, Tanks&Temples, and RayZer split. Metrics include PSNR, SSIM, LPIPS for NVS; AUC@3, AUC@5, AUC@15 for pose estimation.",
        "Results Summary": "Selfi consistently outperforms existing pose-free feed-forward Gaussian methods (e.g., AnySplat, WorldMirror) and achieves performance comparable to/exceeding 3DGS with ground-truth poses. Achieves SOTA on NVS & pose estimation benchmarks across various input settings (varying sequence lengths, overlap).",
        "Why It Matters": "Selfi advances novel view synthesis and 3D reconstruction by bypassing the fragile and computationally intensive SfM pipeline. It enables robust, high-fidelity 3D scene understanding from unposed images, making 3D VFMs more practical for real-world applications without explicit 3D ground truth.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Selfi pipeline overview), Figure 3 & 5 (Qualitative NVS comparisons), Table 1 & 2 (NVS performance with varying inputs/overlap), Table 5 (BA for NVS/Pose).",
        "arxiv_id": "http://arxiv.org/abs/2512.08930v1"
    },
    {
        "Title": "Closing the Train-Test Gap in World Models for Gradient-Based Planning",
        "Field & Subfield": "Reinforcement Learning & Robotics; Model-Based Planning",
        "Key Contributions": "- Proposes Online World Modeling (OWM) to expand WM's reliable state space via simulator-corrected trajectories.\n- Introduces Adversarial World Modeling (AWM) to smooth loss landscape & promote robustness by finetuning on perturbed inputs.\n- Achieves CEM-level performance with 10x speedup for gradient-based planning.",
        "Methodology": "Two finetuning methods (OWM, AWM) for latent world models (DINO-WM) used in gradient-based planning (GBP). OWM generates and finetunes on simulator-corrected GBP trajectories. AWM perturbs inputs to maximize prediction error, smoothing the planning loss landscape.",
        "Strengths": "Significantly improves gradient-based planning reliability and performance, often matching or exceeding CEM. Achieves a 10x computational speedup over CEM. Smooths the planning loss landscape, reducing local minima. Narrows the train-test gap in world model error.",
        "Limitations": "Online World Modeling relies on access to an environment simulator, which might be costly or infeasible in real-world settings. Performance benefits vary by task and specific gradient optimizer used (e.g., Adam generally outperforms GD).",
        "Datasets / Benchmarks": "PushT, PointMaze, Wall (main experiments); Rope, Granular (additional robotic manipulation tasks). Utilizes existing DINO-WM datasets and architectures.",
        "Results Summary": "AWM with Adam GBP outperforms or matches CEM on PushT, PointMaze, Wall tasks, achieving up to +30% success rate increase over DINO-WM (open-loop) and up to 10x faster execution. Both OWM and AWM narrow the train-test gap.",
        "Why It Matters": "Enables practical and efficient gradient-based planning with world models, a critical step for real-world robotics. Addresses a fundamental train-test gap, making learned dynamics models more reliable for long-horizon control.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Table 1 (Planning Results), Figure 2 (Optimization landscape of DINO-WM vs. AWM), Figure 3 (Planning efficiency)",
        "arxiv_id": "2512.09929v1"
    },
    {
        "Title": "GAINS: Gaussian-based Inverse Rendering from Sparse Multi-View Captures",
        "Field & Subfield": "Computer Vision & Graphics, Inverse Rendering",
        "Key Contributions": "- Two-stage inverse rendering for sparse multi-view captures.\n- Synergizes learning-based priors (depth/normal, diffusion) for robust geometry.\n- Combines segmentation, IID, and diffusion priors for stable material recovery.\n- Achieves SOTA relighting, NVS, and material accuracy on sparse inputs.",
        "Methodology": "Two-stage pipeline using 2DGS. Stage I: Geometry refinement with monocular depth/normal and diffusion priors. Stage II: Material properties & lighting estimation using segmentation, intrinsic image decomposition (IID), and diffusion priors, all combined with PBR.",
        "Strengths": "Significantly improves material parameter accuracy, relighting quality, and novel-view synthesis, especially under sparse-view conditions. More robust against overfitting and ambiguity than prior methods. Better intrinsic separation and reduced reflection baking.",
        "Limitations": "Segmentation guidance can trade off robustness for high-frequency detail under dense supervision. IID prior can exhibit view-inconsistency, though mitigated by a weighted loss.",
        "Datasets / Benchmarks": "Synthetic4Relight [44], TensorIR [10], Ref-Real [29]. Evaluated using PSNR, SSIM, LPIPS for NVS/Albedo/Relighting, and MSE for Roughness.",
        "Results Summary": "GAINS consistently outperforms SOTA Gaussian-based IR methods (Ref-GS, GI-GS) on all datasets and metrics, especially with sparse (4-8) views. Achieves higher PSNR, SSIM, lower LPIPS/MSE for NVS, albedo, roughness, and relighting.",
        "Why It Matters": "Addresses the challenging problem of inverse rendering from sparse inputs, which is critical for real-world applications where dense multi-view captures are impractical or impossible, enabling more robust 3D scene understanding.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Table 1, Table 2 (Quantitative evaluation across datasets), Table 3 (Ablation study of priors), Figure 1 (Qualitative intrinsics/relighting), Figure 6 (View-dependent performance)."
    },
    {
        "Title": "ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning",
        "Field & Subfield": "Computer Vision & Generative AI: Video Editing",
        "Key Contributions": "- Introduces Reason-Informed Video Editing (RVE) task.\n- Proposes RVE-Bench, a comprehensive benchmark for RVE.\n- Develops ReViSE, a Self-Reflective Reasoning (SRF) framework.\n- Leverages internal VLM for intrinsic, differential feedback.\n- Achieves SOTA on RVE-Bench.",
        "Methodology": "ReViSE employs a Self-Reflective Reasoning (SRF) framework, unifying generation & internal evaluation. An internal VLM acts as a critic, providing differential feedback. Training uses Unified Semantic Optimization (USO) & Reward Weighted Optimization (RWO) with flow-matching loss.",
        "Strengths": "Bridges the reasoning-editing gap in video models, provides self-supervised intrinsic feedback, significantly improves reasoning accuracy & visual fidelity, and introduces a robust, reasoning-aware evaluation framework using GPT-4o.",
        "Limitations": "Performance is primarily constrained by the capabilities of the base models. Cannot fully exploit potential without access to more powerful foundational models due to resource limitations.",
        "Datasets / Benchmarks": "RVE-Bench (new, 1,000 unique triplets), comprising two subsets: Reasoning-Informed Video Editing (809 samples) & In-Context Video Generation (200 samples). Leverages Ditto-1M and collected movie data.",
        "Results Summary": "ReViSE significantly enhances editing accuracy and visual fidelity on RVE-Bench, achieving a 32% improvement in Overall score on the reasoning-informed video editing subset over state-of-the-art methods across all reasoning categories.",
        "Why It Matters": "Addresses a crucial gap in video editing by enabling models to understand & incorporate complex reasoning (physical plausibility, causal dynamics), moving beyond literal transformations to generate more realistic, logically coherent, and contextually rich videos.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (ReViSE framework overview), Figure 2 (RVE-Bench overview & statistics), Table 1 (Quantitative Results on RVE-Bench).",
        "arxiv_id": "http://arxiv.org/abs/2512.09924v1"
    },
    {
        "Title": "SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model",
        "Field & Subfield": "Computer Vision & 3D Scene Generation",
        "Key Contributions": "- Decoupled 3D scene generation framework for better open-set prior learning.\n- Robust de-occlusion model leveraging image datasets & a new 10K curated dataset.\n- Unified pose estimation diffusion model with global/local attention & new 200K synthetic scene dataset for generalization.",
        "Methodology": "Decoupled 3D scene generation into de-occlusion, 3D object generation, and pose estimation. De-occlusion model finetunes Flux Kontext on a 10K image dataset. Pose estimation uses a diffusion model with global/local self/cross-attention, trained on a 200K synthetic scene dataset, predicting 6D pose & size.",
        "Strengths": "Achieves SOTA on indoor & open-set scenes for geometry quality & pose accuracy. Robust under severe occlusion & diverse objects. Strong generalization. Enables text-controllable de-occlusion, enhancing quality & controllability.",
        "Limitations": "Real-world object arrangements (e.g., force interactions, interpenetration) are not fully captured by datasets. Existing methods have limited control signals (images/simple captions). Deeper understanding for embodied AI tasks remains an unsolved challenge.",
        "Datasets / Benchmarks": "New 10K object image de-occlusion dataset, new 200K synthetic open-set scene dataset. Trained on Objaverse. Evaluated against MIDI, 3D-Front, and other SOTA methods.",
        "Results Summary": "Outperforms SOTA methods on indoor & open-set scenes in both geometry quality (CD-S, F-Score-S, IoU-B) and pose accuracy, and de-occlusion metrics (PSNR, SSIM, CLIP). Demonstrates superior performance, especially under severe occlusion.",
        "Why It Matters": "Advances realistic open-set 3D scene generation, addressing crucial limitations in occlusion handling and accurate pose estimation. Essential for progress in AIGC, embodied AI, simulation environment construction, and 3D perception.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (SceneMaker outputs), Figure 3 (Framework), Table 3 (Quantitative comparison on indoor & open-set), Figure 6 (De-occlusion comparison).",
        "arxiv_id": "http://arxiv.org/abs/2512.10957v1"
    },
    {
        "Title": "StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space",
        "Field & Subfield": "Computer Vision & Novel View Synthesis / Stereo Generation",
        "Key Contributions": "- Depth-free monocular-to-stereo synthesis via end-to-end diffusion in a canonical space.\n- Viewpoint-conditioned framework using Plücker ray embeddings for metric baseline control.\n- Novel evaluation protocol with iSQoE (perceptual comfort) and MEt3R (geometric consistency).",
        "Methodology": "Diffusion-based framework (LDM) using a dual U-Net architecture (initialized from Stable Diffusion 2.0). Models geometry via viewpoint conditioning with Plücker ray embeddings in a canonical rectified space, avoiding explicit depth or warping.",
        "Strengths": "Depth-free handling of complex multi-layer and non-Lambertian scenes. Strong cross-baseline generalization with metric control. Superior perceptual comfort (iSQoE) and geometric consistency (MEt3R) compared to baselines.",
        "Limitations": "All methods, including StereoSpace, still exhibit degradation on extreme multi-layer/transparent scenes. Auxiliary disparity loss slightly degrades iSQoE scores.",
        "Datasets / Benchmarks": "Training: TartanAir, Dynamic Replica, IRS, Falling Things, LayeredFlow, NeRF-Stereo, SceneSplat-7K. Evaluation: Middlebury 2014, DrivingStereo, Booster, LayeredFlow.",
        "Results Summary": "Outperforms prior warp/inpaint, latent-warping, and warped-conditioning methods on iSQoE and MEt3R metrics. Achieves best perceptual comfort and geometric consistency, particularly on complex multi-layer geometries.",
        "Why It Matters": "Enables high-quality stereo image generation from monocular inputs without costly 3D reconstruction, reducing production costs and facilitating 2D to 3D content conversion for immersive media.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Framework Overview), Table 2 (Main Results on Middlebury & DrivingStereo)",
        "arxiv_id": "http://arxiv.org/abs/2512.10959v1"
    },
    {
        "Title": "WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World",
        "Field & Subfield": "Embodied AI & Computer Vision; Generative World Models",
        "Key Contributions": "- Introduces WorldLens, a benchmark for driving world models.\n- Defines 5 evaluation aspects: Generation, Reconstruction, Action-Following, Downstream Task, Human Preference.\n- Curates WorldLens-26K, a large-scale human-annotated video dataset.\n- Develops WorldLens-Agent, an auto-evaluator distilled from human preferences.",
        "Methodology": "WorldLens evaluates driving world models across 5 aspects using 20+ metrics. It combines quantitative metrics (e.g., FVD, LPIPS, NDS) with human preference scores via WorldLens-26K. WorldLens-Agent (Qwen3-VL-8B fine-tuned) enables scalable, human-aligned auto-evaluation.",
        "Strengths": "Comprehensive, multi-dimensional evaluation covering visual realism, geometric consistency, physical plausibility, and functional reliability. Integrates human judgment for alignment. Scalable and interpretable auto-evaluation via WorldLens-Agent. Bridges objective metrics with subjective perception.",
        "Limitations": "Primarily focused on driving scenarios. Human preference dataset may reflect annotator bias. Evaluation agent inherits LLM limitations. Physical realism evaluation is open-ended, requiring new metrics for interactive/multimodal 4D.",
        "Datasets / Benchmarks": "WorldLens (benchmark), WorldLens-26K (human-annotated video dataset), nuScenes (for some evaluations), Kinetics (for I3D features).",
        "Results Summary": "No single world model excels universally. Models with strong textures often violate physics; geometry-stable ones lack behavioral fidelity. DiST-4D performs well in geometry/novel-view. Perceptual quality doesn't imply usability. Temporal consistency and dataset alignment are critical for task-specific effectiveness.",
        "Why It Matters": "Establishes a unified protocol to measure driving world model fidelity, beyond visual realism, to include physical reliability & functional safety. Essential for developing robust embodied AI, safer autonomous driving, and more trustworthy synthetic data generation.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (WorldLens Framework), Figure 2 (Evaluation Aspects), Table 1 (Benchmarking Results).",
        "arxiv_id": "http://arxiv.org/abs/2512.10958v1"
    },
    {
        "Title": "DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders",
        "Field & Subfield": "Computer Vision; Generative Models; Human-Computer Interaction",
        "Key Contributions": "- Lightweight, model-agnostic framework for interactive video diffusion previews.\n- Preserves full generation quality, supports rapid iteration, and is plug-and-play.\n- Generates rich multi-modal previews (RGB, albedo, depth, normals) via multi-branch, multi-loss predictor.\n- Enables novel interactive generation control via stochasticity renoising and latent steering.",
        "Methodology": "A model-agnostic, lightweight multi-branch decoder framework. Linear probing identifies early intrinsic emergence. A multi-loss predictor mitigates superposition. Variations are generated via stochastic renoising (diff. noise) & latent steering (gradient-based optimization).",
        "Strengths": "Provides fast (sub-second), multi-modal previews, enabling early termination & rapid iteration. Offers interactive control over generation. Addresses superposition artifacts. Preserves full base model quality & is plug-and-play. User study confirms high perceptual value.",
        "Limitations": "Scope limited to scene intrinsics; text prompts not explicitly considered. Steering can have failure cases due to out-of-distribution issues or limited model capacity. Higher resolution/coherence could be improved. Simplified steering methods.",
        "Datasets / Benchmarks": "Synthetic video dataset (1,000 videos, 40 categories) generated using DiffusionRenderer. Compared against x0-pred, Video Depth Anything, Diffusion Renderer, Linear Probing. PSNR, MSE, L1, LPIPS metrics used.",
        "Results Summary": "Achieves previews in <1s for 4s videos, outperforming baselines in PSNR for most channels (e.g., RGB 18.03 Ours vs 16.98 x0-pred at 10% steps) and being significantly more efficient. Multi-branch decoder reduces artifacts. User study favors DiffusionBrowser for content predictability, visual fidelity, & scene clarity (74-77% preference).",
        "Why It Matters": "Transforms opaque video diffusion into an interactive, controllable, and resource-efficient process. Allows users to guide generations, terminate unpromising outcomes early, and better understand diffusion's internal dynamics, crucial for practical deployment and advanced research.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (DiffusionBrowser overview), Figure 2 (Linear probing results), Figure 5 (MB decoder comparison), Table 1 (Baselines comparison), Table 3 (User study results).",
        "arxiv_id": "http://arxiv.org/abs/2512.13690v1"
    },
    {
        "Title": "MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives",
        "Field & Subfield": "Computer Vision, Video Generation",
        "Key Contributions": "- Proposes MemFlow for consistent and efficient long video generation.\n- Introduces Narrative Adaptive Memory (NAM) for dynamic retrieval of semantically-aligned context.\n- Develops Sparse Memory Activation (SMA) for relevance-gated memory filtering to balance efficiency and quality.\n- Achieves SOTA quality with real-time inference (18.7 FPS) on H100.",
        "Methodology": "MemFlow integrates Narrative Adaptive Memory (NAM) for dynamic context retrieval (semantic retrieval, redundant removal) and Sparse Memory Activation (SMA) for efficiency. It's built upon an autoregressive-diffusion framework, using a streaming long-tuning strategy with Self-Forcing and DMD loss.",
        "Strengths": "Maintains long-term consistency and narrative coherence, even with complex character/scene changes. Achieves state-of-the-art visual quality and semantic alignment. Balances memory efficiency and quality effectively. Compatible with existing streaming video generation models with KV cache.",
        "Limitations": "Incurs a slight speed reduction (7.9% vs. memory-free baseline, 8.6% vs. LongLive) due to memory updating/activation. Larger memory capacities in NAM can lead to performance instability or underperform baselines due to attention field imbalance.",
        "Datasets / Benchmarks": "Customized 100 narrative scripts (6x10s prompts) for multi-prompt generation. Evaluated using VBench-Long metrics for quality, consistency, and aesthetics. CLIP score for text alignment.",
        "Results Summary": "MemFlow achieved highest overall quality (85.02) and aesthetic score (61.07) for multi-prompt 60s videos. Demonstrated superior subject/background consistency and prompt adherence. Achieved 18.7 FPS inference on a single NVIDIA H100 while maintaining high quality.",
        "Why It Matters": "Addresses the critical challenge of maintaining content consistency in interactive long video generation, which is crucial for advancing creative and cinematic applications. Enables coherent narratives that adapt to dynamic prompts and scene transitions.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Qualitative comparison), Figure 2 (Overall Framework), Table 1 (Multi-prompt 60s comparison), Table 2 (Single-prompt 5s comparison), Table 3 (Memory mechanism ablation).",
        "arxiv_id": "2512.14699v1"
    },
    {
        "Title": "Spherical Leech Quantization for Visual Tokenization and Generation",
        "Field & Subfield": "Computer Vision, Image Quantization & Generation",
        "Key Contributions": "- Unified NPQ methods via lattice coding.\n- Introduced Spherical Leech Quantization (A24-SQ) based on Leech lattice.\n- Simplified autoencoder training without entropy regularization.\n- Achieved state-of-the-art visual tokenization/generation with large codebooks (~200K).",
        "Methodology": "A24-SQ uses fixed 24-dimensional Leech lattice vectors for quantization, mapped from a hypersphere. It's integrated into auto-encoders and autoregressive models using standard l1, GAN, LPIPS losses, eliminating entropy regularization. Efficiency maintained by tiling/JIT-compiling and VF loss for alignment.",
        "Strengths": "Simplified training (no regularization), high efficiency (fixed lattice, no gradient updates, memory/runtime efficient), improved rate-distortion tradeoff, better reconstruction & generation quality, and scalability to very large codebooks, achieving oracle-like performance.",
        "Limitations": "Factorized d-itwise prediction yields worse gFID and lower recall. Codebook usage imbalance with large codebooks requires specific training techniques (Z-loss, Dion optimizer). Future work includes larger-scale, text-conditioned visual generation.",
        "Datasets / Benchmarks": "ImageNet-1k, COCO2017 val, Kodak Lossless True Color Image Suite. Metrics: PSNR, SSIM, LPIPS, rFID, gFID, IS, Precision, Recall.",
        "Results Summary": "A24-SQ outperforms BSQ in reconstruction (rFID 0.83 vs 1.14, better PSNR/SSIM/LPIPS) using fewer bits. Achieved 1.82 FID for AR generation on ImageNet-1k, comparable to oracle (1.78 FID), with a 196,560-way codebook. Pushed precision-recall frontier closer to oracle.",
        "Why It Matters": "Addresses codebook scalability and principled design in visual tokenization. Enables high-quality image compression and generation with very large vocabularies, bridging the gap between vision and language models' vocabulary sizes for general-purpose visual AI.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Method overview & results), Figure 2 (dmin comparison of lattices), Table 4 (A24-SQ vs BSQ details), Table 5 (Reconstruction results), Table 7 (Generation results).",
        "arxiv_id": "http://arxiv.org/abs/2512.14697v1"
    },
    {
        "Title": "TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs",
        "Field & Subfield": "Multimodal AI, Video Understanding, Temporal Grounding",
        "Key Contributions": "• Exposed critical quality issues in existing VTG benchmarks. • Introduced TimeLens-Bench (re-annotated, high-quality eval suite). • Created TimeLens-100K (large-scale, high-quality training data). • Derived optimal algorithmic practices: interleaved textual encoding & thinking-free RLVR. • TimeLens models: SOTA VTG, surpasses GPT-5/Gemini-2.5-Flash.",
        "Methodology": "Systematic investigation on data quality (manual & automated re-annotation for benchmarks/training) & algorithmic design. Key insights: interleaved textual encoding for time; thinking-free RLVR with early stopping & difficulty-based sampling.",
        "Strengths": "Rigorous data overhaul created high-quality, reliable benchmarks & training data. Comprehensive algorithmic study yields practical insights. Achieves SOTA performance. All code, data, models are open-sourced.",
        "Limitations": "Paper focuses on establishing an 'essential baseline' rather than novel methods. Current work is not focused on reasoning-intensive VTG scenarios, which is left for future exploration.",
        "Datasets / Benchmarks": "TimeLens-Bench (re-annotated Charades-STA, ActivityNet Captions, QVHighlights). TimeLens-100K (large-scale, high-quality training dataset).",
        "Results Summary": "TimeLens models achieve SOTA VTG, outperforming open-source and surpassing proprietary models (GPT-5, Gemini-2.5-Flash) on TimeLens-Bench. Performance significantly improved by high-quality data & optimized algorithmic practices.",
        "Why It Matters": "Addresses a critical limitation of MLLMs in temporal awareness for video understanding. Provides robust tools and insights, offering a reliable foundation for future research and evaluation in this vital field.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Fig 2 (Data quality impact, performance gains), Table 1 (Main Results), Fig 5 (Timestamp encoding schemes), Table 3 (Training paradigms).",
        "arxiv_id": "http://arxiv.org/abs/2512.14698v1"
    },
    {
        "Title": "DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models",
        "Field & Subfield": "Artificial Intelligence, Vision-Language Models, Diffusion Models",
        "Key Contributions": "- Proposes DiffusionVL, translating any AR models into diffusion VLMs.\n- Achieves SOTA among diffusion VLMs with <5% of prior data.\n- Narrows performance gap with advanced AR-VLMs.\n- Introduces block-decoding for arbitrary-length generation & KV cache reuse.\n- Attains 2x inference speedup.",
        "Methodology": "Simple diffusion finetuning converts AR-VLMs directly. For AR-LMs, a two-stage process (connector pretraining, then diffusion finetuning) is used. Employs a block diffusion strategy for parallel decoding, arbitrary length generation, and efficient KV-cache reuse.",
        "Strengths": "Achieves SOTA among diffusion VLMs with significantly less training data (<5%). Provides 2x inference speedup over prior dVLMs. Supports arbitrary-length generation and efficient KV-cache reuse, previously absent. Enables feasible conversion from *any* AR model.",
        "Limitations": "Performance, while competitive with AR-VLMs, may still lag top-tier models in some specific aspects. There is a trade-off between smaller block size (slightly better performance) and poorer parallelism.",
        "Datasets / Benchmarks": "MMMU (Pro/Std/Val), MME (Cog/Perp), SeedBench (Image/Video), ChartQA, AI2D, MMBench, RealworldQA, Muirbench. Trained on LLaVA-Pretrain (580K) and LLaVA-Next (738K).",
        "Results Summary": "DiffusionVL achieves SOTA among diffusion VLMs (e.g., 34.4% gain on MMMU-Pro, 37.5% on MME) using <5% data, is competitive with advanced AR-VLMs, and achieves a 2x inference speedup.",
        "Why It Matters": "This work bridges the gap between autoregressive and diffusion paradigms for Vision Language Models, enabling the efficient development of high-performance dVLMs with superior parallel decoding, arbitrary-length generation, and KV cache reuse capabilities.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Performance Comparison), Figure 4 (Speed & Quality for Image Captioning), Tables 1 & 2 (Benchmark Performance Comparison of DiffusionVL vs. AR & other Diffusion Models).",
        "arxiv_id": "http://arxiv.org/abs/2512.15713v1"
    },
    {
        "Title": "In Pursuit of Pixel Supervision for Visual Pre-training",
        "Field & Subfield": "Computer Vision, Self-Supervised Learning, Masked Autoencoders (MAE)",
        "Key Contributions": "- Introduces \"Pixio,\" an enhanced MAE with a deeper decoder, larger mask blocks, and multiple [CLS] tokens.\n- Trains on 2B web-crawled images with a novel, soft self-curation strategy.\n- Achieves competitive or superior performance over SOTA DINOv3 on various dense prediction tasks.",
        "Methodology": "Pixio is an enhanced MAE for pixel-space self-supervised learning. It uses a deeper decoder, larger 4x4 patch mask blocks for challenging pretext, and 8 class tokens. Trained on 2B web images with reconstruction loss and soft self-curation based on reconstruction difficulty.",
        "Strengths": "Demonstrates pixel-space SSL's competitiveness, simplicity, stability, & efficiency. Outperforms or matches DINOv3 on many dense prediction and robot learning tasks. Uses minimally-curated web-scale data, reducing benchmark bias.",
        "Limitations": "Masking is an artificial distortion introducing biases. Fixed masking ratios/granularities can be suboptimal for diverse image complexities. Inferior on KITTI (driving) due to less domain-specific training data than DINOv3.",
        "Datasets / Benchmarks": "Pre-training: 2B web-crawled images (self-curated). Evaluation: NYUv2, KITTI, DIODE, Sintel, DA-2K (Depth); ScanNet++, ETH3D, TartanAirV2-WB (3D Recon); ADE20K, Pascal VOC, LoveDA (Semantic Seg); CortexBench (Robot Learning).",
        "Results Summary": "Pixio significantly outperforms original MAE. It surpasses DINOv3 on most depth estimation benchmarks (e.g., 0.268 RMSE vs 0.320 on NYUv2), 3D reconstruction, & robot learning, and is competitive in semantic segmentation.",
        "Why It Matters": "Validates pixel-space self-supervised learning as a powerful, scalable, and less biased alternative or complement to latent-space methods. Offers a promising path for generic visual representations, especially for future video SSL.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Visualizations of Pixio's understanding), Figure 2 (Pixio updates over MAE), Table 1 (Monocular Depth Estimation), Table 3 (Feed-Forward 3D Reconstruction), Table 5 (Robot Learning).",
        "arxiv_id": "http://arxiv.org/abs/2512.15715v1"
    },
    {
        "Title": "Spatia: Video Generation with Updatable Spatial Memory",
        "Field & Subfield": "Computer Vision & Graphics, Video Generation, 3D-Aware Synthesis, Long-Horizon Consistency",
        "Key Contributions": [
            "Explicit 3D scene point cloud as updatable spatial memory.",
            "Dynamic-static disentanglement for consistent videos with dynamic entities.",
            "Spatially consistent generation across multiple views.",
            "Explicit 3D-aware camera control & interactive editing."
        ],
        "Methodology": "Iterative generation: estimates initial 3D point cloud from image, generates video clips conditioned on memory, then updates memory via visual SLAM. Uses a multi-modal conditional diffusion transformer (Wan2.2) with ControlNet. Static entities are disentangled during point cloud estimation.",
        "Strengths": "Achieves robust long-term spatial/temporal consistency. Handles dynamic entities while preserving static scenes. Enables explicit camera control & 3D-aware interactive editing. Outperforms SOTA in consistency and visual quality metrics. Provides a geometrically grounded framework.",
        "Limitations": "Point cloud density (memory vs. fine-grained guidance tradeoff). Potential for error accumulation in extremely long sequences, though mitigated by iterative updates. Dependency on accurate 3D scene point cloud estimation and camera pose.",
        "Datasets / Benchmarks": [
            "Training: RealEstate (40K videos), SpatialVID (10K HD videos).",
            "Evaluation: WorldScore benchmark, RealEstate test set."
        ],
        "Results Summary": "Achieves SOTA on WorldScore (Avg Score 69.73) and RealEstate (PSNR 18.58, SSIM 0.646, LPIPS 0.254), significantly improving static scene consistency while maintaining dynamic entity quality. Superior memory consistency in closed-loop generation (Match Acc 0.698).",
        "Why It Matters": "Addresses a critical challenge in video generation: long-term consistency. Enables novel applications like interactive 3D editing and precise camera control, moving towards robust \"world models\" for AI with persistent memory and geometrically grounded, scalable frameworks.",
        "Should Read Fully?": "Yes",
        "Key Figures or Tables": "Figure 1 (Spatia Overview), Figure 2 (Training Pipeline), Table 1 (WorldScore Results), Table 3 (Memory Evaluation).",
        "arxiv_id": "http://arxiv.org/abs/2512.15716v1"
    }
]